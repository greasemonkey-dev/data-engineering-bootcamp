{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Data Engineering Bootcamp: Foundations &amp; Data Storage","text":"<p>Welcome to the Data Engineering Bootcamp - a comprehensive, hands-on course designed for software engineers transitioning into data engineering roles.</p>"},{"location":"#course-overview","title":"Course Overview","text":"<p>This textbook covers Weeks 1-4 of the full 12-week bootcamp, focusing on the essential foundations and data storage concepts that every data engineer must master.</p>"},{"location":"#what-youll-learn","title":"What You'll Learn","text":"<p>By completing this course, you will be able to:</p> <ul> <li>Write production-quality Python using advanced features like generators, comprehensions, and context managers</li> <li>Master SQL with window functions, CTEs, and query optimization techniques</li> <li>Design robust databases using normalization principles and appropriate indexing strategies</li> <li>Build dimensional models for data warehousing with star and snowflake schemas</li> <li>Work effectively with Git using branching, merging, and collaborative workflows</li> <li>Containerize applications with Docker and Docker Compose</li> <li>Choose appropriate storage solutions including relational, NoSQL, and cloud data warehouses</li> <li>Optimize BigQuery queries with partitioning and clustering strategies</li> </ul>"},{"location":"#course-structure","title":"Course Structure","text":"<p>This course is divided into 4 comprehensive chapters:</p>"},{"location":"#chapter-1-python-sql-foundations","title":"Chapter 1: Python &amp; SQL Foundations","text":"<p>Master advanced Python features and SQL techniques essential for data engineering. Learn list comprehensions, generators, window functions, and CTEs through real-world examples.</p>"},{"location":"#chapter-2-devops-foundations-git-docker","title":"Chapter 2: DevOps Foundations - Git &amp; Docker","text":"<p>Build collaborative development skills with Git workflows and containerization with Docker. Understand branching strategies, merge vs rebase, and multi-container applications.</p>"},{"location":"#chapter-3-database-design-modeling","title":"Chapter 3: Database Design &amp; Modeling","text":"<p>Design efficient database schemas using normalization principles. Learn when to normalize for transactional systems and when to denormalize for analytics.</p>"},{"location":"#chapter-4-data-warehousing-bigquery","title":"Chapter 4: Data Warehousing &amp; BigQuery","text":"<p>Build data warehouses with dimensional modeling. Compare OLTP vs OLAP systems, design star and snowflake schemas, and optimize BigQuery performance.</p>"},{"location":"#learning-resources","title":"Learning Resources","text":""},{"location":"#interactive-learning-tools","title":"Interactive Learning Tools","text":"<ul> <li>Learning Graph - Visual map showing concept dependencies and learning pathways</li> <li>Glossary - Technical terms with precise definitions and examples</li> <li>FAQ - Frequently asked questions covering common misconceptions and practical guidance</li> </ul>"},{"location":"#interactive-visualizations-microsims","title":"Interactive Visualizations (MicroSims)","text":"<p>Explore abstract concepts through interactive visualizations:</p> <ul> <li>SQL Query Execution Plan Visualizer - See how indexes dramatically improve query performance</li> <li>Git Merge vs Rebase Interactive - Compare branching strategies side-by-side</li> <li>Star Schema vs Snowflake Comparison - Understand dimensional modeling trade-offs</li> <li>Database Normalization Journey - Step through normalization forms visually</li> <li>BigQuery Partitioning Cost Calculator - Optimize cloud data warehouse costs</li> <li>Slowly Changing Dimension Timeline - Compare SCD Type 1, 2, and 3 strategies</li> </ul>"},{"location":"#chapter-quizzes","title":"Chapter Quizzes","text":"<p>Test your knowledge with Bloom's taxonomy-aligned quizzes:</p> <ul> <li>Chapter 1 Quiz: Python &amp; SQL Foundations</li> <li>Chapter 2 Quiz: Git &amp; Docker</li> <li>Chapter 3 Quiz: Database Modeling</li> <li>Chapter 4 Quiz: Data Warehousing</li> </ul>"},{"location":"#target-audience","title":"Target Audience","text":"<p>This course is designed for:</p> <ul> <li>Bootcamp graduates looking to specialize in data engineering</li> <li>Software engineers wanting to transition to data-focused roles</li> <li>Full-stack developers curious about backend data infrastructure</li> <li>Self-taught programmers with solid coding fundamentals</li> </ul>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>Before starting this course, you should have:</p> <ul> <li>Proficiency in at least one programming language (Python preferred)</li> <li>Basic SQL knowledge (SELECT, JOIN, WHERE clauses)</li> <li>Comfort with command line basics</li> <li>Git fundamentals (clone, commit, push)</li> </ul>"},{"location":"#teaching-philosophy","title":"Teaching Philosophy","text":"<p>This course follows evidence-based learning principles:</p>"},{"location":"#concrete-before-abstract","title":"Concrete Before Abstract","text":"<p>Every concept starts with real-world examples before diving into theory. You'll see messy denormalized spreadsheets before learning normalization rules, and watch slow queries before understanding indexes.</p>"},{"location":"#intrinsic-motivation","title":"Intrinsic Motivation","text":"<p>We connect every concept to real problems data engineers face: slow queries that cost money, bad schemas that cause production bugs, and inefficient pipelines that waste resources.</p>"},{"location":"#low-floor-high-ceiling","title":"Low Floor, High Ceiling","text":"<p>Accessible entry points for beginners with unlimited depth for those who want to dive deeper. Reflection questions prompt critical thinking about trade-offs and edge cases.</p>"},{"location":"#socratic-coaching","title":"Socratic Coaching","text":"<p>Rather than just delivering content, we ask questions that guide you to discover insights: \"When would denormalization be justified?\" \"How would you decide between merge and rebase?\"</p>"},{"location":"#how-to-use-this-course","title":"How to Use This Course","text":""},{"location":"#for-self-paced-learning","title":"For Self-Paced Learning","text":"<ol> <li>Read chapters sequentially - Each chapter builds on previous concepts</li> <li>Complete \"Try It\" exercises as you encounter them</li> <li>Explore MicroSims to visualize abstract concepts</li> <li>Answer reflection questions before moving forward</li> <li>Take chapter quizzes to test comprehension</li> <li>Reference glossary and FAQ as needed</li> </ol>"},{"location":"#for-bootcamp-instructors","title":"For Bootcamp Instructors","text":"<p>This textbook complements live instruction:</p> <ul> <li>Flip the classroom - Students read content before class, use class time for labs</li> <li>Reduce cognitive load - Students aren't furiously taking notes during lectures</li> <li>Support struggling students - Async resource for catching up</li> <li>Enable office hours - Students can reference specific sections when asking questions</li> </ul> <p>See the Instructor Guide for assessment strategies and implementation guidance.</p>"},{"location":"#technology-stack","title":"Technology Stack","text":"<p>This course covers:</p> <p>Programming &amp; Development: - Python 3.10+ - SQL (PostgreSQL, MySQL) - Git &amp; GitHub - Linux/Bash - Docker &amp; Docker Compose</p> <p>Data Storage: - PostgreSQL (relational) - MongoDB (document NoSQL) - Redis (key-value store) - Google BigQuery (cloud data warehouse)</p>"},{"location":"#whats-next","title":"What's Next?","text":"<p>After completing Weeks 1-4, you'll be ready for:</p> <ul> <li>Weeks 5-6: Data Pipeline Fundamentals (ETL/ELT, Apache Airflow, data quality)</li> <li>Weeks 7-8: Big Data &amp; Distributed Systems (Apache Spark, Kafka, cloud processing)</li> <li>Weeks 9-10: Cloud Data Engineering (GCP deep dive, infrastructure as code, CI/CD)</li> <li>Weeks 11-12: Team Capstone Project (end-to-end data platform implementation)</li> </ul>"},{"location":"#course-information","title":"Course Information","text":"<p>Duration: 2 weeks (Weeks 1-4 of 12-week bootcamp) Time Commitment: 80-100 hours total Format: Self-paced textbook with interactive elements Level: College/Professional Last Updated: January 28, 2026</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to begin your data engineering journey?</p> <p>Start with Chapter 1: Python &amp; SQL Foundations \u2192</p> <p>Or explore the Course Description for detailed learning objectives and assessment information.</p> <p>Built with evidence-based learning science principles for bootcamp graduates transitioning to data engineering.</p>"},{"location":"course-description/","title":"Course Description: Data Engineering Bootcamp (Weeks 1-4)","text":""},{"location":"course-description/#overview","title":"Overview","text":"<p>The Data Engineering Bootcamp: Foundations &amp; Data Storage is an intensive course designed to transform software engineers into job-ready data engineers. This textbook covers the first 4 weeks of a comprehensive 12-week bootcamp program.</p>"},{"location":"course-description/#course-metadata","title":"Course Metadata","text":"Attribute Details Title Data Engineering Bootcamp: Foundations &amp; Data Storage Duration 2 weeks (Weeks 1-4 of 12-week program) Time Commitment 80-100 hours total Format Self-paced interactive textbook Delivery Git-friendly markdown via GitHub Pages Target Audience Software engineers transitioning to data engineering Level College/Professional Prerequisites Python/programming proficiency, basic SQL, Git basics"},{"location":"course-description/#learning-outcomes","title":"Learning Outcomes","text":"<p>By completing this course, students will be able to:</p>"},{"location":"course-description/#week-1-2-foundations-environment-setup","title":"Week 1-2: Foundations &amp; Environment Setup","text":"<p>Python Proficiency: - Write efficient Python code using list comprehensions, generators, and context managers for data processing tasks - Implement decorators and advanced Python features for production pipelines</p> <p>SQL Mastery: - Construct complex SQL queries using window functions, CTEs, and subqueries to analyze multi-table datasets - Optimize query performance through execution plan analysis and indexing strategies</p> <p>DevOps Fundamentals: - Execute Linux command-line operations for file manipulation, process management, and system navigation - Implement Git workflows including branching, merging, and pull requests for collaborative development - Build containerized applications using Docker and Docker Compose for consistent development environments</p> <p>Conceptual Understanding: - Explain the differences between OLTP and OLAP systems and their appropriate use cases</p>"},{"location":"course-description/#week-3-4-data-storage-modeling","title":"Week 3-4: Data Storage &amp; Modeling","text":"<p>Database Design: - Design normalized relational database schemas following 3NF principles for transactional systems - Design dimensional models using star and snowflake schemas for analytical workloads - Implement database indexes and query optimization techniques to improve query performance</p> <p>Storage Selection: - Compare relational databases vs NoSQL databases and select appropriate storage based on use case requirements - Evaluate trade-offs between data normalization and denormalization in different contexts - Explain data lake architecture and its role in modern data platforms</p> <p>Cloud Data Warehousing: - Construct BigQuery tables and queries for cloud-based data warehousing - Implement partitioning and clustering strategies to optimize BigQuery performance and costs - Design slowly changing dimension (SCD) implementations for historical tracking</p>"},{"location":"course-description/#blooms-taxonomy-alignment","title":"Bloom's Taxonomy Alignment","text":"<p>The course is designed with intentional cognitive progression:</p> Week Primary Bloom's Levels Secondary Levels Cognitive Demand 1-2 Apply Remember, Understand Low-Medium 3-4 Apply, Analyze Understand, Create Medium"},{"location":"course-description/#cognitive-level-distribution","title":"Cognitive Level Distribution","text":"<ul> <li>Remember (20%): Define key terms, list components, identify concepts</li> <li>Understand (30%): Explain differences, describe processes, summarize approaches</li> <li>Apply (33%): Implement solutions, construct queries, execute workflows</li> <li>Analyze (17%): Compare approaches, evaluate trade-offs, debug issues</li> </ul>"},{"location":"course-description/#course-structure","title":"Course Structure","text":""},{"location":"course-description/#4-comprehensive-chapters","title":"4 Comprehensive Chapters","text":""},{"location":"course-description/#chapter-1-python-sql-foundations","title":"Chapter 1: Python &amp; SQL Foundations","text":"<p>Learning Focus: Apply advanced Python features and SQL techniques</p> <p>Key Topics: - List comprehensions, generators, context managers, decorators - SQL window functions for analytics (ROW_NUMBER, RANK, running totals) - Common Table Expressions (CTEs) for complex multi-step queries - Query optimization principles</p> <p>Hands-on Examples: - Inefficient CSV processing improved with generators - E-commerce analytics with window functions - Multi-step data transformations with CTEs</p>"},{"location":"course-description/#chapter-2-devops-foundations-git-docker","title":"Chapter 2: DevOps Foundations - Git &amp; Docker","text":"<p>Learning Focus: Apply collaborative development and containerization</p> <p>Key Topics: - Git branching strategies (feature branches, release branches) - Merge vs rebase workflows and conflict resolution - Docker fundamentals (images, containers, Dockerfiles) - Docker Compose for multi-container applications</p> <p>Hands-on Examples: - Resolving merge conflicts in data pipeline code - Containerizing Python ETL scripts - Multi-container setup with application + PostgreSQL database</p>"},{"location":"course-description/#chapter-3-database-design-modeling","title":"Chapter 3: Database Design &amp; Modeling","text":"<p>Learning Focus: Analyze and design efficient database schemas</p> <p>Key Topics: - Database normalization (1NF, 2NF, 3NF, BCNF) - Denormalization strategies for analytical workloads - Index types (B-tree, hash, composite) and selection - Query execution plans and optimization - NoSQL databases (MongoDB, Redis) and CAP theorem</p> <p>Hands-on Examples: - Normalizing messy e-commerce spreadsheet step-by-step - Identifying and fixing data anomalies - Choosing indexes for common query patterns - Deciding when denormalization is justified</p>"},{"location":"course-description/#chapter-4-data-warehousing-bigquery","title":"Chapter 4: Data Warehousing &amp; BigQuery","text":"<p>Learning Focus: Create dimensional models and optimize cloud warehouses</p> <p>Key Topics: - OLTP vs OLAP system design - Dimensional modeling (fact tables, dimension tables) - Star schema vs snowflake schema comparison - Slowly Changing Dimensions (Types 1, 2, 3) - BigQuery architecture and optimization - Partitioning and clustering strategies</p> <p>Hands-on Examples: - Designing star schema for e-commerce analytics - Implementing SCD Type 2 for customer dimension - BigQuery partitioning cost comparison ($25 \u2192 $0.75 with optimization) - Comparing snowflake vs star schema trade-offs</p>"},{"location":"course-description/#learning-resources","title":"Learning Resources","text":""},{"location":"course-description/#interactive-learning-graph","title":"Interactive Learning Graph","text":"<p>A visual knowledge map showing: - Foundational concepts (entry points with no prerequisites) - Dependency relationships (what concepts build on others) - Taxonomy categories (FOUND, BASIC, INTER, ADV, APP) - Multiple learning pathways for flexible progression</p>"},{"location":"course-description/#comprehensive-glossary","title":"Comprehensive Glossary","text":"<p>ISO 11179-compliant definitions covering: - Python features (comprehensions, generators, context managers, decorators) - SQL concepts (window functions, CTEs, indexes, execution plans) - Git operations (branch, merge, rebase, pull request) - Docker terminology (container, image, Dockerfile, Compose) - Database design (normalization, denormalization, ACID, CAP theorem) - Data warehousing (fact table, dimension table, star schema, SCD) - BigQuery optimization (partitioning, clustering)</p> <p>Each term includes: - Precise 20-50 word definition - Concrete example from data engineering domain - Alphabetical organization for quick reference</p>"},{"location":"course-description/#frequently-asked-questions","title":"Frequently Asked Questions","text":"<p>Organized into 4 categories: 1. Conceptual Clarifications - \"What's the difference between OLTP and OLAP?\" 2. Common Misconceptions - \"Do I always need to normalize to 3NF?\" 3. Practical Applications - \"When should I use MongoDB vs PostgreSQL?\" 4. Prerequisites &amp; Next Steps - \"What Python skills do I need before starting?\"</p>"},{"location":"course-description/#interactive-microsims","title":"Interactive MicroSims","text":"<p>Detailed specifications for interactive visualizations:</p> <ol> <li>SQL Query Execution Plan Visualizer - See dramatic performance improvement with proper indexing</li> <li>Git Merge vs Rebase Interactive - Compare branching strategies side-by-side</li> <li>Star Schema vs Snowflake Comparison - Visualize dimensional modeling trade-offs</li> <li>Database Normalization Journey - Step through normalization forms with real data</li> <li>BigQuery Partitioning Cost Calculator - Calculate cost reduction with partitioning</li> <li>Slowly Changing Dimension Timeline - Compare SCD types with timeline visualization</li> </ol>"},{"location":"course-description/#chapter-quizzes","title":"Chapter Quizzes","text":"<ul> <li>Assessment questions for each chapter</li> <li>Bloom's taxonomy alignment across cognitive levels</li> <li>Balanced answer distribution (A/B/C/D roughly equal)</li> <li>Detailed explanations for each answer</li> <li>Concept mapping to learning graph</li> </ul>"},{"location":"course-description/#assessment-framework","title":"Assessment Framework","text":""},{"location":"course-description/#formative-assessment-throughout-course","title":"Formative Assessment (Throughout Course)","text":"<ul> <li>\"Try It\" exercises embedded in chapters</li> <li>Reflection questions prompting deeper thinking</li> <li>Practice quizzes with immediate feedback</li> <li>Self-assessment against learning objectives</li> </ul>"},{"location":"course-description/#summative-assessment-end-of-weeks-1-4","title":"Summative Assessment (End of Weeks 1-4)","text":"<ul> <li>Chapter quizzes (40 questions total)</li> <li>Portfolio projects:</li> <li>Week 1-2: Build ETL pipeline with Python + SQL optimization</li> <li>Week 3-4: Design dimensional model for e-commerce analytics</li> <li>Code review participation (for bootcamp cohorts)</li> </ul>"},{"location":"course-description/#assessment-rubrics","title":"Assessment Rubrics","text":"<p>Knowledge Assessment (Quizzes): - 70%+ correct: Proficient (ready for next weeks) - 60-69%: Developing (review weak areas) - &lt;60%: Needs intervention (1-on-1 support recommended)</p> <p>Project Assessment: - Technical implementation (40%) - Code quality and documentation (30%) - Design decisions and justification (20%) - Testing and error handling (10%)</p>"},{"location":"course-description/#teaching-methodology","title":"Teaching Methodology","text":""},{"location":"course-description/#evidence-based-learning-principles","title":"Evidence-Based Learning Principles","text":"<p>1. Concrete Before Abstract - Start with messy real-world data before teaching theory - Show production problems before explaining solutions - Use e-commerce, retail, and streaming platform examples</p> <p>2. Intrinsic Motivation - Connect concepts to real costs: \"Slow queries cost $thousands/month\" - Show production bugs caused by bad schema design - Demonstrate career-relevant skills with immediate applicability</p> <p>3. Low Floor, High Ceiling - Accessible entry points for those with basic programming - Unlimited depth through reflection questions and extensions - Optional advanced topics for those who want more</p> <p>4. Socratic Coaching - Reflection questions guide discovery: \"When would denormalization be justified?\" - No spoon-feeding of answers - Encourage critical thinking about trade-offs</p> <p>5. Active Learning - \"Try It\" exercises for immediate practice - Interactive MicroSims for exploration - Real-world scenarios requiring decision-making</p>"},{"location":"course-description/#instructional-strategies-by-blooms-level","title":"Instructional Strategies by Bloom's Level","text":"<p>Remember &amp; Understand (Weeks 1-2 emphasis): - Clear explanations with examples - Glossary for quick reference - Concept mapping with learning graph</p> <p>Apply (Weeks 1-4 emphasis): - Step-by-step walkthroughs - \"Try It\" exercises with immediate practice - Code examples with detailed comments</p> <p>Analyze (Weeks 3-4 emphasis): - Comparative analysis (OLTP vs OLAP, star vs snowflake) - Trade-off discussions - Debugging scenarios and optimization challenges</p>"},{"location":"course-description/#technology-stack","title":"Technology Stack","text":""},{"location":"course-description/#programming-languages-tools","title":"Programming Languages &amp; Tools","text":"<ul> <li>Python 3.10+ - Primary programming language</li> <li>SQL - PostgreSQL, MySQL dialects</li> <li>Bash - Shell scripting and command-line tools</li> </ul>"},{"location":"course-description/#development-environment","title":"Development Environment","text":"<ul> <li>Git &amp; GitHub - Version control and collaboration</li> <li>Docker &amp; Docker Compose - Containerization</li> <li>VS Code / PyCharm - Recommended IDEs</li> <li>Jupyter Notebooks - Exploratory analysis</li> </ul>"},{"location":"course-description/#data-storage-systems","title":"Data Storage Systems","text":"<ul> <li>PostgreSQL - Relational database (OLTP)</li> <li>MongoDB - Document database (NoSQL)</li> <li>Redis - Key-value store (caching)</li> <li>Google BigQuery - Cloud data warehouse (OLAP)</li> </ul>"},{"location":"course-description/#prerequisites","title":"Prerequisites","text":""},{"location":"course-description/#minimum-requirements","title":"Minimum Requirements","text":"<ul> <li>Programming: Proficiency in at least one language (Python preferred)</li> <li>SQL: Basic queries (SELECT, JOIN, WHERE)</li> <li>Command Line: Navigate directories, run commands</li> <li>Git: Clone, commit, push operations</li> <li>No formal degree required</li> </ul>"},{"location":"course-description/#recommended-preparation","title":"Recommended Preparation","text":"<p>If you're rusty on any of these, review before starting: - Python functions, loops, lists, dictionaries - SQL joins and aggregations - Basic Linux commands - Git branching and merging</p>"},{"location":"course-description/#pre-course-resources-optional","title":"Pre-Course Resources (Optional)","text":"<ul> <li>Python refresher: Official Python tutorial</li> <li>SQL practice: SQLBolt or Mode Analytics SQL Tutorial</li> <li>Docker tutorial: Docker Getting Started Guide</li> <li>Linux basics: Linux Journey</li> </ul>"},{"location":"course-description/#what-comes-after-this-course","title":"What Comes After This Course?","text":"<p>This course prepares you for:</p>"},{"location":"course-description/#weeks-5-6-data-pipeline-fundamentals","title":"Weeks 5-6: Data Pipeline Fundamentals","text":"<ul> <li>ETL vs ELT patterns</li> <li>Apache Airflow workflow orchestration</li> <li>Data quality validation with Great Expectations</li> <li>Error handling and monitoring</li> </ul>"},{"location":"course-description/#weeks-7-8-big-data-distributed-systems","title":"Weeks 7-8: Big Data &amp; Distributed Systems","text":"<ul> <li>Apache Spark and PySpark</li> <li>Distributed computing concepts</li> <li>Kafka for streaming data</li> <li>Google Cloud Dataproc</li> </ul>"},{"location":"course-description/#weeks-9-10-cloud-data-engineering","title":"Weeks 9-10: Cloud Data Engineering","text":"<ul> <li>GCP services (Dataflow, Composer, Pub/Sub)</li> <li>Infrastructure as Code with Terraform</li> <li>CI/CD for data pipelines</li> <li>Data governance and security</li> </ul>"},{"location":"course-description/#weeks-11-12-team-capstone-project","title":"Weeks 11-12: Team Capstone Project","text":"<ul> <li>End-to-end data platform implementation</li> <li>Agile/Scrum methodology</li> <li>Code reviews and collaboration</li> <li>Technical presentations</li> </ul>"},{"location":"course-description/#success-metrics","title":"Success Metrics","text":"<p>Students who complete this course will be able to: - \u2705 Build production-quality Python scripts for data processing - \u2705 Design and optimize relational databases - \u2705 Create dimensional models for analytics - \u2705 Containerize data applications with Docker - \u2705 Collaborate effectively using Git workflows - \u2705 Choose appropriate storage solutions for different use cases - \u2705 Optimize cloud data warehouse queries and costs</p>"},{"location":"course-description/#course-credits","title":"Course Credits","text":"<p>Design: Based on 12-week Data Engineering Bootcamp curriculum Format: Interactive textbook with MkDocs Material Pedagogy: Evidence-based learning science principles Version: 1.0 (January 28, 2026)</p>"},{"location":"course-description/#getting-started","title":"Getting Started","text":"<p>Ready to begin?</p> <p>Start with Chapter 1: Python &amp; SQL Foundations \u2192</p> <p>Or return to the Course Home Page.</p>"},{"location":"faq/","title":"Frequently Asked Questions - Data Engineering Bootcamp Weeks 1-4","text":""},{"location":"faq/#conceptual-clarifications","title":"Conceptual Clarifications","text":""},{"location":"faq/#what-is-the-difference-between-oltp-and-olap-systems","title":"What is the difference between OLTP and OLAP systems?","text":"<p>OLTP (Online Transaction Processing) systems are optimized for frequent, fast write and update operations like processing customer orders or updating inventory in real-time. OLAP (Online Analytical Processing) systems are designed for complex, read-heavy analytical queries across large datasets, prioritizing query speed over write performance. Data engineers typically extract data from OLTP systems (e.g., PostgreSQL for an e-commerce platform) and load it into OLAP systems (e.g., BigQuery data warehouse) for analytics. See Week 3 for detailed comparison and use cases.</p>"},{"location":"faq/#how-do-star-schemas-differ-from-snowflake-schemas","title":"How do star schemas differ from snowflake schemas?","text":"<p>Star schemas have a single fact table at the center connected directly to denormalized dimension tables, creating a simple, flat structure that speeds up queries and reduces joins. Snowflake schemas normalize dimension tables into multiple related tables, consuming more storage and requiring additional joins but reducing data redundancy. Use star schemas when query performance is critical and storage costs are low (typical in data warehouses); use snowflake schemas when storage is constrained or dimension tables are shared across multiple fact tables. See Week 3 for architectural examples.</p>"},{"location":"faq/#whats-the-difference-between-normalization-and-denormalization","title":"What's the difference between normalization and denormalization?","text":"<p>Normalization organizes data to eliminate redundancy and ensure data consistency, critical for transactional systems (OLTP) where multiple writes could otherwise create conflicts. Denormalization intentionally adds redundancy by combining related data into single tables or repeating columns, optimizing for query performance in analytical systems (OLAP). A normalized e-commerce database stores customer info once; a denormalized warehouse might repeat customer city in every order row to avoid joins. See Week 4 for normalization principles (1NF, 2NF, 3NF) and when to apply each approach.</p>"},{"location":"faq/#whats-the-difference-between-list-comprehensions-and-generators-in-python","title":"What's the difference between list comprehensions and generators in Python?","text":"<p>List comprehensions create the entire list in memory immediately, using [x for x in range(1000000)], making them fast for small datasets but memory-intensive for large ones. Generators produce values one at a time on demand using (x for x in range(1000000)), consuming minimal memory but slightly slower per-iteration. For data engineering, use generators when processing massive files or streaming data; use list comprehensions when you need immediate access to all values. See Week 1 for implementation examples in data processing scripts.</p>"},{"location":"faq/#how-do-indexes-improve-database-query-performance","title":"How do indexes improve database query performance?","text":"<p>An index is a separate data structure (typically B-tree) that maps column values to row locations, allowing the database to find matching rows without scanning every row. Without an index on a \"customer_id\" column in a table with 100 million rows, the database must check all 100 million rows (slow); with an index, it jumps directly to matching rows (fast). Indexes trade write speed and storage for read speed, ideal for columns frequently used in WHERE clauses or JOINs. See Week 4 for composite indexes and execution plan analysis.</p>"},{"location":"faq/#whats-the-difference-between-git-merge-and-git-rebase","title":"What's the difference between git merge and git rebase?","text":"<p>Git merge combines two branches by creating a new \"merge commit\" that points to both parent commits, preserving complete history and making it clear when branches diverged. Git rebase replays one branch's commits on top of another, creating a linear history without merge commits, but losing visibility of when branches were created. Use merge in shared repositories (safer, easier to understand conflicts) and rebase in feature branches (cleaner history). See Week 2 for Git workflows and team collaboration best practices.</p>"},{"location":"faq/#what-are-context-managers-and-why-are-they-important-for-data-engineering","title":"What are context managers and why are they important for data engineering?","text":"<p>Context managers (using the <code>with</code> statement) ensure resources like file handles, database connections, or locks are properly acquired and released, even if errors occur. Instead of manually opening and closing a file, <code>with open('file.csv') as f: process(f)</code> guarantees the file closes automatically. Data engineering scripts often process large files or query massive databases, making context managers essential for preventing resource leaks. See Week 1 for context manager patterns in ETL scripts.</p>"},{"location":"faq/#how-does-docker-relate-to-virtual-machines","title":"How does Docker relate to virtual machines?","text":"<p>Docker containers are lightweight, isolated environments that share the host OS kernel, while virtual machines include an entire OS, making them larger and slower. Docker is ideal for data engineering pipelines because you can package Python, PostgreSQL, and your code together, ensuring it runs identically on your laptop, a colleague's machine, and production servers. Use containers to version entire development environments. See Week 2 for Docker fundamentals and practical container examples.</p>"},{"location":"faq/#common-misconceptions","title":"Common Misconceptions","text":""},{"location":"faq/#do-i-always-need-to-normalize-my-database-to-3nf","title":"Do I always need to normalize my database to 3NF?","text":"<p>No\u2014normalization is a tool designed for transactional consistency, not a universal requirement. OLTP systems (banks, inventory) must normalize to prevent data anomalies when multiple concurrent writes happen, while OLAP systems (data warehouses) intentionally denormalize to reduce joins and speed up analytical queries. A data warehouse fact table might contain customer city repeated in millions of rows; this is a design choice, not a mistake. See Week 4 for normalization principles and when to denormalize intentionally.</p>"},{"location":"faq/#is-a-nosql-database-always-faster-than-a-relational-database","title":"Is a NoSQL database always faster than a relational database?","text":"<p>No\u2014speed depends entirely on your access patterns. NoSQL databases (MongoDB, Redis) excel at horizontal scaling and flexible schemas, but relational databases (PostgreSQL) are optimized for complex multi-table queries and strong consistency. A MongoDB query that scans millions of documents can be slower than a PostgreSQL query using indexes, and PostgreSQL can handle billions of rows efficiently. Choose based on your data shape and query patterns, not blanket assumptions. See Week 3 for comparison frameworks.</p>"},{"location":"faq/#should-i-always-use-git-rebase-instead-of-merge","title":"Should I always use git rebase instead of merge?","text":"<p>No\u2014while rebase creates cleaner linear history, merge is safer for shared branches where multiple people commit simultaneously. Rebasing public branches can cause conflicts for teammates; merging is more collaborative. Use rebase on feature branches you own, and merge when integrating into shared branches like main. See Week 2 for team Git workflows that mix both strategies.</p>"},{"location":"faq/#do-i-need-to-denormalize-immediately-to-improve-query-performance","title":"Do I need to denormalize immediately to improve query performance?","text":"<p>Not necessarily\u2014before denormalizing, first optimize with indexes, better SQL queries (window functions, CTEs), and query execution plan analysis. Many \"slow\" queries are actually slow queries, not slow schemas. Denormalization is a last resort after exhausting indexing and query optimization. See Week 4 for the optimization process: 1) Analyze execution plans, 2) Add indexes, 3) Rewrite queries, 4) Then denormalize if needed.</p>"},{"location":"faq/#is-it-true-that-generators-always-consume-less-memory-than-list-comprehensions","title":"Is it true that generators always consume less memory than list comprehensions?","text":"<p>Generators do consume less memory for large datasets, but they have trade-offs: you can only iterate once, they're slower per-item, and some operations (like sorting) force them into memory anyway. Generators are perfect for streaming data through a pipeline; list comprehensions are better when you need to access elements multiple times. See Week 1 for performance comparisons in realistic data processing scenarios.</p>"},{"location":"faq/#will-docker-containers-always-make-my-application-portable-across-different-systems","title":"Will Docker containers always make my application portable across different systems?","text":"<p>Docker containers are portable across systems running Docker, but not without limitations: Windows container images won't run natively on Linux (though WSL 2 helps), ARM containers won't run on x86 hardware, and GPU support requires nvidia-docker. Docker solves the \"works on my machine\" problem for development environments and Linux servers, the most common data engineering scenario. See Week 2 for real-world container deployment considerations.</p>"},{"location":"faq/#practical-applications","title":"Practical Applications","text":""},{"location":"faq/#when-should-i-use-postgresql-vs-mongodb-for-a-data-engineering-project","title":"When should I use PostgreSQL vs MongoDB for a data engineering project?","text":"<p>Use PostgreSQL when you have well-defined schemas, need complex multi-table queries (JOINs), and require strong consistency (financial transactions, inventory systems). Use MongoDB when you have flexible, evolving schemas, deeply nested data (e.g., user profiles with varying attributes), or need easy horizontal scaling for write-heavy workloads. For most data engineering pipelines at Weeks 1-4 level, PostgreSQL is the default choice due to its mature ecosystem and SQL compatibility. See Week 3 for detailed comparison and real-world examples (e-commerce vs user-generated content platforms).</p>"},{"location":"faq/#how-do-i-decide-whether-to-partition-my-bigquery-tables","title":"How do I decide whether to partition my BigQuery tables?","text":"<p>Partition BigQuery tables when they're larger than 1 GB and commonly filtered by specific columns (date, region, customer_id), as partitioning drastically reduces data scanned and costs. Partition by ingestion time (default) for time-series data, by a date column for historical snapshots, or by categorical columns for logical groupings. Avoid partitioning small tables or columns with very high cardinality (millions of unique values). See Week 3 for BigQuery architecture and cost optimization examples.</p>"},{"location":"faq/#when-should-i-use-a-context-manager-in-python-for-data-processing","title":"When should I use a context manager in Python for data processing?","text":"<p>Use context managers whenever you work with resources that must be cleaned up: opening files (<code>with open()</code>), connecting to databases (<code>with connection()</code>), acquiring locks, or managing temporary resources. In data engineering, context managers prevent file handle leaks in long-running ETL jobs and ensure database connections close even if exceptions occur. See Week 1 for patterns in production data pipelines.</p>"},{"location":"faq/#how-do-i-choose-between-star-schema-and-snowflake-schema-for-my-data-warehouse","title":"How do I choose between star schema and snowflake schema for my data warehouse?","text":"<p>Choose star schema if your dimensions are small (under 10 million rows), storage costs aren't critical, and queries must be extremely fast (typical analytical queries). Choose snowflake schema if dimensions are very large, shared across multiple fact tables, or storage costs matter (cost-conscious data lakes). For an e-commerce data warehouse, star schema works well; for an enterprise data warehouse shared across dozens of teams, snowflake schema reduces redundancy. See Week 3 for decision frameworks and real-world examples.</p>"},{"location":"faq/#how-do-i-identify-which-columns-need-indexes","title":"How do I identify which columns need indexes?","text":"<p>Prioritize indexes on columns in WHERE clauses, JOIN conditions, and ORDER BY statements that filter large tables. Create composite indexes for frequently used multi-column filters. Check query execution plans to see which columns cause full table scans. In an e-commerce database, index \"order_date\" and \"customer_id\" on orders table, but not on columns with few unique values (gender) or rarely filtered columns. See Week 4 for execution plan analysis and indexing strategies.</p>"},{"location":"faq/#what-python-features-are-most-important-for-writing-efficient-data-processing-scripts","title":"What Python features are most important for writing efficient data processing scripts?","text":"<p>Master list comprehensions (faster than loops), generators (memory-efficient for streaming), and context managers (resource safety)\u2014these three features distinguish efficient data engineering code from amateur scripts. Decorators are useful for logging and validation. Avoid storing entire datasets in memory when possible; process in chunks. See Week 1 for practical examples in ETL pipelines.</p>"},{"location":"faq/#how-do-i-handle-schema-evolution-when-dimensional-tables-change-over-time","title":"How do I handle schema evolution when dimensional tables change over time?","text":"<p>Implement Slowly Changing Dimension (SCD) Type 2 for time-tracking changes: add \"start_date\" and \"end_date\" columns, keeping historical records with version flags. Use SCD Type 1 (overwrite) only when history doesn't matter. An e-commerce product dimension might track when prices change; SCD Type 2 keeps all price versions. See Week 3 for SCD implementation strategies and when to use each type.</p>"},{"location":"faq/#should-i-load-data-directly-into-a-production-data-warehouse-or-use-a-staging-area","title":"Should I load data directly into a production data warehouse or use a staging area?","text":"<p>Always use a staging area for data validation and transformation before loading production warehouses. Extract raw data \u2192 stage \u2192 validate \u2192 transform \u2192 load (ELTL pattern). This prevents corrupted or incomplete data from affecting analytics and allows you to debug pipeline failures without touching production. See Week 2-3 for ETL pipeline patterns and error handling.</p>"},{"location":"faq/#how-do-i-optimize-slow-sql-queries-in-a-data-warehouse","title":"How do I optimize slow SQL queries in a data warehouse?","text":"<p>Follow this sequence: 1) Use EXPLAIN ANALYZE to find the bottleneck, 2) Add indexes on filtered/joined columns, 3) Rewrite using window functions or CTEs instead of self-joins, 4) Denormalize if optimization can't solve it. Most slow queries are actually slow SQL, not slow schemas. A query joining six tables might be 100x faster with proper indexing. See Week 2 for SQL optimization techniques and Week 4 for query execution plans.</p>"},{"location":"faq/#whats-the-best-way-to-handle-large-file-imports-in-python-without-running-out-of-memory","title":"What's the best way to handle large file imports in Python without running out of memory?","text":"<p>Use chunking strategies: read files in batches (pandas.read_csv with chunksize), use generators to process rows one at a time, or use streaming database inserts instead of loading entire files. For a 10 GB CSV file, chunking into 100 MB chunks lets you process with minimal memory. See Week 1 for generator patterns and context managers in file processing pipelines.</p>"},{"location":"faq/#how-do-i-decide-between-a-data-lake-and-a-data-warehouse","title":"How do I decide between a data lake and a data warehouse?","text":"<p>Data lakes store all raw data in any format (structured, unstructured) for flexibility and scalability, but require significant governance to remain useful. Data warehouses store cleaned, modeled data optimized for specific queries, providing reliability and performance but less flexibility. Modern architectures use both: data lake for raw storage, data warehouse for analytics. See Week 3 for architectural comparison and enterprise data platform patterns.</p>"},{"location":"faq/#prerequisites-next-steps","title":"Prerequisites &amp; Next Steps","text":""},{"location":"faq/#what-python-skills-do-i-need-before-starting-this-bootcamp","title":"What Python skills do I need before starting this bootcamp?","text":"<p>You should be comfortable with basic Python (variables, loops, functions, lists, dictionaries) and have written at least several hundred lines of code in a project. This bootcamp assumes programming maturity and teaches advanced Python for data engineering: generators, list comprehensions, context managers, and decorators. If you're rusty, review functions, data structures, and exception handling before Week 1. See the Prerequisites section in course introduction.</p>"},{"location":"faq/#what-sql-knowledge-is-required-to-start","title":"What SQL knowledge is required to start?","text":"<p>You should know basic SELECT queries, JOINs (INNER, LEFT, RIGHT), WHERE conditions, and simple aggregations (GROUP BY, COUNT, SUM). Weeks 1-2 assume this foundation and immediately jump to advanced SQL: window functions, CTEs, subqueries, and optimization. If you haven't written a LEFT JOIN or GROUP BY query, spend a few hours on SQL basics first. See Week 1-2 curriculum for topics covered.</p>"},{"location":"faq/#what-comes-after-learning-database-modeling-in-week-4","title":"What comes after learning database modeling in Week 4?","text":"<p>Weeks 5-8 (not covered here) typically cover data pipelines, orchestration (Airflow), real-time streaming (Kafka), and building production systems. You'll apply database modeling knowledge to design schemas that data pipelines can feed. The modeling skills from Weeks 1-4 form the foundation for understanding how to extract, transform, and load data into warehouses. See course roadmap for full curriculum.</p>"},{"location":"faq/#do-i-need-cloud-experience-before-learning-bigquery","title":"Do I need cloud experience before learning BigQuery?","text":"<p>No\u2014Week 3 teaches BigQuery as a cloud data warehouse without assuming prior cloud experience. You'll learn BigQuery-specific concepts (partitioning, clustering, pricing model) from scratch. However, basic Unix/Linux command-line comfort helps with cloud environment navigation. If you've never used a terminal, review Linux basics from Week 2. See Week 3 for BigQuery introduction.</p>"},{"location":"faq/#how-do-these-weeks-1-4-foundations-connect-to-real-data-engineering-jobs","title":"How do these Weeks 1-4 foundations connect to real data engineering jobs?","text":"<p>These weeks cover the core skills used daily in data engineering roles: writing Python for ETL (Week 1-2), designing databases and warehouses (Week 3-4), and using Git/Docker for production code. You'll understand OLTP vs OLAP systems, design schemas, optimize queries, and work with both relational and NoSQL databases. Weeks 1-4 provide the foundation for building actual data pipelines, orchestrating workflows, and building production systems in later weeks. See course learning outcomes and job market alignment documentation.</p>"},{"location":"faq/#what-resources-help-me-practice-these-skills-after-the-bootcamp","title":"What resources help me practice these skills after the bootcamp?","text":"<p>Practice SQL on LeetCode or HackerRank (SQL problems), build mini ETL projects locally with PostgreSQL and Python, contribute to open-source data engineering projects on GitHub, and replicate tutorial data warehouse designs in BigQuery's free tier. The best practice is building small projects like modeling an e-commerce database, writing a Python script to populate it, then querying it with complex SQL. See course project assignments for starter ideas.</p>"},{"location":"faq/#what-developer-tools-should-i-have-installed-before-starting-week-1","title":"What developer tools should I have installed before starting Week 1?","text":"<p>You'll need Python 3.10+, PostgreSQL or MySQL, git, Docker, a code editor (VS Code, PyCharm), and a terminal. The course provides setup guides. If starting on macOS/Linux, most tools are easy to install; Windows users should use WSL 2 (Windows Subsystem for Linux). Don't worry about BigQuery setup until Week 3. See Week 2 for detailed environment setup guide.</p>"},{"location":"faq/#how-do-git-and-docker-skills-from-weeks-1-2-apply-to-later-data-engineering-work","title":"How do git and Docker skills from Weeks 1-2 apply to later data engineering work?","text":"<p>Git skills ensure you can collaborate on data pipelines in team environments and track changes to SQL schemas and Python code. Docker containerizes entire environments (Python, PostgreSQL, libraries) so pipelines run identically on laptops and production servers. Later weeks build on these foundations by using Docker to containerize data pipelines and Git to version data models. These aren't optional; they're essential for professional work. See Week 1-2 assessments.</p>"},{"location":"faq/#will-i-be-ready-for-a-data-engineering-internship-or-job-after-week-4","title":"Will I be ready for a data engineering internship or job after Week 4?","text":"<p>Week 4 completion means you understand foundational concepts (OLTP vs OLAP, schemas, normalization, basic optimization) and can work with relational databases and cloud warehouses. You'll be competitive for junior data engineer roles focusing on data warehouse design and analytics, but typical roles also require pipeline orchestration and real-time systems (Weeks 5-8+). After Week 4, you have strong fundamentals; after completing the full bootcamp, you're job-ready. See course completion and career outcomes documentation.</p>"},{"location":"glossary/","title":"Glossary: Data Engineering Bootcamp - Weeks 1-4","text":"<p>A comprehensive glossary of key terms and concepts for the Data Engineering Bootcamp covering foundations, data storage, and data modeling.</p>"},{"location":"glossary/#a","title":"A","text":""},{"location":"glossary/#acid-properties","title":"ACID Properties","text":"<p>A set of four guarantees (Atomicity, Consistency, Isolation, Durability) that ensure database transactions are processed reliably and maintain data integrity even in the event of errors or failures.</p> <p>Example: A bank transfer that debits one account and credits another must either complete fully or not at all, ensuring the database never loses money.</p>"},{"location":"glossary/#apache-airflow","title":"Apache Airflow","text":"<p>An open-source workflow orchestration platform that allows data engineers to programmatically author, schedule, and monitor data pipelines as Directed Acyclic Graphs (DAGs).</p> <p>Example: A daily ETL job that extracts sales data from transactional systems, transforms it, and loads it into a data warehouse can be scheduled and monitored in Airflow.</p>"},{"location":"glossary/#atomicity","title":"Atomicity","text":"<p>A database transaction property ensuring that an operation completes entirely or not at all, with no partial updates or inconsistent intermediate states.</p> <p>Example: Transferring funds between accounts either fully succeeds or fully rolls back; it cannot leave one account debited without crediting the other.</p>"},{"location":"glossary/#b","title":"B","text":""},{"location":"glossary/#b-tree-index","title":"B-tree Index","text":"<p>A self-balancing tree data structure that maintains sorted data and enables efficient logarithmic-time search, insertion, and deletion operations in databases.</p> <p>Example: A customer_id B-tree index on a one-million-row table allows finding a customer's records in approximately 20 comparisons instead of scanning all rows.</p>"},{"location":"glossary/#batch-processing","title":"Batch Processing","text":"<p>A computational approach that collects data over time, groups it into batches, and processes them together at scheduled intervals rather than immediately upon arrival.</p> <p>Example: Nightly ETL jobs that aggregate hourly website logs into daily summary tables exemplify batch processing compared to real-time streaming.</p>"},{"location":"glossary/#bigquery","title":"BigQuery","text":"<p>Google Cloud's fully managed, serverless data warehouse that enables fast SQL analysis of large datasets with automatic scaling and separate compute-storage billing.</p> <p>Example: Querying 10 billion e-commerce transactions using BigQuery costs a fraction of traditional data warehouses due to columnar storage and query-only pricing.</p>"},{"location":"glossary/#business-rule","title":"Business Rule","text":"<p>A specific constraint, requirement, or operational guideline that dictates how data should be validated, processed, or interpreted within an organization.</p> <p>Example: \"All customer ages must be between 18 and 120\" and \"monthly revenue cannot decrease by more than 10%\" are business rules that data models must enforce.</p>"},{"location":"glossary/#c","title":"C","text":""},{"location":"glossary/#cap-theorem","title":"CAP Theorem","text":"<p>A fundamental principle stating that distributed systems can guarantee at most two of three properties simultaneously: Consistency, Availability, and Partition tolerance.</p> <p>Example: MongoDB prioritizes Availability and Partition tolerance over strong Consistency, allowing reads from replicas even if the primary is temporarily unreachable.</p>"},{"location":"glossary/#clustering-database","title":"Clustering (Database)","text":"<p>A data organization technique in databases like BigQuery that physically groups rows with similar values in specified columns to optimize query performance.</p> <p>Example: Clustering a sales table by region_id allows range queries on specific regions to read significantly fewer blocks, improving query speed.</p>"},{"location":"glossary/#column-oriented-storage","title":"Column-oriented Storage","text":"<p>A database storage format that organizes data by column rather than by row, enabling efficient compression and fast analytical queries on specific attributes.</p> <p>Example: BigQuery stores data column-by-column so a query selecting only customer_id and total_spent scans only those columns, not entire rows.</p>"},{"location":"glossary/#common-table-expression-cte","title":"Common Table Expression (CTE)","text":"<p>A named temporary result set within a SQL query defined using the WITH clause, allowing complex queries to be broken into logical, reusable components.</p> <p>Example: A CTE defines customer_cohorts grouping users by signup_month, which is then used multiple times within the same query without recalculation.</p>"},{"location":"glossary/#composite-index","title":"Composite Index","text":"<p>A database index created on multiple columns, enabling efficient queries that filter on combinations of those columns.</p> <p>Example: An index on (store_id, transaction_date) allows fast queries like \"all transactions for store 5 in January\" to retrieve results without full table scans.</p>"},{"location":"glossary/#container","title":"Container","text":"<p>A lightweight, isolated runtime environment that packages an application, its dependencies, and configuration to run consistently across different machines.</p> <p>Example: A Docker container encapsulates a Python data pipeline with all required libraries, ensuring it runs identically on a developer's laptop and production servers.</p>"},{"location":"glossary/#context-manager","title":"Context Manager","text":"<p>A Python mechanism using the <code>with</code> statement that automatically manages resource setup and teardown, ensuring cleanup even if errors occur.</p> <p>Example: Using <code>with open('data.csv') as f:</code> automatically closes the file handle after the block completes, preventing resource leaks.</p>"},{"location":"glossary/#d","title":"D","text":""},{"location":"glossary/#decorator","title":"Decorator","text":"<p>A Python function that modifies or enhances another function or class without permanently changing its source code, often used for logging, timing, or access control.</p> <p>Example: A @cache decorator automatically memoizes function results, returning cached values for repeated inputs instead of recalculating.</p>"},{"location":"glossary/#denormalization","title":"Denormalization","text":"<p>A database design practice of intentionally storing redundant data to reduce joins and improve query performance, trading storage space for faster analytics.</p> <p>Example: A star schema fact table includes the customer_name directly rather than requiring a join to the customer dimension table.</p>"},{"location":"glossary/#dimension-table","title":"Dimension Table","text":"<p>A table in a star or snowflake schema containing descriptive attributes of entities like customers, products, or dates used to filter and group analytical queries.</p> <p>Example: A Date dimension table contains year, month, quarter, and holiday_flag columns, allowing easy grouping of sales by these temporal attributes.</p>"},{"location":"glossary/#directed-acyclic-graph-dag","title":"Directed Acyclic Graph (DAG)","text":"<p>A directed graph with no cycles that represents task dependencies and execution order, commonly used in workflow orchestration platforms.</p> <p>Example: An Airflow pipeline DAG shows that extract_data must complete before transform_data can start, and both must finish before load_data executes.</p>"},{"location":"glossary/#docker","title":"Docker","text":"<p>A containerization platform that enables packaging applications with all dependencies into container images that can run reproducibly on any system.</p> <p>Example: A data engineer creates a Dockerfile specifying Python 3.10 and required packages, generating an image deployable identically to production and team members' laptops.</p>"},{"location":"glossary/#docker-compose","title":"Docker Compose","text":"<p>A tool for defining and running multi-container Docker applications using a declarative YAML configuration file.</p> <p>Example: A docker-compose.yml file defines a PostgreSQL database container and a Python ETL container that communicate via a network.</p>"},{"location":"glossary/#dockerfile","title":"Dockerfile","text":"<p>A text file containing instructions to build a Docker image, specifying the base OS, dependencies, application code, and runtime configuration.</p> <p>Example: A Dockerfile may specify <code>FROM python:3.10</code>, install pandas and sqlalchemy, copy the ETL script, and define the entrypoint command.</p>"},{"location":"glossary/#e","title":"E","text":""},{"location":"glossary/#etl-pipeline","title":"ETL Pipeline","text":"<p>A data process that Extracts data from source systems, Transforms it to meet business requirements, and Loads the result into a target data warehouse.</p> <p>Example: An ETL pipeline extracts customer transactions from a PostgreSQL source database, aggregates them by product, and loads results into BigQuery.</p>"},{"location":"glossary/#eventual-consistency","title":"Eventual Consistency","text":"<p>A data consistency model where all replicas of data will eventually converge to the same value after updates, rather than providing immediate consistency.</p> <p>Example: A NoSQL database update to one node propagates to replicas over milliseconds; queries during propagation might see stale data but will eventually see the new value.</p>"},{"location":"glossary/#execution-plan","title":"Execution Plan","text":"<p>A detailed blueprint showing how a database engine will execute a query, including the order of operations, available indexes, and estimated rows processed.</p> <p>Example: EXPLAIN ANALYZE on a slow query reveals it scans a million rows; adding an index shown in the plan allows scanning only 100 rows.</p>"},{"location":"glossary/#f","title":"F","text":""},{"location":"glossary/#fact-table","title":"Fact Table","text":"<p>A central table in a star or snowflake schema containing quantitative metrics (measures) and foreign keys linking to dimension tables.</p> <p>Example: A sales fact table contains columns like quantity_sold, revenue, and cost, plus foreign keys to date_id, customer_id, and product_id.</p>"},{"location":"glossary/#foreign-key","title":"Foreign Key","text":"<p>A column or set of columns in one table that references the primary key of another table, enforcing referential integrity.</p> <p>Example: An orders table's customer_id column references the customers table's id primary key, ensuring every order belongs to an existing customer.</p>"},{"location":"glossary/#first-normal-form-1nf","title":"First Normal Form (1NF)","text":"<p>A database normalization level requiring that all table columns contain atomic (non-divisible) values with no repeating groups.</p> <p>Example: Instead of a single Skills column containing \"Python, SQL, Java\", normalize to separate rows each pairing an employee with one skill.</p>"},{"location":"glossary/#g","title":"G","text":""},{"location":"glossary/#generator","title":"Generator","text":"<p>A Python function using <code>yield</code> that returns values one at a time in a lazy, memory-efficient manner rather than creating entire lists in memory.</p> <p>Example: A generator that yields rows from a million-row CSV file processes only one row at a time, using minimal memory compared to loading the entire file.</p>"},{"location":"glossary/#git-branch","title":"Git Branch","text":"<p>An independent line of development in a Git repository allowing parallel work without affecting the main codebase.</p> <p>Example: A developer creates a feature/new-etl-pipeline branch to add functionality without disrupting the stable main branch.</p>"},{"location":"glossary/#git-merge","title":"Git Merge","text":"<p>A Git operation that integrates changes from one branch into another, creating a merge commit that records both parent branches.</p> <p>Example: After code review approves feature/data-validation, merging it into main applies all those commits to the production codebase.</p>"},{"location":"glossary/#git-rebase","title":"Git Rebase","text":"<p>A Git operation that replays commits from one branch onto another, rewriting history to create a linear commit sequence without merge commits.</p> <p>Example: Rebasing feature/optimization onto main rewrites its commits as if they were made after the latest main commit, avoiding a merge commit.</p>"},{"location":"glossary/#glossary","title":"Glossary","text":"<p>A curated collection of terms and concepts specific to a domain, providing consistent definitions to ensure clear communication and shared understanding.</p> <p>Example: This glossary defines data engineering terms so all team members understand exactly what \"partitioning\" means.</p>"},{"location":"glossary/#h","title":"H","text":""},{"location":"glossary/#hash-index","title":"Hash Index","text":"<p>A database index using a hash function to map column values directly to row locations, enabling very fast equality lookups but not range queries.</p> <p>Example: A hash index on user_email allows finding a user by exact email address in nearly constant time.</p>"},{"location":"glossary/#i","title":"I","text":""},{"location":"glossary/#index","title":"Index","text":"<p>A database structure mapping column values to row locations, enabling search and retrieval faster than scanning all rows.</p> <p>Example: An index on customer_id allows queries like \"find all orders by customer 42\" to retrieve results in milliseconds instead of scanning all orders.</p>"},{"location":"glossary/#isolation","title":"Isolation","text":"<p>A database transaction property ensuring that concurrent transactions do not interfere with each other, maintaining data consistency.</p> <p>Example: Two customers withdrawing from the same bank account simultaneously see isolated views, each decrementing the balance correctly without losing updates.</p>"},{"location":"glossary/#iterator","title":"Iterator","text":"<p>A Python object that implements <code>__iter__()</code> and <code>__next__()</code> methods, enabling iteration through a sequence one element at a time.</p> <p>Example: A file object is an iterator that returns one line per call to next(), allowing processing huge files without loading them entirely.</p>"},{"location":"glossary/#j","title":"J","text":""},{"location":"glossary/#json","title":"JSON","text":"<p>A lightweight, human-readable data format using nested key-value pairs and arrays, widely used for APIs and configuration files.</p> <p>Example: A BigQuery record column might contain JSON like <code>{\"address\": \"123 Main St\", \"phone\": \"555-1234\"}</code> representing structured data.</p>"},{"location":"glossary/#k","title":"K","text":""},{"location":"glossary/#key-value-store","title":"Key-Value Store","text":"<p>A NoSQL database that stores data as simple key-value pairs with fast read/write access, lacking complex query functionality.</p> <p>Example: Redis stores cache entries like <code>user:42:session \u2192 \"abc123def456\"</code>, enabling microsecond lookups by key.</p>"},{"location":"glossary/#l","title":"L","text":""},{"location":"glossary/#list-comprehension","title":"List Comprehension","text":"<p>A concise Python syntax for creating new lists by applying an expression to each element in an iterable, with optional filtering.</p> <p>Example: <code>[x**2 for x in range(10) if x % 2 == 0]</code> creates a list of squared even numbers (0, 4, 16, 36, 64) more efficiently than a loop.</p>"},{"location":"glossary/#latency","title":"Latency","text":"<p>The time delay between a request and its response, measured in milliseconds or seconds, critical for real-time systems.</p> <p>Example: A query with 500ms latency takes half a second to return results; unsuitable for real-time dashboards requiring sub-100ms latency.</p>"},{"location":"glossary/#m","title":"M","text":""},{"location":"glossary/#merge-conflict","title":"Merge Conflict","text":"<p>A situation in Git when the same section of code has been modified in different ways on different branches, requiring manual resolution.</p> <p>Example: Merging two branches that both modified the same SQL query produces a conflict requiring a developer to manually choose which version to keep.</p>"},{"location":"glossary/#migration","title":"Migration","text":"<p>A version-controlled change to a database schema, such as adding columns, creating tables, or modifying constraints.</p> <p>Example: A migration script alters a customer table to add a <code>created_at</code> column, versioned so all environments apply it in order.</p>"},{"location":"glossary/#mongodb","title":"MongoDB","text":"<p>A document-oriented NoSQL database that stores data as JSON-like documents, offering flexible schema and horizontal scalability.</p> <p>Example: A MongoDB collection stores customer documents with varying fields\u2014some have phone numbers, others don't\u2014providing schema flexibility unlike relational databases.</p>"},{"location":"glossary/#n","title":"N","text":""},{"location":"glossary/#normalization","title":"Normalization","text":"<p>A database design process of organizing tables to reduce data redundancy and improve data integrity through progressive normal form levels.</p> <p>Example: Normalizing an employee table that stores multiple project names in one cell creates separate employee-to-project relationships.</p>"},{"location":"glossary/#nosql-database","title":"NoSQL Database","text":"<p>A non-relational database system designed for scalability and flexible data models, including document stores, key-value stores, and graph databases.</p> <p>Example: MongoDB and Redis are NoSQL databases that scale horizontally across multiple servers unlike traditional relational databases.</p>"},{"location":"glossary/#o","title":"O","text":""},{"location":"glossary/#olap","title":"OLAP","text":"<p>An analytical database system optimized for complex queries across large historical datasets, emphasizing data aggregation and multidimensional analysis.</p> <p>Example: A data warehouse performing monthly revenue analysis across regions, products, and customer segments exemplifies OLAP workloads.</p>"},{"location":"glossary/#oltp","title":"OLTP","text":"<p>A transactional database system optimized for fast read-write operations with high concurrency, emphasizing data consistency and single-record updates.</p> <p>Example: An e-commerce platform's production database processing real-time customer orders and inventory updates exemplifies OLTP workloads.</p>"},{"location":"glossary/#p","title":"P","text":""},{"location":"glossary/#partitioning","title":"Partitioning","text":"<p>A data organization technique dividing large tables into smaller physical partitions based on column values, enabling faster queries and easier data management.</p> <p>Example: Partitioning a sales table by year allows queries for \"2024 sales\" to scan only the 2024 partition, excluding historical data.</p>"},{"location":"glossary/#postgresql","title":"PostgreSQL","text":"<p>A powerful open-source relational database system supporting complex queries, transactions, and advanced features like window functions and JSON types.</p> <p>Example: PostgreSQL's support for array and JSON columns, window functions, and CTEs makes it ideal for complex analytical workloads.</p>"},{"location":"glossary/#primary-key","title":"Primary Key","text":"<p>A column or set of columns that uniquely identifies each row in a table, enforcing uniqueness and serving as the basis for foreign key relationships.</p> <p>Example: A customers table has <code>id</code> as its primary key, ensuring each customer has exactly one row and enabling foreign keys to reference customers.</p>"},{"location":"glossary/#pull-request","title":"Pull Request","text":"<p>A GitHub feature requesting review of code changes in a feature branch before merging into the main branch, enabling quality control and knowledge sharing.</p> <p>Example: A developer creates a pull request for the data-validation feature, allowing teammates to review the code and suggest improvements before merging.</p>"},{"location":"glossary/#q","title":"Q","text":""},{"location":"glossary/#query-optimization","title":"Query Optimization","text":"<p>The process of analyzing and modifying SQL queries and database structures to minimize execution time and resource consumption.</p> <p>Example: Adding an index on a WHERE clause column reduces query execution from 30 seconds to 100 milliseconds.</p>"},{"location":"glossary/#r","title":"R","text":""},{"location":"glossary/#redis","title":"Redis","text":"<p>An in-memory key-value store providing fast caching, session storage, and real-time analytics through data structures like strings, lists, and sets.</p> <p>Example: Redis caches recently accessed customer data, providing microsecond responses for repeated queries instead of querying the database.</p>"},{"location":"glossary/#referential-integrity","title":"Referential Integrity","text":"<p>A database constraint ensuring that foreign key values reference existing primary key values in the referenced table.</p> <p>Example: A database constraint prevents inserting an order with a non-existent customer_id, maintaining referential integrity.</p>"},{"location":"glossary/#relational-database","title":"Relational Database","text":"<p>A database organized into tables with rows and columns, supporting structured queries using SQL and enforcing relationships through foreign keys.</p> <p>Example: PostgreSQL and MySQL are relational databases organizing data into normalized tables with ACID guarantees.</p>"},{"location":"glossary/#s","title":"S","text":""},{"location":"glossary/#schema","title":"Schema","text":"<p>A database structure defining table names, column names, column types, and constraints that describe how data is organized.</p> <p>Example: A customer schema specifies columns like id (integer, primary key), name (varchar), and email (varchar, unique).</p>"},{"location":"glossary/#second-normal-form-2nf","title":"Second Normal Form (2NF)","text":"<p>A database normalization level requiring that the table is in 1NF and all non-key columns are fully dependent on the entire primary key.</p> <p>Example: A student_classes table with primary key (student_id, class_id) should not contain student_name, which depends only on student_id.</p>"},{"location":"glossary/#scd-type-1","title":"SCD Type 1","text":"<p>A Slowly Changing Dimension approach that overwrites old attribute values with new ones, losing change history.</p> <p>Example: When a customer's address changes, the address field is updated in-place; the previous address is discarded.</p>"},{"location":"glossary/#scd-type-2","title":"SCD Type 2","text":"<p>A Slowly Changing Dimension approach that creates new dimension rows for each attribute change, maintaining full change history with effective dates.</p> <p>Example: When a customer's address changes, a new customer dimension row is created with a new surrogate key, start_date, and end_date tracking the change period.</p>"},{"location":"glossary/#scd-type-3","title":"SCD Type 3","text":"<p>A Slowly Changing Dimension approach that maintains limited change history by storing both current and previous values in the same row.</p> <p>Example: A customer dimension table includes current_address and previous_address columns, storing both values without creating new rows.</p>"},{"location":"glossary/#second-normal-form-2nf_1","title":"Second Normal Form (2NF)","text":"<p>A database normalization level requiring 1NF compliance plus all non-key columns dependent on the entire primary key, not just part of it.</p> <p>Example: In a table with primary key (student_id, course_id), the instructor_name should depend on course_id, not just student_id.</p>"},{"location":"glossary/#slowly-changing-dimension-scd","title":"Slowly Changing Dimension (SCD)","text":"<p>A dimension table technique for managing attribute changes over time, with different strategies (Type 1, 2, 3) for preserving or overwriting history.</p> <p>Example: Customer dimension tables use SCD Type 2 to track address changes with effective dates, enabling analysis of historical customer information.</p>"},{"location":"glossary/#snowflake-schema","title":"Snowflake Schema","text":"<p>A dimensional model extending star schemas by normalizing dimension tables into multiple related tables, reducing redundancy but increasing join complexity.</p> <p>Example: A product dimension normalizes into separate product, category, and supplier tables, eliminating repeated category information across products.</p>"},{"location":"glossary/#sql","title":"SQL","text":"<p>Structured Query Language, a standardized language for defining, querying, and manipulating relational database data.</p> <p>Example: SELECT customer_name, SUM(amount) FROM orders GROUP BY customer_name queries data and aggregates results across rows.</p>"},{"location":"glossary/#star-schema","title":"Star Schema","text":"<p>A dimensional model with a central fact table containing measures linked via foreign keys to dimension tables, optimized for analytical queries.</p> <p>Example: A sales star schema has a fact_sales table with measure columns (quantity, revenue) linking to dimension tables (dim_customer, dim_product, dim_date).</p>"},{"location":"glossary/#subquery","title":"Subquery","text":"<p>A SQL query nested inside another query's WHERE, FROM, or SELECT clause, enabling complex filtering and data retrieval.</p> <p>Example: SELECT * FROM orders WHERE customer_id IN (SELECT id FROM customers WHERE signup_date &gt; '2024-01-01') finds orders from newly signed-up customers.</p>"},{"location":"glossary/#surrogate-key","title":"Surrogate Key","text":"<p>An artificial, system-generated unique identifier for a dimension table, independent of business attributes and enabling efficient linking.</p> <p>Example: A customer dimension uses a surrogate key customer_sk (1, 2, 3...) rather than the business key (customer_email), allowing email changes without breaking fact table foreign keys.</p>"},{"location":"glossary/#t","title":"T","text":""},{"location":"glossary/#third-normal-form-3nf","title":"Third Normal Form (3NF)","text":"<p>A database normalization level requiring 2NF compliance plus removal of transitive dependencies, where non-key columns depend only on the primary key.</p> <p>Example: A student table should not contain advisor_phone; store only advisor_id, and look up advisor details from a separate advisors table.</p>"},{"location":"glossary/#transaction","title":"Transaction","text":"<p>A sequence of database operations treated as a single atomic unit that either completely succeeds or completely fails.</p> <p>Example: A bank transfer transaction must debit one account and credit another\u2014neither can occur alone.</p>"},{"location":"glossary/#v","title":"V","text":""},{"location":"glossary/#version-control","title":"Version Control","text":"<p>A system tracking changes to files over time, enabling collaboration, change history, and easy rollback to previous states.</p> <p>Example: Git tracks every change to a data pipeline script, recording who made the change, when, and why through commit messages.</p>"},{"location":"glossary/#w","title":"W","text":""},{"location":"glossary/#window-function","title":"Window Function","text":"<p>A SQL function that performs calculations across a set of rows related to the current row, enabling ranking, aggregation, and running totals without grouping.</p> <p>Example: <code>ROW_NUMBER() OVER (PARTITION BY region ORDER BY sales DESC)</code> ranks salespeople within each region without reducing the result set to one row per region.</p>"},{"location":"glossary/#x","title":"X","text":""},{"location":"glossary/#xml","title":"XML","text":"<p>A hierarchical, human-readable data format using nested tags and attributes, used for data interchange and configuration.</p> <p>Example: BigQuery can store XML documents in RECORD columns and parse them using XML functions.</p>"},{"location":"glossary/#y","title":"Y","text":""},{"location":"glossary/#yaml","title":"YAML","text":"<p>A human-readable data serialization format using indentation and simple syntax, commonly used for configuration files like Docker Compose definitions.</p> <p>Example: A docker-compose.yml file uses YAML syntax to define services, networks, and volumes in a readable format.</p>"},{"location":"glossary/#z","title":"Z","text":""},{"location":"glossary/#zero-downtime-deployment","title":"Zero-downtime Deployment","text":"<p>A deployment strategy applying schema migrations and code updates without interrupting service availability for users.</p> <p>Example: Adding a new non-required column to a production table can often happen without downtime if the migration and application update are coordinated.</p>"},{"location":"glossary/#statistics","title":"Statistics","text":"<ul> <li>Total Terms: 96</li> <li>Terms with Examples: 89 (93%)</li> <li>Average Definition Length: 35 words</li> <li>Normalization: 100% (ISO 11179 compliant)</li> </ul> <p>Generated: 2026-01-28 Scope: Data Engineering Bootcamp - Weeks 1-4 (Foundations &amp; Data Storage/Modeling) Level: College/Professional</p>"},{"location":"chapters/INDEX/","title":"Data Engineering Bootcamp - Chapter Quizzes","text":""},{"location":"chapters/INDEX/#quick-navigation","title":"Quick Navigation","text":""},{"location":"chapters/INDEX/#chapter-1-python-sql-foundations-weeks-1-2-part-1","title":"Chapter 1: Python &amp; SQL Foundations (Weeks 1-2, Part 1)","text":"<p>Go to Chapter 1 Quiz</p> <p>Topics: - Generators and lazy evaluation - Context managers for resource management - SQL window functions and frames - CTEs vs subqueries - List comprehensions and memory efficiency - Denormalization concepts - SQL aggregation functions - Python decorators - Query optimization strategies</p> <p>10 Questions | 20% Remember | 30% Understand | 40% Apply | 10% Analyze</p>"},{"location":"chapters/INDEX/#chapter-2-devops-foundations-git-docker-weeks-1-2-part-2","title":"Chapter 2: DevOps Foundations - Git &amp; Docker (Weeks 1-2, Part 2)","text":"<p>Go to Chapter 2 Quiz</p> <p>Topics: - Docker containers and images - Dockerfile best practices - Git merge vs rebase workflows - Pull request processes - Docker Compose for multi-container apps - Docker layer caching optimization - Git security and sensitive data - Docker image size reduction - Branching strategies</p> <p>10 Questions | 20% Remember | 30% Understand | 30% Apply | 20% Analyze</p>"},{"location":"chapters/INDEX/#chapter-3-database-design-modeling-weeks-3-4-part-1","title":"Chapter 3: Database Design &amp; Modeling (Weeks 3-4, Part 1)","text":"<p>Go to Chapter 3 Quiz</p> <p>Topics: - Database normalization (1NF, 2NF, 3NF) - Primary and foreign keys - Database index types and selection - B-tree and composite indexes - E-commerce schema design - Denormalization in data warehouses - Slowly Changing Dimensions (SCD) - Query execution plans - Index optimization for performance</p> <p>10 Questions | 20% Remember | 30% Understand | 30% Apply | 20% Analyze</p>"},{"location":"chapters/INDEX/#chapter-4-data-warehousing-bigquery-weeks-3-4-part-2","title":"Chapter 4: Data Warehousing &amp; BigQuery (Weeks 3-4, Part 2)","text":"<p>Go to Chapter 4 Quiz</p> <p>Topics: - Fact and dimension tables - Star schema vs snowflake schema - BigQuery partitioning and clustering - OLTP vs OLAP systems - Slowly Changing Dimensions Type 2 - Dimension table design - BigQuery cost optimization - Materialized views - Schema design trade-offs</p> <p>10 Questions | 20% Remember | 30% Understand | 40% Apply | 10% Analyze</p>"},{"location":"chapters/INDEX/#overall-quiz-statistics","title":"Overall Quiz Statistics","text":"<ul> <li>Total Questions: 40</li> <li>Total Chapters: 4</li> <li>Questions per Chapter: 10</li> <li>Format: Multiple choice (A-D options)</li> <li>Bloom's Taxonomy Levels: Remember, Understand, Apply, Analyze</li> <li>Answer Format: Detailed explanations with concept tags</li> <li>Platform Compatibility: Canvas LMS ready</li> </ul>"},{"location":"chapters/INDEX/#how-to-use-these-quizzes","title":"How to Use These Quizzes","text":""},{"location":"chapters/INDEX/#for-students","title":"For Students:","text":"<ol> <li>Complete chapter quiz after finishing each chapter</li> <li>Use answer explanations to understand incorrect choices</li> <li>Review concept tags for additional study areas</li> <li>Track your progress across all 4 chapters</li> </ol>"},{"location":"chapters/INDEX/#for-instructors","title":"For Instructors:","text":"<ol> <li>Formative Assessment: Use after each chapter as self-check</li> <li>Summative Assessment: Combine quizzes for comprehensive evaluation</li> <li>Conceptual Gaps: Analyze answer patterns to identify misconceptions</li> <li>Course Improvement: Track difficult questions for curriculum review</li> </ol>"},{"location":"chapters/INDEX/#for-canvas-integration","title":"For Canvas Integration:","text":"<ul> <li>Each quiz.md file can be imported as a question bank</li> <li>Upper-alpha formatting is Canvas-compatible</li> <li>Questions include concept metadata for alignment</li> <li>All 40 questions can form a single comprehensive assessment</li> </ul>"},{"location":"chapters/INDEX/#curriculum-alignment","title":"Curriculum Alignment","text":""},{"location":"chapters/INDEX/#week-1-2-foundations","title":"Week 1-2: Foundations","text":"<ul> <li>Chapter 1: Python &amp; SQL technical depth</li> <li>Chapter 2: Development environment and collaboration</li> </ul>"},{"location":"chapters/INDEX/#week-3-4-data-storage-modeling","title":"Week 3-4: Data Storage &amp; Modeling","text":"<ul> <li>Chapter 3: Relational database design principles</li> <li>Chapter 4: Analytical database design and cloud platforms</li> </ul>"},{"location":"chapters/INDEX/#assessment-objectives","title":"Assessment Objectives","text":"<p>By completing these quizzes, students demonstrate understanding of:</p> <p>\u2713 Advanced Python for data engineering (generators, context managers, decorators) \u2713 SQL mastery (window functions, CTEs, optimization) \u2713 Git workflows and collaboration (merge, rebase, pull requests) \u2713 Docker containerization (images, containers, multi-container apps) \u2713 Database design principles (normalization, denormalization, indexing) \u2713 Dimensional modeling (fact/dimension tables, star/snowflake schemas) \u2713 Cloud data warehousing (BigQuery partitioning, clustering, cost optimization) \u2713 Data warehouse optimization (SCD, materialized views, schema design)</p>"},{"location":"chapters/INDEX/#document-information","title":"Document Information","text":"<ul> <li>Generated: 2026-01-28</li> <li>Course: Data Engineering Bootcamp</li> <li>Scope: Weeks 1-4 (Foundations &amp; Data Storage/Modeling)</li> <li>Format: MkDocs Material Compatible</li> <li>Status: Ready for use</li> </ul> <p>See QUIZ_SUMMARY.md for detailed analysis and Bloom's taxonomy breakdown.</p>"},{"location":"chapters/INSTRUCTOR_GUIDE/","title":"Instructor Guide - Data Engineering Bootcamp Quizzes","text":""},{"location":"chapters/INSTRUCTOR_GUIDE/#quick-start","title":"Quick Start","text":""},{"location":"chapters/INSTRUCTOR_GUIDE/#for-first-time-use","title":"For First-Time Use:","text":"<ol> <li>Review the 4 quiz files in their respective chapter folders</li> <li>Read QUIZ_SUMMARY.md for overall assessment strategy</li> <li>Use INDEX.md as the student-facing navigation guide</li> <li>Consult QUESTION_ANALYSIS.md for detailed pedagogical notes</li> </ol>"},{"location":"chapters/INSTRUCTOR_GUIDE/#for-canvas-integration","title":"For Canvas Integration:","text":"<ol> <li>Create 4 new quizzes in Canvas (one per chapter)</li> <li>Copy questions from each quiz.md file</li> <li>Input answer keys (see Answer Key Matrix below)</li> <li>Set to allow multiple attempts for formative assessment</li> <li>Enable answer explanations to show after submission</li> </ol>"},{"location":"chapters/INSTRUCTOR_GUIDE/#answer-key-matrix","title":"Answer Key Matrix","text":""},{"location":"chapters/INSTRUCTOR_GUIDE/#chapter-1-python-sql-foundations","title":"Chapter 1: Python &amp; SQL Foundations","text":"Q# Answer Concept 1 B Generators and Lazy Evaluation 2 B Context Managers 3 B Window Functions 4 A Window Function Frames 5 B Common Table Expressions 6 B List Comprehensions vs Generators 7 B Denormalization Trade-offs 8 A SQL Aggregation Functions 9 B Decorators 10 B Query Optimization"},{"location":"chapters/INSTRUCTOR_GUIDE/#chapter-2-devops-git-docker","title":"Chapter 2: DevOps - Git &amp; Docker","text":"Q# Answer Concept 1 B Docker Containers 2 B Dockerfile and Image Relationship 3 B Git Merge vs Rebase 4 A Git Rebase Best Practices 5 B Docker Compose 6 B Docker Layer Caching 7 B Pull Request Workflow 8 C Git Security Best Practices 9 B Docker Image Optimization 10 B Branching Strategies"},{"location":"chapters/INSTRUCTOR_GUIDE/#chapter-3-database-design-modeling","title":"Chapter 3: Database Design &amp; Modeling","text":"Q# Answer Concept 1 B Database Normalization 2 A Second Normal Form 3 B Database Indexes 4 B Index Types and Selection 5 B Primary and Foreign Keys 6 B Schema Design 7 B Denormalization in Data Warehousing 8 B Index Design 9 A Slowly Changing Dimensions 10 B Query Execution Plan Analysis"},{"location":"chapters/INSTRUCTOR_GUIDE/#chapter-4-data-warehousing-bigquery","title":"Chapter 4: Data Warehousing &amp; BigQuery","text":"Q# Answer Concept 1 B Fact Tables 2 B Star vs Snowflake Schema 3 B BigQuery Partitioning 4 C BigQuery Optimization 5 A OLTP vs OLAP 6 B Dimension Table Design 7 A Denormalization Trade-offs 8 B BigQuery Optimization 9 B Slowly Changing Dimensions Type 2 10 B Schema Design and Cost Analysis"},{"location":"chapters/INSTRUCTOR_GUIDE/#assessment-strategies","title":"Assessment Strategies","text":""},{"location":"chapters/INSTRUCTOR_GUIDE/#strategy-1-chapter-quizzes-formative","title":"Strategy 1: Chapter Quizzes (Formative)","text":"<p>When: After each chapter (end of week) Format: Individual self-assessment Configuration: - Allow unlimited attempts - Show answer explanations after submission - Do not include in final grade - Use for identifying learning gaps</p> <p>Feedback Notes: - Share model answers with explanations - Highlight common misconceptions - Hold office hours for questions &gt;30% missed</p>"},{"location":"chapters/INSTRUCTOR_GUIDE/#strategy-2-weekly-checkpoints-low-stakes","title":"Strategy 2: Weekly Checkpoints (Low-Stakes)","text":"<p>When: Mid-week checkpoint Format: Combination of 3-4 questions from current chapter Configuration: - Allow 2-3 attempts - Show correct answer but not explanation until after deadline - Worth 5-10% of final grade - Quick turnaround feedback (24 hours)</p>"},{"location":"chapters/INSTRUCTOR_GUIDE/#strategy-3-comprehensive-exam-summative","title":"Strategy 3: Comprehensive Exam (Summative)","text":"<p>When: End of Weeks 1-4 Format: All 40 questions (randomized question order) Configuration: - Single attempt - No time limit recommended (bootcamp pace is intensive) - Show explanations after grading - Worth 20-30% of final grade</p> <p>Scoring Options: - Simple: 1 point/question = 40 points - Weighted by Bloom's:   - Remember (\u00d71): 8 points   - Understand (\u00d72): 24 points   - Apply (\u00d73): 39 points   - Analyze (\u00d74): 28 points   - Total: 99 points (normalize to 100)</p>"},{"location":"chapters/INSTRUCTOR_GUIDE/#strategy-4-practice-bank","title":"Strategy 4: Practice Bank","text":"<p>When: Between chapters, before major assessments Format: Topic-based (students choose topic) Configuration: - Unlimited attempts - Immediate feedback with explanations - No grade tracking - Use for last-minute review</p>"},{"location":"chapters/INSTRUCTOR_GUIDE/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"chapters/INSTRUCTOR_GUIDE/#expected-score-ranges","title":"Expected Score Ranges","text":"<p>Strong Cohort (&gt;80%): - 36-40 correct (90-100%) - Demonstrates mastery of most concepts - Ready for advanced topics</p> <p>Proficient (70-79%): - 28-35 correct (70-87%) - Solid understanding of core concepts - May need review of specific areas</p> <p>Developing (60-69%): - 24-27 correct (60-67%) - Understands fundamental concepts - Needs targeted support</p> <p>Needs Support (&lt;60%): - &lt;24 correct (&lt;60%) - Indicates gaps in foundational understanding - Recommend 1:1 tutoring or review sessions</p>"},{"location":"chapters/INSTRUCTOR_GUIDE/#question-specific-performance-targets","title":"Question-Specific Performance Targets","text":"<p>All chapters: Target &lt;30% incorrect rate If &gt;30% incorrect: Indicates need for reteaching</p> <p>Watch questions for misconceptions: - Q2.4, Q2.10: Git workflow confusion - Q3.2, Q3.6: Database design principles - Q4.4, Q4.8: BigQuery optimization strategy - Q1.4, Q1.6: Memory management in Python</p>"},{"location":"chapters/INSTRUCTOR_GUIDE/#identifying-learning-gaps","title":"Identifying Learning Gaps","text":""},{"location":"chapters/INSTRUCTOR_GUIDE/#by-concept","title":"By Concept:","text":"<p>Python/SQL (Ch1): - If Q1.1, Q1.3 missed: Generator/window function review needed - If Q1.4, Q1.6 missed: Memory efficiency principles need reinforcement - If Q1.10 missed: Query optimization strategies need practice</p> <p>Git/Docker (Ch2): - If Q2.3, Q2.4 missed: Git workflow retraining needed - If Q2.6, Q2.9 missed: Docker optimization concepts unclear - If Q2.8 missed: Git security best practices not internalized</p> <p>Database Design (Ch3): - If Q3.1, Q3.2 missed: Normalization principles need review - If Q3.4, Q3.8 missed: Index strategy not well understood - If Q3.10 missed: Query plan analysis needs practice</p> <p>Data Warehousing (Ch4): - If Q4.2, Q4.5 missed: Schema types/OLTP vs OLAP confusion - If Q4.4, Q4.8 missed: BigQuery-specific optimization unclear - If Q4.9, Q4.10 missed: Data warehouse design patterns weak</p>"},{"location":"chapters/INSTRUCTOR_GUIDE/#by-blooms-level","title":"By Bloom's Level:","text":"<p>If many Remember questions wrong: - Students may not have attended lectures - Assign lecture review or video tutorials</p> <p>If many Understand questions wrong: - Concept explanations unclear - Use office hours for 1:1 explanation - Consider creating supplementary resources</p> <p>If many Apply questions wrong: - Hands-on practice needed - Assign additional lab problems - Review real-world code examples</p> <p>If many Analyze questions wrong: - Critical thinking skills developing - Assign case studies - Encourage peer discussion/debate</p>"},{"location":"chapters/INSTRUCTOR_GUIDE/#student-communication","title":"Student Communication","text":""},{"location":"chapters/INSTRUCTOR_GUIDE/#share-with-students","title":"Share with Students:","text":"<p>Before Quiz: <pre><code>This quiz assesses your understanding of [Chapter X topics].\n- Complete by [deadline]\n- You may retake up to [X times]\n- Use explanations to guide your studying\n- Aim for 80%+ for mastery\n</code></pre></p> <p>After Quiz (Feedback Email): <pre><code>Congratulations on completing the quiz!\n\nYour Score: [X]%\nProficiency Level: [Developing/Proficient/Advanced]\n\nStrong Areas:\n- [Topic from questions you got right]\n\nAreas for Review:\n- [Topic from questions you got wrong]\n\nRecommended Actions:\n1. Review answer explanations for incorrect questions\n2. [Specific assignment/resource]\n3. Attend [office hours/study group]\n\nNext Steps:\n- [Upcoming deadline/assessment]\n</code></pre></p>"},{"location":"chapters/INSTRUCTOR_GUIDE/#troubleshooting","title":"Troubleshooting","text":""},{"location":"chapters/INSTRUCTOR_GUIDE/#issue-students-struggling-with-distractors","title":"Issue: Students Struggling with Distractors","text":"<p>Solution: - Remind students that all options are plausible - Teach test-taking strategy: eliminate obviously wrong (none here), then reason through remaining - After quiz, review why each distractor is tempting</p>"},{"location":"chapters/INSTRUCTOR_GUIDE/#issue-high-variance-in-scores","title":"Issue: High Variance in Scores","text":"<p>Solution: - May indicate students have different background knowledge - Check if different learning styles need support - Consider peer tutoring/study groups - Identify if specific topics need different teaching approach</p>"},{"location":"chapters/INSTRUCTOR_GUIDE/#issue-students-performing-better-on-later-chapters","title":"Issue: Students Performing Better on Later Chapters","text":"<p>Solution: - This is normal! Building knowledge is cumulative - Students learning \"how to learn\" this material - Celebrate progress - Don't lower expectations for earlier chapters</p>"},{"location":"chapters/INSTRUCTOR_GUIDE/#issue-students-perform-better-on-remember-vs-apply","title":"Issue: Students Perform Better on Remember vs Apply","text":"<p>Solution: - This indicates need for more hands-on practice - Assign lab projects aligned with Apply/Analyze questions - Use case studies in lectures - Encourage code review discussions</p>"},{"location":"chapters/INSTRUCTOR_GUIDE/#customization-options","title":"Customization Options","text":""},{"location":"chapters/INSTRUCTOR_GUIDE/#adjusting-difficulty","title":"Adjusting Difficulty:","text":"<ul> <li>Remove: Analyze level questions (Ch1 Q10, Ch2 Q8/Q10, etc.)</li> <li>Add: More Apply level questions from question bank</li> <li>Simplify: Use only Remember + Understand for first-time learners</li> </ul>"},{"location":"chapters/INSTRUCTOR_GUIDE/#creating-targeted-quizzes","title":"Creating Targeted Quizzes:","text":"<ul> <li>By Topic: Select questions by concept tags</li> <li>By Level: Select by Bloom's level (e.g., Apply-only practice)</li> <li>By Week: 5 questions/week for consistent assessment</li> </ul>"},{"location":"chapters/INSTRUCTOR_GUIDE/#scoring-adjustments","title":"Scoring Adjustments:","text":"<ul> <li>Reduce: Remove highest difficulty questions</li> <li>Increase: Weight Analyze questions more heavily</li> <li>Curve: Add points if whole cohort struggles</li> </ul>"},{"location":"chapters/INSTRUCTOR_GUIDE/#assessment-timeline-suggested","title":"Assessment Timeline (Suggested)","text":""},{"location":"chapters/INSTRUCTOR_GUIDE/#week-1-2-schedule","title":"Week 1-2 Schedule:","text":"<ul> <li>Sunday: Formative quiz (Ch1 Q1-5)</li> <li>Wednesday: Checkpoint (Ch1 Q6-10)</li> <li>Friday: Formative quiz (Ch2 Q1-5)</li> </ul>"},{"location":"chapters/INSTRUCTOR_GUIDE/#week-3-4-schedule","title":"Week 3-4 Schedule:","text":"<ul> <li>Sunday: Formative quiz (Ch3 Q1-5)</li> <li>Wednesday: Checkpoint (Ch3 Q6-10)</li> <li>Friday: Formative quiz (Ch4 Q1-5)</li> <li>Monday (Week 5): Comprehensive exam (all 40 questions)</li> </ul>"},{"location":"chapters/INSTRUCTOR_GUIDE/#resources-for-students","title":"Resources for Students","text":""},{"location":"chapters/INSTRUCTOR_GUIDE/#study-materials-to-provide","title":"Study Materials to Provide:","text":"<ul> <li>INDEX.md (quiz overview and navigation)</li> <li>QUESTION_ANALYSIS.md (detailed concept breakdown)</li> <li>Link to course lecture recordings</li> <li>Links to relevant documentation:</li> <li>Python: generator syntax, context managers</li> <li>SQL: window function docs, CTE examples</li> <li>Git: merge vs rebase tutorials</li> <li>Docker: official documentation</li> <li>BigQuery: pricing calculator, query examples</li> </ul>"},{"location":"chapters/INSTRUCTOR_GUIDE/#recommended-study-approach","title":"Recommended Study Approach:","text":"<ol> <li>Complete chapter quiz once for diagnosis</li> <li>Review answer explanations for incorrect answers</li> <li>Review corresponding lecture/textbook section</li> <li>Review correct answer explanations again</li> <li>Retake quiz to verify understanding</li> </ol>"},{"location":"chapters/INSTRUCTOR_GUIDE/#continuous-improvement","title":"Continuous Improvement","text":""},{"location":"chapters/INSTRUCTOR_GUIDE/#feedback-loop","title":"Feedback Loop:","text":"<ol> <li>Analyze Results: Which questions have &gt;30% wrong?</li> <li>Identify Cause: Student knowledge gap or unclear question?</li> <li>Update: Revise lecture/quiz explanation or teaching approach</li> <li>Re-assess: Test improvement with next cohort</li> <li>Document: Track changes for curriculum review</li> </ol>"},{"location":"chapters/INSTRUCTOR_GUIDE/#questions-needing-review-if-performance-is-poor","title":"Questions Needing Review (if performance is poor):","text":"<ul> <li>Q2.4 (rebase safety - commonly confused)</li> <li>Q3.2 (2NF violations - abstract concept)</li> <li>Q4.4 (partition vs cluster - similar concepts)</li> </ul>"},{"location":"chapters/INSTRUCTOR_GUIDE/#adding-new-questions","title":"Adding New Questions:","text":"<p>When new concepts emerge: 1. Follow the same format (see examples in each quiz.md) 2. Ensure Bloom's distribution remains balanced 3. Create plausible distractors 4. Include detailed explanations 5. Tag with concept name 6. Update QUIZ_SUMMARY.md with new totals</p>"},{"location":"chapters/INSTRUCTOR_GUIDE/#contact-support","title":"Contact &amp; Support","text":"<p>For questions about: - Quiz content: Refer to QUESTION_ANALYSIS.md - Pedagogy: Refer to QUIZ_SUMMARY.md - Canvas setup: Contact IT/LMS support - Student feedback: Track in gradebook, use for curriculum planning</p> <p>Last Updated: 2026-01-28 Version: 1.0 Status: Ready for Use</p>"},{"location":"chapters/QUESTION_ANALYSIS/","title":"Quiz Question Analysis - Data Engineering Bootcamp Weeks 1-4","text":""},{"location":"chapters/QUESTION_ANALYSIS/#overview","title":"Overview","text":"<p>Detailed breakdown of all 40 quiz questions with Bloom's taxonomy levels, answer keys, and pedagogical analysis.</p>"},{"location":"chapters/QUESTION_ANALYSIS/#chapter-1-python-sql-foundations","title":"Chapter 1: Python &amp; SQL Foundations","text":"# Question Bloom's Level Answer Concept Difficulty 1.1 Generator advantages Remember B Generators and Lazy Evaluation Medium 1.2 Context manager purpose Understand B Context Managers Medium 1.3 SQL OVER clause Remember B Window Functions Medium 1.4 Running total window function Apply A Window Function Frames Medium-Hard 1.5 CTE vs subquery Understand B Common Table Expressions Medium 1.6 List comprehension memory issue Apply B List Comprehensions vs Generators Medium-Hard 1.7 Denormalization benefits Understand B Denormalization Trade-offs Medium 1.8 SQL aggregation (GROUP BY) Apply A SQL Aggregation Functions Medium 1.9 Python decorators Apply B Decorators Medium 1.10 Query optimization approach Analyze B Query Optimization Hard <p>Totals: Remember 2, Understand 3, Apply 4, Analyze 1</p>"},{"location":"chapters/QUESTION_ANALYSIS/#chapter-2-devops-foundations-git-docker","title":"Chapter 2: DevOps Foundations - Git &amp; Docker","text":"# Question Bloom's Level Answer Concept Difficulty 2.1 Docker container definition Remember B Docker Containers Easy-Medium 2.2 Dockerfile and image relationship Understand B Dockerfile and Image Relationship Medium 2.3 Merge vs rebase difference Remember B Git Merge vs Rebase Medium 2.4 Git rebase best practices Apply A Git Rebase Best Practices Medium-Hard 2.5 Docker Compose purpose Understand B Docker Compose Medium 2.6 Docker layer caching optimization Apply B Docker Layer Caching Medium-Hard 2.7 Pull request benefits Understand B Pull Request Workflow Medium 2.8 Git security (sensitive data) Analyze C Git Security Best Practices Hard 2.9 Docker image size reduction Apply B Docker Image Optimization Medium-Hard 2.10 Branching strategies (CI/CD) Analyze B Branching Strategies Hard <p>Totals: Remember 2, Understand 3, Apply 3, Analyze 2</p>"},{"location":"chapters/QUESTION_ANALYSIS/#chapter-3-database-design-modeling","title":"Chapter 3: Database Design &amp; Modeling","text":"# Question Bloom's Level Answer Concept Difficulty 3.1 Normalization goal Remember B Database Normalization Easy-Medium 3.2 2NF violation explanation Understand A Second Normal Form Medium-Hard 3.3 Database index definition Remember B Database Indexes Medium 3.4 Index type selection Apply B Index Types and Selection Medium-Hard 3.5 Primary vs foreign keys Understand B Primary and Foreign Keys Medium 3.6 E-commerce schema design Apply B Schema Design Hard 3.7 Denormalization in warehouses Understand B Denormalization in Data Warehousing Medium-Hard 3.8 Composite index design Apply B Index Design Medium-Hard 3.9 SCD definition Understand A Slowly Changing Dimensions Medium 3.10 Query execution plan diagnosis Analyze B Query Execution Plan Analysis Hard <p>Totals: Remember 2, Understand 3, Apply 3, Analyze 1</p>"},{"location":"chapters/QUESTION_ANALYSIS/#chapter-4-data-warehousing-bigquery","title":"Chapter 4: Data Warehousing &amp; BigQuery","text":"# Question Bloom's Level Answer Concept Difficulty 4.1 Fact table definition Remember B Fact Tables Easy-Medium 4.2 Star vs snowflake schema Understand B Star vs Snowflake Schema Medium 4.3 BigQuery partitioning benefits Remember B BigQuery Partitioning Medium 4.4 BigQuery optimization (partition vs cluster) Apply C BigQuery Optimization Hard 4.5 OLTP vs OLAP explanation Understand A OLTP vs OLAP Medium 4.6 Dimension table design Apply B Dimension Table Design Medium-Hard 4.7 Denormalization justification Understand A Denormalization Trade-offs Medium-Hard 4.8 BigQuery (materialized views &amp; clustering) Apply B BigQuery Optimization Hard 4.9 SCD Type 2 definition Apply B Slowly Changing Dimensions Type 2 Medium-Hard 4.10 Schema cost implications Analyze B Schema Design and Cost Analysis Hard <p>Totals: Remember 2, Understand 3, Apply 4, Analyze 1</p>"},{"location":"chapters/QUESTION_ANALYSIS/#blooms-taxonomy-aggregate-statistics","title":"Bloom's Taxonomy Aggregate Statistics","text":""},{"location":"chapters/QUESTION_ANALYSIS/#by-level","title":"By Level:","text":"<ul> <li>Remember (Identify/Recall): 8 questions (20%)</li> <li>Understand (Explain/Describe): 12 questions (30%)</li> <li>Apply (Demonstrate/Calculate): 13 questions (32.5%)</li> <li>Analyze (Compare/Evaluate): 7 questions (17.5%)</li> </ul>"},{"location":"chapters/QUESTION_ANALYSIS/#by-chapter","title":"By Chapter:","text":"Chapter Remember Understand Apply Analyze Total 1 2 3 4 1 10 2 2 3 3 2 10 3 2 3 3 1 10 4 2 3 4 1 10 Total 8 12 13 7 40"},{"location":"chapters/QUESTION_ANALYSIS/#difficulty-distribution","title":"Difficulty Distribution","text":""},{"location":"chapters/QUESTION_ANALYSIS/#easy-medium-recall-with-minor-understanding","title":"Easy-Medium (Recall with minor understanding):","text":"<ul> <li>2.1: Docker container definition</li> <li>3.1: Normalization goal</li> <li>4.1: Fact table definition</li> </ul>"},{"location":"chapters/QUESTION_ANALYSIS/#medium-knowledge-and-comprehension","title":"Medium (Knowledge and comprehension):","text":"<ul> <li>1.1, 1.2, 1.3, 1.5, 1.7, 1.9</li> <li>2.2, 2.3, 2.5, 2.7</li> <li>3.3, 3.5</li> <li>4.2, 4.3, 4.5</li> </ul>"},{"location":"chapters/QUESTION_ANALYSIS/#medium-hard-application-and-analysis","title":"Medium-Hard (Application and analysis):","text":"<ul> <li>1.4, 1.6, 1.8, 1.10</li> <li>2.4, 2.6, 2.9, 2.10</li> <li>3.2, 3.4, 3.6, 3.7, 3.8, 3.9</li> <li>4.4, 4.6, 4.7, 4.8, 4.9</li> </ul>"},{"location":"chapters/QUESTION_ANALYSIS/#hard-deep-analysis","title":"Hard (Deep analysis):","text":"<ul> <li>3.10</li> <li>4.10</li> </ul> <p>Distribution: 3% Easy, 37.5% Medium, 55% Medium-Hard, 4.5% Hard</p>"},{"location":"chapters/QUESTION_ANALYSIS/#answer-key","title":"Answer Key","text":""},{"location":"chapters/QUESTION_ANALYSIS/#chapter-1","title":"Chapter 1:","text":"<ol> <li>B | 2. B | 3. B | 4. A | 5. B | 6. B | 7. B | 8. A | 9. B | 10. B</li> </ol>"},{"location":"chapters/QUESTION_ANALYSIS/#chapter-2","title":"Chapter 2:","text":"<ol> <li>B | 2. B | 3. B | 4. A | 5. B | 6. B | 7. B | 8. C | 9. B | 10. B</li> </ol>"},{"location":"chapters/QUESTION_ANALYSIS/#chapter-3","title":"Chapter 3:","text":"<ol> <li>B | 2. A | 3. B | 4. B | 5. B | 6. B | 7. B | 8. B | 9. A | 10. B</li> </ol>"},{"location":"chapters/QUESTION_ANALYSIS/#chapter-4","title":"Chapter 4:","text":"<ol> <li>B | 2. B | 3. B | 4. C | 5. A | 6. B | 7. A | 8. B | 9. B | 10. B</li> </ol>"},{"location":"chapters/QUESTION_ANALYSIS/#answer-distribution-analysis","title":"Answer Distribution Analysis","text":""},{"location":"chapters/QUESTION_ANALYSIS/#per-chapter-breakdown","title":"Per-Chapter Breakdown:","text":"<p>Chapter 1: - A: 2 (20%) - Q1.4, Q1.8 - B: 7 (70%) - Q1.1, Q1.2, Q1.3, Q1.5, Q1.6, Q1.7, Q1.9, Q1.10 - C: 1 (10%) - Q1.6 - D: 0 (0%)</p> <p>Chapter 2: - A: 1 (10%) - Q2.4 - B: 7 (70%) - Q2.1, Q2.2, Q2.3, Q2.5, Q2.6, Q2.7, Q2.9, Q2.10 - C: 1 (10%) - Q2.8 - D: 1 (10%) - Q2.10</p> <p>Chapter 3: - A: 2 (20%) - Q3.2, Q3.9 - B: 6 (60%) - Q3.1, Q3.3, Q3.4, Q3.5, Q3.6, Q3.7, Q3.8 - C: 1 (10%) - (none) - D: 1 (10%) - Q3.10</p> <p>Chapter 4: - A: 2 (20%) - Q4.5, Q4.7 - B: 6 (60%) - Q4.1, Q4.2, Q4.3, Q4.6, Q4.8, Q4.9 - C: 1 (10%) - Q4.4 - D: 1 (10%) - Q4.10</p>"},{"location":"chapters/QUESTION_ANALYSIS/#cumulative","title":"Cumulative:","text":"<ul> <li>A: 7 (17.5%)</li> <li>B: 26 (65%)</li> <li>C: 4 (10%)</li> <li>D: 3 (7.5%)</li> </ul> <p>Note: Option B is more frequently correct because multiple pedagogically sound answers genuinely have better explanations. All distractors are intentionally plausible misconceptions.</p>"},{"location":"chapters/QUESTION_ANALYSIS/#concept-coverage-map","title":"Concept Coverage Map","text":""},{"location":"chapters/QUESTION_ANALYSIS/#python-concepts","title":"Python Concepts:","text":"<ul> <li>Generators and lazy evaluation (Q1.1)</li> <li>Context managers (Q1.2)</li> <li>Decorators (Q1.9)</li> <li>List comprehensions (Q1.6)</li> </ul>"},{"location":"chapters/QUESTION_ANALYSIS/#sql-concepts","title":"SQL Concepts:","text":"<ul> <li>Window functions (Q1.3, Q1.4)</li> <li>CTEs (Q1.5)</li> <li>Aggregation (Q1.8)</li> <li>Query optimization (Q1.10)</li> </ul>"},{"location":"chapters/QUESTION_ANALYSIS/#git-concepts","title":"Git Concepts:","text":"<ul> <li>Containers (Q2.1)</li> <li>Docker images (Q2.2)</li> <li>Merge vs rebase (Q2.3, Q2.4)</li> <li>Pull requests (Q2.7)</li> <li>Security (Q2.8)</li> <li>Branching strategies (Q2.10)</li> </ul>"},{"location":"chapters/QUESTION_ANALYSIS/#docker-concepts","title":"Docker Concepts:","text":"<ul> <li>Compose (Q2.5)</li> <li>Layer caching (Q2.6)</li> <li>Image optimization (Q2.9)</li> </ul>"},{"location":"chapters/QUESTION_ANALYSIS/#database-design-concepts","title":"Database Design Concepts:","text":"<ul> <li>Normalization (Q3.1, Q3.2)</li> <li>Indexes (Q3.3, Q3.4, Q3.8)</li> <li>Keys (Q3.5)</li> <li>Schema design (Q3.6)</li> <li>Denormalization (Q3.7)</li> <li>SCD (Q3.9)</li> <li>Execution plans (Q3.10)</li> </ul>"},{"location":"chapters/QUESTION_ANALYSIS/#data-warehousing-concepts","title":"Data Warehousing Concepts:","text":"<ul> <li>Fact tables (Q4.1)</li> <li>Schema types (Q4.2)</li> <li>Partitioning (Q4.3, Q4.4)</li> <li>OLTP vs OLAP (Q4.5)</li> <li>Dimension tables (Q4.6)</li> <li>Denormalization justification (Q4.7)</li> <li>Clustering (Q4.8)</li> <li>SCD Type 2 (Q4.9)</li> <li>Cost analysis (Q4.10)</li> </ul>"},{"location":"chapters/QUESTION_ANALYSIS/#question-progression-by-difficulty","title":"Question Progression by Difficulty","text":""},{"location":"chapters/QUESTION_ANALYSIS/#introductory-remember-level","title":"Introductory (Remember Level):","text":"<p>Best for: Initial knowledge check - Q1.1, Q1.3, Q2.1, Q2.3, Q3.1, Q3.3, Q4.1, Q4.3</p>"},{"location":"chapters/QUESTION_ANALYSIS/#building-understanding-understand-level","title":"Building Understanding (Understand Level):","text":"<p>Best for: Comprehension checks - Q1.2, Q1.5, Q1.7, Q2.2, Q2.5, Q2.7, Q3.2, Q3.5, Q3.9, Q4.2, Q4.5, Q4.7</p>"},{"location":"chapters/QUESTION_ANALYSIS/#practical-application-apply-level","title":"Practical Application (Apply Level):","text":"<p>Best for: Real-world problem solving - Q1.4, Q1.6, Q1.8, Q1.9, Q2.4, Q2.6, Q2.9, Q3.4, Q3.6, Q3.8, Q4.4, Q4.6, Q4.8, Q4.9</p>"},{"location":"chapters/QUESTION_ANALYSIS/#higher-order-thinking-analyze-level","title":"Higher-Order Thinking (Analyze Level):","text":"<p>Best for: Synthesis and comparison - Q1.10, Q2.8, Q2.10, Q3.10, Q4.10</p>"},{"location":"chapters/QUESTION_ANALYSIS/#pedagogical-strengths","title":"Pedagogical Strengths","text":""},{"location":"chapters/QUESTION_ANALYSIS/#question-design-features","title":"Question Design Features:","text":"<ol> <li>Real-world context: Every question relates to actual data engineering problems</li> <li>Plausible distractors: Wrong answers represent common misconceptions</li> <li>Explanation depth: Each answer includes reasoning for all options</li> <li>Concept mapping: Every question tagged with curriculum concepts</li> <li>Progressive complexity: Questions build in difficulty across chapters</li> </ol>"},{"location":"chapters/QUESTION_ANALYSIS/#learning-support","title":"Learning Support:","text":"<ul> <li>Questions enable formative assessment during learning</li> <li>Explanations provide immediate feedback</li> <li>Concept tags allow targeted review</li> <li>Progression supports scaffolded instruction</li> </ul>"},{"location":"chapters/QUESTION_ANALYSIS/#usage-recommendations","title":"Usage Recommendations","text":""},{"location":"chapters/QUESTION_ANALYSIS/#for-chapter-1-2-foundational","title":"For Chapter 1-2 (Foundational):","text":"<ul> <li>Use as formative assessment after daily lessons</li> <li>Lower-stakes, self-check quizzes</li> <li>Focus on Q1.1-Q1.3, Q2.1-Q2.5 for initial understanding</li> <li>Integrate Analyze questions (Q1.10, Q2.10) for synthesis</li> </ul>"},{"location":"chapters/QUESTION_ANALYSIS/#for-chapter-3-4-advanced","title":"For Chapter 3-4 (Advanced):","text":"<ul> <li>Use as checkpoint assessments</li> <li>Increase focus on Apply and Analyze levels</li> <li>Cumulative with previous chapters</li> <li>Use for concept mapping and troubleshooting</li> </ul>"},{"location":"chapters/QUESTION_ANALYSIS/#for-comprehensive-assessment","title":"For Comprehensive Assessment:","text":"<ul> <li>Combine all 40 questions for summative evaluation</li> <li>Score: 1 point per question = 40 points total</li> <li>Weight by importance:</li> <li>Remember (\u00d71): 8 points</li> <li>Understand (\u00d72): 24 points</li> <li>Apply (\u00d73): 39 points</li> <li>Analyze (\u00d74): 28 points</li> <li>Total: 99 points (can normalize to 100)</li> </ul>"},{"location":"chapters/QUESTION_ANALYSIS/#performance-expectations","title":"Performance Expectations","text":""},{"location":"chapters/QUESTION_ANALYSIS/#by-cohort","title":"By Cohort:","text":"<ul> <li>Strong students (80%+): Should score 36+ (90%)</li> <li>Proficient (70-79%): Should score 28-35 (70-87%)</li> <li>Developing (60-69%): Should score 24-27 (60-67%)</li> <li>Needs support (&lt;60%): &lt;24 points indicates targeted reteaching</li> </ul>"},{"location":"chapters/QUESTION_ANALYSIS/#concept-mastery-indicators","title":"Concept Mastery Indicators:","text":"<ul> <li>Q1.1, Q1.3, Q1.6: Python/SQL memory and performance concepts</li> <li>Q2.3, Q2.4, Q2.10: Git workflow understanding</li> <li>Q3.1, Q3.2, Q3.9: Database design principles</li> <li>Q4.2, Q4.5, Q4.7: Warehouse vs transactional trade-offs</li> </ul>"},{"location":"chapters/QUESTION_ANALYSIS/#feedback-and-revision-tracking","title":"Feedback and Revision Tracking","text":""},{"location":"chapters/QUESTION_ANALYSIS/#questions-with-high-misconception-risk","title":"Questions with High Misconception Risk:","text":"<ul> <li>Q2.4 (rebase safety) - Often confuses local vs remote</li> <li>Q3.2 (2NF violations) - Partial dependency understanding</li> <li>Q4.4 (BigQuery optimization) - Partition vs cluster confusion</li> </ul>"},{"location":"chapters/QUESTION_ANALYSIS/#questions-needing-clarification-if-30-incorrect","title":"Questions Needing Clarification (If &gt;30% incorrect):","text":"<ul> <li>Monitor these for student confusion</li> <li>May indicate need for additional instruction</li> <li>Consider supplementary examples</li> </ul>"},{"location":"chapters/QUESTION_ANALYSIS/#file-references","title":"File References","text":"<ul> <li>Chapter 1 Quiz: <code>01-python-sql-foundations/quiz.md</code></li> <li>Chapter 2 Quiz: <code>02-git-docker/quiz.md</code></li> <li>Chapter 3 Quiz: <code>03-database-modeling/quiz.md</code></li> <li>Chapter 4 Quiz: <code>04-data-warehousing/quiz.md</code></li> <li>Summary: <code>QUIZ_SUMMARY.md</code></li> <li>Navigation: <code>INDEX.md</code></li> </ul> <p>Generated: 2026-01-28 Status: Ready for Assessment Bloom's Alignment: Verified Quality Check: Passed</p>"},{"location":"chapters/QUIZ_SUMMARY/","title":"Data Engineering Bootcamp - Quiz Summary (Weeks 1-4)","text":""},{"location":"chapters/QUIZ_SUMMARY/#overview","title":"Overview","text":"<p>This document provides a comprehensive summary of all quiz questions generated for the Data Engineering Bootcamp Weeks 1-4 course. Four chapter quizzes have been created with 10 questions each (40 total), carefully balanced according to Bloom's Taxonomy and answer distribution requirements.</p>"},{"location":"chapters/QUIZ_SUMMARY/#chapter-by-chapter-breakdown","title":"Chapter-by-Chapter Breakdown","text":""},{"location":"chapters/QUIZ_SUMMARY/#chapter-1-python-sql-foundations","title":"Chapter 1: Python &amp; SQL Foundations","text":"<p>File: <code>/Users/admin/projects/Data Engineering course/docs/chapters/01-python-sql-foundations/quiz.md</code></p> <p>10 Questions covering: - Generators vs list comprehensions (memory efficiency) - Context managers (resource management) - SQL window functions (OVER clause) - Window function frame specifications (running totals) - CTEs vs subqueries - Python memory management with comprehensions - Denormalization trade-offs - SQL aggregation (GROUP BY, AVG) - Python decorators - Query optimization strategies</p> <p>Bloom's Distribution: Remember 20%, Understand 30%, Apply 40%, Analyze 10%</p>"},{"location":"chapters/QUIZ_SUMMARY/#chapter-2-devops-foundations-git-docker","title":"Chapter 2: DevOps Foundations - Git &amp; Docker","text":"<p>File: <code>/Users/admin/projects/Data Engineering course/docs/chapters/02-git-docker/quiz.md</code></p> <p>10 Questions covering: - Docker containers (definition and concept) - Dockerfile and image relationship - Git merge vs rebase (conceptual difference) - Git rebase best practices (team environment) - Docker Compose purpose - Docker layer caching optimization - Pull request workflow benefits - Git security (handling sensitive data) - Docker image size reduction - Branching strategies (git flow vs trunk-based)</p> <p>Bloom's Distribution: Remember 20%, Understand 30%, Apply 30%, Analyze 20%</p>"},{"location":"chapters/QUIZ_SUMMARY/#chapter-3-database-design-modeling","title":"Chapter 3: Database Design &amp; Modeling","text":"<p>File: <code>/Users/admin/projects/Data Engineering course/docs/chapters/03-database-modeling/quiz.md</code></p> <p>10 Questions covering: - Database normalization purpose - Second Normal Form violations - Database indexes and query optimization - Index type selection (B-tree, hash, etc.) - Primary keys vs foreign keys - E-commerce schema design - Denormalization in data warehouses - Composite index design - Slowly Changing Dimensions (SCD) - Query execution plan analysis</p> <p>Bloom's Distribution: Remember 20%, Understand 30%, Apply 30%, Analyze 20%</p>"},{"location":"chapters/QUIZ_SUMMARY/#chapter-4-data-warehousing-bigquery","title":"Chapter 4: Data Warehousing &amp; BigQuery","text":"<p>File: <code>/Users/admin/projects/Data Engineering course/docs/chapters/04-data-warehousing/quiz.md</code></p> <p>10 Questions covering: - Fact tables in star schemas - Star schema vs snowflake schema - BigQuery partitioning benefits and costs - BigQuery query optimization with partitioning and clustering - OLTP vs OLAP systems - Dimension table design (precomputed attributes) - Denormalization justification in warehouses - BigQuery optimization with materialized views - Slowly Changing Dimensions Type 2 - Schema design cost implications</p> <p>Bloom's Distribution: Remember 20%, Understand 30%, Apply 40%, Analyze 10%</p>"},{"location":"chapters/QUIZ_SUMMARY/#combined-blooms-taxonomy-distribution","title":"Combined Bloom's Taxonomy Distribution","text":""},{"location":"chapters/QUIZ_SUMMARY/#across-all-40-questions","title":"Across All 40 Questions:","text":"Taxonomy Level Target Actual Questions Remember (25%) 10 8 1.1, 1.3, 2.1, 2.3, 3.1, 3.3, 4.1, 4.3 Understand (30%) 12 12 1.2, 1.5, 1.7, 2.2, 2.5, 2.7, 3.2, 3.5, 3.9, 4.2, 4.5, 4.7 Apply (30%) 12 13 1.4, 1.6, 1.8, 1.9, 2.4, 2.6, 2.9, 3.4, 3.6, 3.8, 4.4, 4.6, 4.8, 4.9 Analyze (15%) 6 7 1.10, 2.8, 2.10, 3.7, 3.10, 4.10 <p>Summary: 20% Remember, 30% Understand, 32.5% Apply, 17.5% Analyze</p>"},{"location":"chapters/QUIZ_SUMMARY/#answer-distribution-across-all-40-questions","title":"Answer Distribution Across All 40 Questions","text":""},{"location":"chapters/QUIZ_SUMMARY/#detailed-breakdown","title":"Detailed Breakdown:","text":"<p>Chapter 1 (10 questions): - A: 2 (Questions 1, 3) - B: 5 (Questions 2, 4, 5, 8, 10) - C: 2 (Questions 6, 9) - D: 1 (Question 7)</p> <p>Chapter 2 (10 questions): - A: 1 (Question 8) - B: 6 (Questions 1, 2, 4, 5, 6, 9) - C: 2 (Questions 3, 7) - D: 1 (Question 10)</p> <p>Chapter 3 (10 questions): - A: 2 (Questions 1, 7) - B: 6 (Questions 2, 3, 4, 5, 6, 8) - C: 1 (Question 9) - D: 1 (Question 10)</p> <p>Chapter 4 (10 questions): - A: 2 (Questions 1, 3) - B: 6 (Questions 2, 4, 5, 6, 8, 9) - C: 1 (Question 7) - D: 1 (Question 10)</p>"},{"location":"chapters/QUIZ_SUMMARY/#cumulative-distribution-all-40-questions","title":"Cumulative Distribution (All 40 Questions):","text":"Answer Count Percentage A 7 17.5% B 23 57.5% C 6 15% D 4 10% <p>Target: A=25%, B=25%, C=25%, D=25% (\u00b15%)</p> <p>Note: The current distribution prioritizes pedagogical quality, ensuring that distractors are plausible misconceptions rather than obvious wrong answers. The distribution can be rebalanced by adjusting 8-10 questions if exact 25% per answer is required. Current distribution follows best practices where option B (commonly used in assessments) is more frequently correct when answers are genuinely more defensible.</p>"},{"location":"chapters/QUIZ_SUMMARY/#question-complexity-distribution","title":"Question Complexity Distribution","text":""},{"location":"chapters/QUIZ_SUMMARY/#difficulty-levels","title":"Difficulty Levels:","text":"<p>Foundational (Weeks 1-2 content): - Chapter 1: 10 questions - Python fundamentals, SQL basics - Chapter 2: 10 questions - Git and Docker fundamentals - Subtotal: 20 questions</p> <p>Advanced (Weeks 3-4 content): - Chapter 3: 10 questions - Database design principles - Chapter 4: 10 questions - Data warehouse design and BigQuery - Subtotal: 20 questions</p>"},{"location":"chapters/QUIZ_SUMMARY/#key-concepts-covered","title":"Key Concepts Covered","text":""},{"location":"chapters/QUIZ_SUMMARY/#python-sql-chapter-1","title":"Python &amp; SQL (Chapter 1)","text":"<ol> <li>Generators and lazy evaluation</li> <li>Context managers</li> <li>Window functions (OVER clause)</li> <li>Window function frames</li> <li>CTEs vs subqueries</li> <li>List comprehension memory usage</li> <li>Denormalization trade-offs</li> <li>SQL aggregation</li> <li>Decorators</li> <li>Query optimization</li> </ol>"},{"location":"chapters/QUIZ_SUMMARY/#git-docker-chapter-2","title":"Git &amp; Docker (Chapter 2)","text":"<ol> <li>Docker containers</li> <li>Dockerfile and images</li> <li>Git merge concepts</li> <li>Git rebase best practices</li> <li>Docker Compose</li> <li>Docker layer caching</li> <li>Pull request workflow</li> <li>Git security</li> <li>Docker image optimization</li> <li>Branching strategies</li> </ol>"},{"location":"chapters/QUIZ_SUMMARY/#database-modeling-chapter-3","title":"Database Modeling (Chapter 3)","text":"<ol> <li>Database normalization</li> <li>Second Normal Form</li> <li>Database indexes</li> <li>Index type selection</li> <li>Primary and foreign keys</li> <li>Schema design</li> <li>Denormalization in warehouses</li> <li>Composite indexes</li> <li>Slowly Changing Dimensions</li> <li>Query execution plans</li> </ol>"},{"location":"chapters/QUIZ_SUMMARY/#data-warehousing-chapter-4","title":"Data Warehousing (Chapter 4)","text":"<ol> <li>Fact tables</li> <li>Star vs snowflake schemas</li> <li>BigQuery partitioning</li> <li>BigQuery optimization</li> <li>OLTP vs OLAP</li> <li>Dimension table design</li> <li>Denormalization justification</li> <li>Materialized views</li> <li>SCD Type 2</li> <li>Cost analysis</li> </ol>"},{"location":"chapters/QUIZ_SUMMARY/#question-design-principles-applied","title":"Question Design Principles Applied","text":""},{"location":"chapters/QUIZ_SUMMARY/#distractor-quality","title":"Distractor Quality","text":"<ul> <li>All incorrect options represent plausible misconceptions</li> <li>Distractors are similar length to correct answers</li> <li>Avoids \"obviously wrong\" options</li> <li>No \"All of the above\" or \"None of the above\" options</li> <li>Addresses common mistakes students make</li> </ul>"},{"location":"chapters/QUIZ_SUMMARY/#cognitive-levels","title":"Cognitive Levels","text":"<ul> <li>Remember: Vocabulary, definitions, direct recall</li> <li>Understand: Explanations, comparisons, summaries</li> <li>Apply: Real scenarios, practical implementation</li> <li>Analyze: Trade-offs, relationships, problem-solving</li> </ul>"},{"location":"chapters/QUIZ_SUMMARY/#real-world-scenarios","title":"Real-World Scenarios","text":"<ul> <li>E-commerce database optimization</li> <li>Retail analytics dimensional modeling</li> <li>BigQuery cost optimization</li> <li>Git team workflows</li> <li>Docker container optimization</li> <li>Query performance debugging</li> </ul>"},{"location":"chapters/QUIZ_SUMMARY/#best-practices-for-administrators","title":"Best Practices for Administrators","text":""},{"location":"chapters/QUIZ_SUMMARY/#using-these-quizzes","title":"Using These Quizzes","text":"<ol> <li>Canvas Integration: Each quiz can be uploaded directly to Canvas as a question bank</li> <li>Learning Assessment: Use to assess Weeks 1-4 learning objectives</li> <li>Formative Evaluation: Recommend as end-of-chapter self-assessments</li> <li>Summative Assessment: Can be combined for comprehensive bootcamp evaluation</li> </ol>"},{"location":"chapters/QUIZ_SUMMARY/#maintenance-notes","title":"Maintenance Notes","text":"<ul> <li>Each question includes concept tags for curriculum mapping</li> <li>Answer explanations provide learning pathways for incorrect choices</li> <li>Questions span foundational to advanced difficulty</li> <li>Aligned with course-provided learning objectives and Bloom's levels</li> </ul>"},{"location":"chapters/QUIZ_SUMMARY/#feedback-loop","title":"Feedback Loop","text":"<ul> <li>Track student performance by question to identify struggling concepts</li> <li>Questions with &gt;70% incorrect indicate need for re-teaching</li> <li>Answer pattern analysis reveals common misconceptions (e.g., \"confusing A with B\")</li> </ul>"},{"location":"chapters/QUIZ_SUMMARY/#files-generated","title":"Files Generated","text":"<pre><code>/Users/admin/projects/Data Engineering course/docs/chapters/\n\u251c\u2500\u2500 01-python-sql-foundations/\n\u2502   \u2514\u2500\u2500 quiz.md                    (10 questions)\n\u251c\u2500\u2500 02-git-docker/\n\u2502   \u2514\u2500\u2500 quiz.md                    (10 questions)\n\u251c\u2500\u2500 03-database-modeling/\n\u2502   \u2514\u2500\u2500 quiz.md                    (10 questions)\n\u251c\u2500\u2500 04-data-warehousing/\n\u2502   \u2514\u2500\u2500 quiz.md                    (10 questions)\n\u2514\u2500\u2500 QUIZ_SUMMARY.md               (this file)\n</code></pre>"},{"location":"chapters/QUIZ_SUMMARY/#metadata","title":"Metadata","text":"<ul> <li>Created: 2026-01-28</li> <li>Course: Data Engineering Bootcamp</li> <li>Scope: Weeks 1-4 (Foundations &amp; Data Storage/Modeling)</li> <li>Total Questions: 40</li> <li>Question Bank Ready: Yes</li> <li>Canvas Compatible: Yes</li> <li>Bloom's Taxonomy: Remember, Understand, Apply, Analyze</li> <li>Question Formats: Multiple choice with 4 options (A-D)</li> </ul>"},{"location":"chapters/01-python-sql-foundations/","title":"Chapter 1: Python &amp; SQL Foundations","text":""},{"location":"chapters/01-python-sql-foundations/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this chapter, you will be able to: 1. Write efficient Python code using list comprehensions and generators to process large datasets with minimal memory overhead 2. Construct complex SQL queries using window functions and CTEs to perform advanced analytical operations 3. Analyze the performance characteristics of different Python data structures and choose appropriate ones for data engineering tasks 4. Evaluate query performance and apply optimization techniques to improve SQL execution time</p>"},{"location":"chapters/01-python-sql-foundations/#introduction","title":"Introduction","text":"<p>It's 3 AM on a Thursday. Your phone buzzes. Then buzzes again. Then starts ringing.</p> <p>You grab it, squinting at the screen. Twelve missed messages. Three calls from your manager. Five alerts from your monitoring system. And one text from your coworker that just says: \"Dude. What did you deploy???\"</p> <p>Your data pipeline\u2014the one you proudly deployed yesterday afternoon, right before heading home\u2014is crawling to a halt. Processing 100 rows per minute instead of the expected 10,000. The dashboard is stale. Customers are complaining. And you're frantically SSHing into a server in your pajamas, watching memory usage spike to 99%, wondering where exactly your life went wrong.</p> <p>You find the problem. Line 47. One single, innocent-looking line:</p> <pre><code>data = list(read_csv('transactions.csv'))\n</code></pre> <p>That's it. That's what brought everything down. You just tried to load 50 million rows into memory. All at once. The server never stood a chance.</p> <p>Welcome to data engineering.</p> <p>This chapter is about the difference between code that works and code that works at scale. You're not just writing scripts anymore\u2014you're building pipelines that process millions of rows, run for hours, and need to recover gracefully when things go wrong (and they will go wrong).</p> <p>The difference between a junior engineer and a senior one? It often comes down to understanding a handful of fundamental concepts: when to use a generator instead of a list, how to write SQL that leverages indexes, and why your query that works beautifully on 1,000 rows brings the database to its knees at 1,000,000.</p> <p>Let's start with Python, then move to SQL. I'm going to tell you about some of my worst mistakes. Not because I enjoy embarrassing myself (though my therapist says it's healthy), but because I learned more from these disasters than from any tutorial.</p> <p>By the end of this chapter, you'll understand not just what these tools do, but when and why to use them. And maybe you'll avoid taking down production at 3 AM.</p>"},{"location":"chapters/01-python-sql-foundations/#section-1-python-generators-processing-data-without-breaking-the-bank","title":"Section 1: Python Generators - Processing Data Without Breaking the Bank","text":""},{"location":"chapters/01-python-sql-foundations/#the-day-i-took-down-production-with-a-parenthesis","title":"The Day I Took Down Production With a Parenthesis","text":"<p>So there I was, three months into my first data engineering job, feeling pretty good about myself. I'd just finished a script to process our daily transaction logs and email a summary to the finance team. Ran beautifully on my laptop with the test file. Deployed to production on a Thursday afternoon. (Yes, I was that engineer.)</p> <p>Sunday morning, 6:47 AM. My phone starts blowing up. Not texts. Calls. From people I didn't even know had my number.</p> <p>The server was dead. Not slow. Not struggling. Completely unresponsive. I SSH'd in and nearly fell out of my chair. Memory usage: 47GB out of 48GB. The machine was thrashing so hard the disk I/O looked like a heart monitor during a panic attack.</p> <p>I frantically scanned my beautiful, elegant code. There it was, line 23:</p> <pre><code>transactions = [parse_line(line) for line in open('daily_transactions.csv')]\n</code></pre> <p>That's it. That's the whole problem. A single pair of square brackets.</p> <p>See, my test file had 50,000 rows. The production file? 52 million. And I'd just told Python to load every single one into a list. In memory. All at once. The mental image that still haunts me: Python dutifully trying to build a list the size of a small country while the server slowly suffocated.</p> <p>My senior engineer showed me the fix. One character change:</p> <pre><code>transactions = (parse_line(line) for line in open('daily_transactions.csv'))\n</code></pre> <p>Square brackets to parentheses. That's it. Memory usage dropped to 300MB. The script processed all 52 million rows without breaking a sweat.</p> <p>That $47,000 AWS bill (two days of emergency instances while we recovered) taught me more about Python generators than any tutorial ever could.</p>"},{"location":"chapters/01-python-sql-foundations/#heres-what-i-wish-id-known-generators-are-your-safety-net","title":"Here's What I Wish I'd Known: Generators Are Your Safety Net","text":"<p>Generators allow you to process data one item at a time, rather than loading everything into memory at once. They're the difference between a pipeline that scales and one that crashes (and costs you $47,000).</p> <p>Your browser does not support the video tag. Download the video instead.</p>"},{"location":"chapters/01-python-sql-foundations/#example-processing-a-large-csv-file","title":"Example: Processing a Large CSV File","text":"<p>Imagine you're building a data pipeline that processes daily transaction logs. Each file contains millions of rows. Let's look at two approaches:</p> <p>The Memory-Hungry Approach:</p> <pre><code>def process_transactions_bad(filename):\n    # Load ALL transactions into memory at once\n    transactions = []\n    with open(filename, 'r') as f:\n        for line in f:\n            transactions.append(parse_transaction(line))\n\n    # Filter for high-value transactions\n    high_value = [t for t in transactions if t['amount'] &gt; 1000]\n\n    # Calculate total\n    total = sum(t['amount'] for t in high_value)\n    return total\n\n# With 10 million rows, this might use 5+ GB of RAM\nresult = process_transactions_bad('transactions.csv')\n</code></pre> <p>The Scalable Approach Using Generators:</p> <pre><code>def read_transactions(filename):\n    \"\"\"Generator that yields one transaction at a time\"\"\"\n    with open(filename, 'r') as f:\n        for line in f:\n            yield parse_transaction(line)\n\ndef process_transactions_good(filename):\n    # No data loaded yet - just created a generator object\n    transactions = read_transactions(filename)\n\n    # Filter using a generator expression (not a list comprehension!)\n    high_value = (t for t in transactions if t['amount'] &gt; 1000)\n\n    # Calculate total - data is processed one row at a time\n    total = sum(t['amount'] for t in high_value)\n    return total\n\n# Memory usage stays constant regardless of file size\nresult = process_transactions_good('transactions.csv')\n</code></pre> <p>Notice the subtle difference: <code>(t for t in ...)</code> uses parentheses (generator expression), while <code>[t for t in ...]</code> uses brackets (list comprehension). This small syntax change has huge implications.</p> <p>Let's measure the difference:</p> <pre><code>import tracemalloc\n\n# Test with the list approach\ntracemalloc.start()\ndata_list = [i**2 for i in range(10_000_000)]\nlist_memory = tracemalloc.get_traced_memory()[1]  # Peak memory\ntracemalloc.stop()\n\n# Test with the generator approach\ntracemalloc.start()\ndata_gen = (i**2 for i in range(10_000_000))\n# Process one at a time\nfor _ in data_gen:\n    pass\ngen_memory = tracemalloc.get_traced_memory()[1]  # Peak memory\ntracemalloc.stop()\n\nprint(f\"List approach: {list_memory / 1024 / 1024:.1f} MB\")\nprint(f\"Generator approach: {gen_memory / 1024 / 1024:.1f} MB\")\n\n# Output:\n# List approach: 381.5 MB\n# Generator approach: 0.1 MB\n</code></pre>"},{"location":"chapters/01-python-sql-foundations/#why-this-matters","title":"Why This Matters","text":"<p>In data engineering, you rarely work with data that fits comfortably in memory. Consider:</p> <ul> <li>Log files: A single day of application logs can be 100+ GB</li> <li>Database exports: Exporting a production table might yield billions of rows</li> <li>Stream processing: Data arrives continuously; you can't \"load it all\" because there is no \"all\"</li> </ul> <p>Generators enable streaming data processing\u2014the cornerstone of scalable data pipelines. They let you build systems that process terabytes of data on machines with gigabytes of RAM.</p>"},{"location":"chapters/01-python-sql-foundations/#try-it","title":"Try It","text":"<p>Open a Python REPL and try this:</p> <pre><code># This will crash on most laptops\nnumbers = [i for i in range(1_000_000_000)]\n\n# This works fine\nnumbers = (i for i in range(1_000_000_000))\nfirst_ten = [next(numbers) for _ in range(10)]\nprint(first_ten)  # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n</code></pre> <p>Now imagine you're reading from a database instead of generating numbers. Can you sketch out how you'd write a generator function that yields rows from a SQL query one at a time?</p>"},{"location":"chapters/01-python-sql-foundations/#section-2-list-comprehensions-and-generator-expressions","title":"Section 2: List Comprehensions and Generator Expressions","text":""},{"location":"chapters/01-python-sql-foundations/#the-pipeline-that-looked-perfect-until-it-wasnt","title":"The Pipeline That Looked Perfect (Until It Wasn't)","text":"<p>Let me tell you about the code review that still makes me cringe.</p> <p>I'd built this beautiful ETL pipeline to process customer records. Clean code. Nicely commented. Each transformation step on its own line. I was proud of this thing. Submitted the PR on Sunday morning, confident my tech lead would approve it by lunch.</p> <p>Instead, I got a single comment: \"This will crash in production. Try it with the real dataset first.\"</p> <p>I was confused. Insulted, even. My code was elegant! Look at how readable these chained transformations were:</p> <pre><code>lines = f.readlines()\nrecords = [parse_customer(line) for line in lines]\nnormalized = [normalize_phone(r) for r in records]\nvalid = [r for r in normalized if is_valid_email(r['email'])]\n</code></pre> <p>So I did what she asked. Downloaded the actual production customer database. 18 million records. Hit run.</p> <p>My laptop froze. Not \"spinning beach ball\" frozen. Full kernel panic frozen. Had to force restart. Lost my unsaved code. (Yes, I learned about git commits that day too.)</p> <p>Turns out, each of those innocent-looking list comprehensions was creating a complete copy of 18 million records in memory. Four complete copies. My poor laptop with its 16GB of RAM never stood a chance.</p> <p>My tech lead showed me the fix:</p> <pre><code>lines = f  # Don't even call readlines()\nrecords = (parse_customer(line) for line in lines)\nnormalized = ({**r, 'phone': normalize_phone(r['phone'])} for r in records)\nvalid = (r for r in normalized if is_valid_email(r['email']))\n</code></pre> <p>Same logic. Different brackets. Memory usage went from \"kernel panic\" to \"barely noticeable.\"</p> <p>The kicker? She said \"Now you understand why we don't let juniors merge to main without testing on real data.\" I didn't deploy to production that time. I crashed my own laptop in the safety of my bedroom.</p>"},{"location":"chapters/01-python-sql-foundations/#heres-what-i-learned-choose-your-brackets-wisely","title":"Here's What I Learned: Choose Your Brackets Wisely","text":"<p>List comprehensions and generator expressions provide concise, readable syntax for transforming and filtering data. Choose list comprehensions when you need to reuse the data; choose generator expressions when you're processing it once.</p>"},{"location":"chapters/01-python-sql-foundations/#example-data-transformation-pipeline","title":"Example: Data Transformation Pipeline","text":"<p>You're building an ETL pipeline that needs to: 1. Read customer records from a CSV 2. Parse and validate each record 3. Normalize phone numbers 4. Filter out invalid emails 5. Output to a database</p> <p>Readable but Inefficient:</p> <pre><code>def process_customers_verbose(filename):\n    # Step 1: Read all data\n    with open(filename) as f:\n        lines = f.readlines()\n\n    # Step 2: Parse\n    records = []\n    for line in lines:\n        records.append(parse_customer(line))\n\n    # Step 3: Normalize phones\n    normalized = []\n    for record in records:\n        record['phone'] = normalize_phone(record['phone'])\n        normalized.append(record)\n\n    # Step 4: Filter\n    valid = []\n    for record in normalized:\n        if is_valid_email(record['email']):\n            valid.append(record)\n\n    return valid\n</code></pre> <p>Pythonic and Efficient:</p> <pre><code>def process_customers_pythonic(filename):\n    with open(filename) as f:\n        # Chained generator expressions - each step processes one item at a time\n        records = (parse_customer(line) for line in f)\n        normalized = (\n            {**record, 'phone': normalize_phone(record['phone'])}\n            for record in records\n        )\n        valid = (\n            record for record in normalized\n            if is_valid_email(record['email'])\n        )\n\n        # Only when we iterate over 'valid' does any processing happen\n        for record in valid:\n            insert_to_database(record)\n</code></pre> <p>Notice how the second version: - Uses less memory (no intermediate lists) - Is more readable (each transformation is one line) - Is more composable (easy to add/remove steps)</p>"},{"location":"chapters/01-python-sql-foundations/#when-to-use-each","title":"When to Use Each","text":"<p>Use list comprehensions when: - You need to iterate over the data multiple times - The dataset is small enough to fit in memory - You need to check the length or access by index</p> <pre><code># Good use of list comprehension\nuser_ids = [row['user_id'] for row in users]\nprint(f\"Processing {len(user_ids)} users\")  # Need length\nfor user_id in user_ids:\n    process(user_id)\nfor user_id in user_ids:  # Iterate again\n    cleanup(user_id)\n</code></pre> <p>Use generator expressions when: - You process each item exactly once - The dataset is large - You're feeding the data into another function (like <code>sum()</code>, <code>max()</code>, or database insert)</p> <pre><code># Good use of generator expression\ntotal_revenue = sum(\n    order['amount']\n    for order in read_orders('orders.csv')\n    if order['status'] == 'completed'\n)\n</code></pre>"},{"location":"chapters/01-python-sql-foundations/#try-it_1","title":"Try It","text":"<p>Let's say you have a list of URLs and you want to extract the domain names. Try writing both versions:</p> <pre><code>urls = [\n    'https://example.com/page1',\n    'https://test.org/page2',\n    'https://example.com/page3'\n]\n\n# Using list comprehension\ndomains_list = [url.split('/')[2] for url in urls]\n\n# Using generator expression\ndomains_gen = (url.split('/')[2] for url in urls)\n\n# What happens if you print them?\nprint(domains_list)  # ['example.com', 'test.org', 'example.com']\nprint(domains_gen)   # &lt;generator object at 0x...&gt;\n\n# To see generator values, convert to list or iterate\nprint(list(domains_gen))  # Now you see the values\n</code></pre> <p>Which one would you use if you had 10 million URLs and just wanted to count unique domains?</p>"},{"location":"chapters/01-python-sql-foundations/#section-3-sql-window-functions-analytics-without-self-joins","title":"Section 3: SQL Window Functions - Analytics Without Self-Joins","text":""},{"location":"chapters/01-python-sql-foundations/#the-query-that-took-three-days-and-shouldve-taken-three-minutes","title":"The Query That Took Three Days (And Should've Taken Three Minutes)","text":"<p>Thursday afternoon. The VP of Product walks over to my desk. Never a good sign.</p> <p>\"Hey, can you pull a quick report? For each customer, show their order history with a running total of spending and rank each order by amount. Need it for tomorrow's board meeting.\"</p> <p>\"Sure,\" I said. \"Quick report\" sounded easy. How hard could it be?</p> <p>Six hours later, I was still at the office. My query was a monster. Multiple subqueries. Three self-joins. Temporary tables everywhere. It looked like SQL had a fight with itself and lost.</p> <p>Worse? It was slow. Running it on our orders table (about 2 million rows) took 45 minutes. And it kept timing out.</p> <p>I did what any desperate engineer does at 11 PM on a Thursday: I posted in the company Slack. \"Anyone know how to make this faster? \ud83c\udd98\"</p> <p>Our senior data analyst\u2014bless her\u2014responded immediately: \"Are you seriously not using window functions for this?\"</p> <p>Window functions? I'd heard of them. Vaguely. Hadn't really bothered learning them because, you know, I could do everything with JOINs and GROUP BY. Right?</p> <p>She sent me a query. Same exact output. One-tenth the lines of code. Ran in 14 seconds.</p> <pre><code>SELECT\n    customer_id,\n    amount,\n    SUM(amount) OVER (PARTITION BY customer_id ORDER BY order_date) as running_total,\n    RANK() OVER (PARTITION BY customer_id ORDER BY amount DESC) as amount_rank\nFROM orders\nORDER BY customer_id, order_date;\n</code></pre> <p>I stared at it. That's it? That's the whole thing?</p> <p>Turns out, I'd spent three days trying to solve a problem that SQL had a built-in solution for. The database could calculate all those running totals and rankings in a single pass over the data. My monstrous JOIN-based approach was making it scan the table multiple times.</p> <p>Made the board meeting. Barely. Never forgot window functions again.</p>"},{"location":"chapters/01-python-sql-foundations/#heres-what-window-functions-actually-do","title":"Here's What Window Functions Actually Do","text":"<p>Window functions let you perform calculations across related rows without grouping them together. They're essential for analytics queries like running totals, rankings, and period-over-period comparisons.</p>"},{"location":"chapters/01-python-sql-foundations/#example-e-commerce-analytics-dashboard","title":"Example: E-Commerce Analytics Dashboard","text":"<p>Your product manager asks: \"For each customer, show their order history with a running total of spending and a rank for each order by amount.\"</p> <p>The Hard Way (Multiple Queries or Self-Joins):</p> <pre><code>-- First, get orders with rankings (requires complex subquery)\nSELECT\n    o1.customer_id,\n    o1.order_id,\n    o1.order_date,\n    o1.amount,\n    COUNT(o2.order_id) as rank_by_amount\nFROM orders o1\nLEFT JOIN orders o2\n    ON o1.customer_id = o2.customer_id\n    AND o2.amount &gt;= o1.amount\nGROUP BY o1.customer_id, o1.order_id, o1.order_date, o1.amount\nORDER BY o1.customer_id, rank_by_amount;\n\n-- Then you'd need another query for running totals...\n</code></pre> <p>This query is hard to read, hard to maintain, and slow (multiple passes over the data).</p> <p>The Window Function Way:</p> <pre><code>SELECT\n    customer_id,\n    order_id,\n    order_date,\n    amount,\n    -- Running total of spending per customer\n    SUM(amount) OVER (\n        PARTITION BY customer_id\n        ORDER BY order_date\n    ) as running_total,\n    -- Rank orders by amount within each customer\n    RANK() OVER (\n        PARTITION BY customer_id\n        ORDER BY amount DESC\n    ) as amount_rank,\n    -- Compare to previous order\n    LAG(amount) OVER (\n        PARTITION BY customer_id\n        ORDER BY order_date\n    ) as previous_order_amount\nFROM orders\nORDER BY customer_id, order_date;\n</code></pre> <p>Sample Output:</p> <pre><code>customer_id | order_id | order_date | amount | running_total | amount_rank | previous_order_amount\n------------|----------|------------|--------|---------------|-------------|----------------------\n1001        | 5001     | 2024-01-15 | 50.00  | 50.00         | 3           | NULL\n1001        | 5002     | 2024-02-20 | 120.00 | 170.00        | 1           | 50.00\n1001        | 5003     | 2024-03-10 | 75.00  | 245.00        | 2           | 120.00\n1002        | 5004     | 2024-01-18 | 200.00 | 200.00        | 1           | NULL\n1002        | 5005     | 2024-02-22 | 150.00 | 350.00        | 2           | 200.00\n</code></pre>"},{"location":"chapters/01-python-sql-foundations/#understanding-the-over-clause","title":"Understanding the OVER Clause","text":"<p>The <code>OVER</code> clause defines the \"window\" of rows for the calculation:</p> <ul> <li>PARTITION BY: Divides rows into groups (like GROUP BY, but without collapsing rows)</li> <li>ORDER BY: Defines the order within each partition</li> <li>Frame specification (optional): Defines exactly which rows to include (e.g., \"last 7 days\")</li> </ul> <pre><code>-- Running total for last 7 days\nSUM(amount) OVER (\n    PARTITION BY customer_id\n    ORDER BY order_date\n    ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n)\n</code></pre>"},{"location":"chapters/01-python-sql-foundations/#common-window-functions","title":"Common Window Functions","text":"<p>Ranking Functions: <pre><code>-- No gaps in ranking (1, 2, 3, 4)\nROW_NUMBER() OVER (ORDER BY amount DESC)\n\n-- Gaps when ties exist (1, 2, 2, 4)\nRANK() OVER (ORDER BY amount DESC)\n\n-- No gaps even with ties (1, 2, 2, 3)\nDENSE_RANK() OVER (ORDER BY amount DESC)\n</code></pre></p> <p>Offset Functions: <pre><code>-- Previous row value\nLAG(amount, 1) OVER (ORDER BY order_date)\n\n-- Next row value\nLEAD(amount, 1) OVER (ORDER BY order_date)\n\n-- First value in partition\nFIRST_VALUE(amount) OVER (PARTITION BY customer_id ORDER BY order_date)\n</code></pre></p> <p>Aggregate Functions as Window Functions: <pre><code>-- Average of partition\nAVG(amount) OVER (PARTITION BY customer_id)\n\n-- Count within window\nCOUNT(*) OVER (PARTITION BY customer_id ORDER BY order_date)\n</code></pre></p>"},{"location":"chapters/01-python-sql-foundations/#why-this-matters_1","title":"Why This Matters","text":"<p>Window functions are crucial for: - Real-time dashboards: Calculate metrics without pre-aggregation - Time-series analysis: Period-over-period comparisons, moving averages - Ranking and percentiles: Top N per category, quartile analysis - Deduplication: Use <code>ROW_NUMBER()</code> to remove duplicates</p> <p>Most importantly, they're much faster than self-joins or multiple queries because the database engine makes a single pass over the data.</p>"},{"location":"chapters/01-python-sql-foundations/#try-it_2","title":"Try It","text":"<p>Given this products table:</p> <pre><code>CREATE TABLE products (\n    category VARCHAR(50),\n    product_name VARCHAR(100),\n    price DECIMAL(10,2),\n    sales_count INT\n);\n</code></pre> <p>Write a query that shows: 1. Each product's rank by price within its category 2. The price difference from the most expensive product in the category 3. What percentage of total category sales this product represents</p> Solution <pre><code>SELECT\n    category,\n    product_name,\n    price,\n    sales_count,\n    RANK() OVER (PARTITION BY category ORDER BY price DESC) as price_rank,\n    FIRST_VALUE(price) OVER (PARTITION BY category ORDER BY price DESC) - price as price_gap,\n    ROUND(100.0 * sales_count / SUM(sales_count) OVER (PARTITION BY category), 2) as pct_category_sales\nFROM products\nORDER BY category, price DESC;\n</code></pre>"},{"location":"chapters/01-python-sql-foundations/#section-4-common-table-expressions-ctes-writing-readable-sql","title":"Section 4: Common Table Expressions (CTEs) - Writing Readable SQL","text":""},{"location":"chapters/01-python-sql-foundations/#the-query-my-future-self-couldnt-understand","title":"The Query My Future Self Couldn't Understand","text":"<p>Picture this: You write a complex SQL query. It works perfectly. You're a genius. You commit it and move on to the next task.</p> <p>Six months later, that query breaks. Your manager asks you to fix it. You open the file and... you have absolutely no idea what it does.</p> <p>This happened to me. Except it was only three weeks later, not six months. And the person who wrote it? Past me. And past me was apparently some kind of sadist who hated future me.</p> <p>Here's what I found:</p> <pre><code>SELECT customer_id, total_spent, spending_rank\nFROM (\n    SELECT customer_id, SUM(amount) as total_spent\n    FROM orders\n    WHERE order_date &gt;= CURRENT_DATE - INTERVAL '30 days'\n    GROUP BY customer_id\n    HAVING SUM(amount) &gt; (\n        SELECT AVG(customer_total)\n        FROM (\n            SELECT SUM(amount) as customer_total\n            FROM orders\n            WHERE order_date &gt;= CURRENT_DATE - INTERVAL '30 days'\n            GROUP BY customer_id\n        ) as avg_calc\n    )\n) as high_spenders\nORDER BY spending_rank;\n</code></pre> <p>I stared at this for twenty minutes. Subqueries inside subqueries inside subqueries. It was like SQL Inception. Every time I thought I understood one level, I'd realize there was another one nested inside.</p> <p>The worst part? The query was broken because I needed to add one simple filter. But I couldn't figure out where to add it without breaking the whole house of cards.</p> <p>Our database admin walked by, saw me mumbling to myself, and took pity. She rewrote it in five minutes:</p> <pre><code>WITH recent_orders AS (\n    SELECT customer_id, amount\n    FROM orders\n    WHERE order_date &gt;= CURRENT_DATE - INTERVAL '30 days'\n),\ncustomer_totals AS (\n    SELECT customer_id, SUM(amount) as total_spent\n    FROM recent_orders\n    GROUP BY customer_id\n),\navg_spending AS (\n    SELECT AVG(total_spent) as avg_total\n    FROM customer_totals\n),\nhigh_spenders AS (\n    SELECT ct.customer_id, ct.total_spent\n    FROM customer_totals ct\n    CROSS JOIN avg_spending avg\n    WHERE ct.total_spent &gt; avg.avg_total\n)\nSELECT customer_id, total_spent,\n       RANK() OVER (ORDER BY total_spent DESC) as spending_rank\nFROM high_spenders\nORDER BY spending_rank;\n</code></pre> <p>Same exact result. But now I could read it. Each CTE was a named step. I could test them individually. I could understand what each piece did. And most importantly, I could add that filter in about 10 seconds because I knew exactly where it belonged.</p> <p>She said, \"Write SQL like you're leaving notes for yourself. Because you are.\"</p> <p>That changed everything.</p>"},{"location":"chapters/01-python-sql-foundations/#heres-why-ctes-are-your-friend","title":"Here's Why CTEs Are Your Friend","text":"<p>CTEs let you break complex queries into logical, named steps\u2014like writing functions in SQL. They make your queries easier to understand, debug, and maintain.</p>"},{"location":"chapters/01-python-sql-foundations/#example-multi-step-business-logic","title":"Example: Multi-Step Business Logic","text":"<p>Your analytics team needs a report showing: 1. Customers who placed orders in the last 30 days 2. Their total spending in that period 3. Only customers who spent more than the average 4. Ranked by total spending</p> <p>The Nested Subquery Nightmare:</p> <pre><code>SELECT\n    customer_id,\n    total_spent,\n    RANK() OVER (ORDER BY total_spent DESC) as spending_rank\nFROM (\n    SELECT\n        customer_id,\n        SUM(amount) as total_spent\n    FROM orders\n    WHERE order_date &gt;= CURRENT_DATE - INTERVAL '30 days'\n    GROUP BY customer_id\n    HAVING SUM(amount) &gt; (\n        SELECT AVG(customer_total)\n        FROM (\n            SELECT SUM(amount) as customer_total\n            FROM orders\n            WHERE order_date &gt;= CURRENT_DATE - INTERVAL '30 days'\n            GROUP BY customer_id\n        ) as avg_calc\n    )\n) as high_spenders\nORDER BY spending_rank;\n</code></pre> <p>This query works, but it's a mess. Try debugging it. Try explaining it to a colleague. Try modifying it six months from now.</p> <p>The CTE Way:</p> <pre><code>WITH recent_orders AS (\n    -- Step 1: Get recent orders\n    SELECT\n        customer_id,\n        amount,\n        order_date\n    FROM orders\n    WHERE order_date &gt;= CURRENT_DATE - INTERVAL '30 days'\n),\ncustomer_totals AS (\n    -- Step 2: Calculate total spending per customer\n    SELECT\n        customer_id,\n        SUM(amount) as total_spent,\n        COUNT(*) as order_count\n    FROM recent_orders\n    GROUP BY customer_id\n),\navg_spending AS (\n    -- Step 3: Calculate average spending\n    SELECT AVG(total_spent) as avg_total\n    FROM customer_totals\n),\nhigh_spenders AS (\n    -- Step 4: Filter for above-average spenders\n    SELECT\n        ct.customer_id,\n        ct.total_spent,\n        ct.order_count\n    FROM customer_totals ct\n    CROSS JOIN avg_spending avg\n    WHERE ct.total_spent &gt; avg.avg_total\n)\n-- Final query\nSELECT\n    customer_id,\n    total_spent,\n    order_count,\n    RANK() OVER (ORDER BY total_spent DESC) as spending_rank\nFROM high_spenders\nORDER BY spending_rank;\n</code></pre> <p>Each CTE is a named, reusable query. You can: - Reference it multiple times (like a variable) - Read the logic step by step (like functions) - Test each step independently (copy the CTE and add a SELECT)</p>"},{"location":"chapters/01-python-sql-foundations/#recursive-ctes-handling-hierarchical-data","title":"Recursive CTEs: Handling Hierarchical Data","text":"<p>CTEs can be recursive, which is powerful for hierarchical data like org charts, category trees, or graph traversals.</p> <p>Example: Employee Hierarchy</p> <pre><code>WITH RECURSIVE employee_hierarchy AS (\n    -- Base case: start with CEO (no manager)\n    SELECT\n        employee_id,\n        employee_name,\n        manager_id,\n        1 as level,\n        employee_name as path\n    FROM employees\n    WHERE manager_id IS NULL\n\n    UNION ALL\n\n    -- Recursive case: find direct reports\n    SELECT\n        e.employee_id,\n        e.employee_name,\n        e.manager_id,\n        eh.level + 1,\n        eh.path || ' &gt; ' || e.employee_name\n    FROM employees e\n    JOIN employee_hierarchy eh ON e.manager_id = eh.employee_id\n)\nSELECT\n    employee_id,\n    REPEAT('  ', level - 1) || employee_name as org_chart,\n    level,\n    path\nFROM employee_hierarchy\nORDER BY path;\n</code></pre> <p>Output: <pre><code>employee_id | org_chart           | level | path\n------------|---------------------|-------|---------------------------\n1           | Alice (CEO)         | 1     | Alice (CEO)\n2           |   Bob (VP)          | 2     | Alice (CEO) &gt; Bob (VP)\n4           |     Dana (Manager)  | 3     | Alice (CEO) &gt; Bob (VP) &gt; Dana (Manager)\n7           |       Frank (IC)    | 4     | Alice (CEO) &gt; Bob (VP) &gt; Dana (Manager) &gt; Frank (IC)\n</code></pre></p>"},{"location":"chapters/01-python-sql-foundations/#why-this-matters_2","title":"Why This Matters","text":"<p>In data engineering, you often work with complex business logic: - Multi-stage data transformations - Incremental updates (e.g., \"process only new records since last run\") - Data quality checks at each step - Historical vs. current data comparisons</p> <p>CTEs help you: - Debug faster: Test each step independently - Optimize selectively: Profile which CTE is slow, optimize that one - Collaborate better: Colleagues can understand your queries - Refactor safely: Change one CTE without breaking others</p>"},{"location":"chapters/01-python-sql-foundations/#try-it_3","title":"Try It","text":"<p>You have these tables:</p> <pre><code>-- users: user_id, signup_date, country\n-- orders: order_id, user_id, order_date, amount\n-- products: product_id, product_name, category\n-- order_items: order_id, product_id, quantity\n</code></pre> <p>Write a CTE-based query to find: 1. Users who signed up in the last 90 days 2. Made at least 3 orders 3. Purchased from at least 2 different product categories 4. Show their total spending and favorite category (by $ spent)</p> Solution Framework <pre><code>WITH recent_users AS (\n    -- Step 1: Get users who signed up in last 90 days\n    SELECT user_id, signup_date, country\n    FROM users\n    WHERE signup_date &gt;= CURRENT_DATE - INTERVAL '90 days'\n),\nuser_orders AS (\n    -- Step 2: Get their orders\n    SELECT ru.user_id, o.order_id, o.amount\n    FROM recent_users ru\n    JOIN orders o ON ru.user_id = o.user_id\n),\nuser_categories AS (\n    -- Step 3: Find categories purchased\n    SELECT\n        uo.user_id,\n        p.category,\n        SUM(uo.amount) as category_spending\n    FROM user_orders uo\n    JOIN order_items oi ON uo.order_id = oi.order_id\n    JOIN products p ON oi.product_id = p.product_id\n    GROUP BY uo.user_id, p.category\n),\nqualified_users AS (\n    -- Step 4: Filter for users meeting criteria\n    SELECT\n        user_id,\n        COUNT(DISTINCT category) as categories_purchased,\n        SUM(category_spending) as total_spending\n    FROM user_categories\n    GROUP BY user_id\n    HAVING\n        COUNT(DISTINCT category) &gt;= 2\n        AND (SELECT COUNT(*) FROM user_orders WHERE user_id = user_categories.user_id) &gt;= 3\n)\n-- Continue from here...\nSELECT * FROM qualified_users;\n</code></pre>"},{"location":"chapters/01-python-sql-foundations/#scars-ive-earned-so-you-dont-have-to","title":"Scars I've Earned (So You Don't Have To)","text":""},{"location":"chapters/01-python-sql-foundations/#1-using-list-comprehensions-when-you-need-generators","title":"1. Using List Comprehensions When You Need Generators","text":"<p>The Mistake: <pre><code># Loading 1 billion records into memory\ndata = [row for row in read_database('SELECT * FROM huge_table')]\n</code></pre></p> <p>The Fix: <pre><code># Processing one row at a time\ndata = (row for row in read_database('SELECT * FROM huge_table'))\nfor row in data:\n    process(row)\n</code></pre></p> <p>When to worry: Any time your data source is larger than available RAM, or when you're processing data exactly once.</p> <p>What it cost me: $47,000 in AWS bills and a very awkward conversation with my CTO.</p>"},{"location":"chapters/01-python-sql-foundations/#2-forgetting-partition-by-in-window-functions","title":"2. Forgetting PARTITION BY in Window Functions","text":"<p>The Mistake: <pre><code>-- This calculates running total across ALL customers\nSELECT\n    customer_id,\n    SUM(amount) OVER (ORDER BY order_date) as running_total\nFROM orders;\n</code></pre></p> <p>The Fix: <pre><code>-- Running total per customer\nSELECT\n    customer_id,\n    SUM(amount) OVER (\n        PARTITION BY customer_id\n        ORDER BY order_date\n    ) as running_total\nFROM orders;\n</code></pre></p> <p>What it cost me: Three days debugging why customer totals were wildly wrong, one very confused VP of Product, and the nickname \"Overflow\" (because my totals kept overflowing into other customers).</p>"},{"location":"chapters/01-python-sql-foundations/#3-over-nesting-ctes","title":"3. Over-Nesting CTEs","text":"<p>The Mistake: <pre><code>WITH step1 AS (...),\n     step2 AS (SELECT * FROM step1),\n     step3 AS (SELECT * FROM step2),\n     step4 AS (SELECT * FROM step3)\n     -- 10 more steps...\n</code></pre></p> <p>The Fix: If you have more than 5-6 CTEs, consider breaking the query into multiple steps or creating a temporary table for intermediate results. CTEs are great for readability, but too many can hurt performance and become hard to follow.</p> <p>What it cost me: A query that took 2 hours to run when it should've taken 5 minutes. Turns out, the query optimizer gave up and just did what I said instead of optimizing it.</p>"},{"location":"chapters/01-python-sql-foundations/#4-not-understanding-generator-exhaustion","title":"4. Not Understanding Generator Exhaustion","text":"<p>The Mistake: <pre><code>data = (x for x in range(1000))\nprint(sum(data))  # 499500\nprint(sum(data))  # 0 (generator is exhausted!)\n</code></pre></p> <p>The Fix: Either recreate the generator, or use a list if you need multiple iterations: <pre><code># Option 1: Recreate\ndata = (x for x in range(1000))\nprint(sum(data))\ndata = (x for x in range(1000))  # Recreate\nprint(sum(data))\n\n# Option 2: Use list if data is small\ndata = list(range(1000))\nprint(sum(data))\nprint(sum(data))  # Works fine\n</code></pre></p> <p>What it cost me: Two hours debugging why my second calculation always returned zero. Felt like an idiot when I figured it out.</p>"},{"location":"chapters/01-python-sql-foundations/#reflection-questions","title":"Reflection Questions","text":"<ol> <li>When would you NOT use a generator?</li> </ol> <p>Think about these scenarios:    - You need to access the length of the data    - You need to iterate over the data multiple times    - You need random access (indexing)    - The dataset is small and you want faster access</p> <p>What's the trade-off you're making in each case?</p> <ol> <li>Your query with window functions is slow. What would you check first?</li> </ol> <p>Hint: Window functions require sorting. What does sorting require? What if your PARTITION BY column isn't indexed?</p> <ol> <li>You have a 10-step data transformation pipeline. When should you use CTEs vs. temporary tables vs. intermediate files?</li> </ol> <p>Consider:    - How long does each step take?    - Do you need to inspect intermediate results?    - Are you running this once or repeatedly?    - What if step 7 fails\u2014do you want to recompute steps 1-6?</p> <ol> <li>A colleague writes a query with five self-joins to calculate ranks and running totals. You suggest window functions. They say, \"But my way works.\" How do you convince them?</li> </ol> <p>This is about more than performance\u2014it's about maintainability, readability, and what happens when the data grows 10x next year.</p>"},{"location":"chapters/01-python-sql-foundations/#summary","title":"Summary","text":"<ul> <li> <p>Generators enable streaming data processing, allowing you to work with datasets larger than memory by processing one item at a time. Use generator expressions <code>(x for x in ...)</code> when you only need to iterate once.</p> </li> <li> <p>List comprehensions are for reusable data that fits in memory. Use <code>[x for x in ...]</code> when you need to iterate multiple times, check length, or use indexing.</p> </li> <li> <p>Window functions eliminate self-joins and enable advanced analytics like running totals, rankings, and period-over-period comparisons in a single query pass.</p> </li> <li> <p>CTEs break complex queries into logical steps, making SQL more readable, debuggable, and maintainable. Use them to structure multi-step business logic.</p> </li> <li> <p>Performance matters at scale: Code that works on 1,000 rows may fail at 1,000,000. Always consider memory usage, query execution plans, and whether your solution will scale.</p> </li> <li> <p>Learn from mistakes quickly: The best engineers aren't the ones who never mess up\u2014they're the ones who mess up, learn fast, and share the lessons so others don't have to repeat them.</p> </li> </ul>"},{"location":"chapters/01-python-sql-foundations/#next-steps","title":"Next Steps","text":"<p>Now that you have Python and SQL foundations, it's time to think about collaboration and deployment. In Chapter 2, we'll explore Git workflows for team development and Docker for creating reproducible, portable environments. You'll learn how to:</p> <ul> <li>Manage merge conflicts when multiple engineers work on the same pipeline</li> <li>Use branching strategies for safe feature development</li> <li>Containerize your data pipelines for consistent execution across environments</li> <li>Orchestrate multi-container systems with Docker Compose</li> </ul> <p>The patterns you learned here\u2014streaming processing, query optimization, readable code structure\u2014will carry forward. But now we need to ensure your carefully crafted pipelines work the same way on your laptop, your colleague's machine, and in production.</p> <p>And maybe, just maybe, you'll avoid deploying to production on a Thursday afternoon.</p>"},{"location":"chapters/01-python-sql-foundations/quiz/","title":"Chapter 1 Quiz: Python &amp; SQL Foundations","text":""},{"location":"chapters/01-python-sql-foundations/quiz/#1-what-is-the-primary-advantage-of-using-a-generator-instead-of-a-list-comprehension-for-processing-large-datasets","title":"1. What is the primary advantage of using a generator instead of a list comprehension for processing large datasets?","text":"<ol> <li>Generators execute faster than list comprehensions</li> <li>Generators use lazy evaluation and consume less memory</li> <li>Generators can only be iterated once, ensuring data integrity</li> <li>Generators automatically parallelize operations across CPU cores</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Generators use lazy evaluation, producing values one at a time only when requested, which significantly reduces memory consumption for large datasets. List comprehensions create the entire list in memory immediately. Option A is incorrect because execution speed is comparable; the main benefit is memory efficiency. Option C describes a characteristic but not a benefit. Option D is incorrect as generators do not automatically parallelize\u2014that requires explicit threading or multiprocessing libraries.</p> <p>Concept: Generators and Lazy Evaluation</p>"},{"location":"chapters/01-python-sql-foundations/quiz/#2-which-of-the-following-best-describes-the-purpose-of-a-context-manager-in-python","title":"2. Which of the following best describes the purpose of a context manager in Python?","text":"<ol> <li>It manages the flow of control between different functions</li> <li>It ensures proper acquisition and release of resources</li> <li>It optimizes memory allocation for data structures</li> <li>It monitors the execution context of decorators</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Context managers (using the <code>with</code> statement) ensure that resources like file handles, database connections, or locks are properly acquired before use and released afterward, even if an exception occurs. This prevents resource leaks. Option A confuses context managers with control flow statements. Option C is incorrect; memory management is handled elsewhere. Option D incorrectly conflates context managers with decorators.</p> <p>Concept: Context Managers</p>"},{"location":"chapters/01-python-sql-foundations/quiz/#3-in-a-sql-window-function-what-does-the-over-clause-define","title":"3. In a SQL window function, what does the OVER clause define?","text":"<ol> <li>The filtering criteria that determines which rows are included</li> <li>The frame of rows over which the function is calculated</li> <li>The sort order for the entire query result set</li> <li>The join condition between multiple tables</li> </ol> Show Answer <p>The correct answer is B.</p> <p>The OVER clause specifies the window (partition, ordering, and frame) over which the window function operates. It defines which rows are considered for the calculation. Option A describes the WHERE clause. Option C is partially true but incomplete; ordering is part of the window specification, not the entire result set ordering. Option D relates to JOIN syntax, not window functions.</p> <p>Concept: Window Functions</p>"},{"location":"chapters/01-python-sql-foundations/quiz/#4-how-would-you-write-a-query-to-find-the-running-total-of-sales-amount-ordered-by-date-using-a-window-function","title":"4. How would you write a query to find the running total of sales amount ordered by date using a window function?","text":"<ol> <li><code>SELECT date, sales, SUM(sales) OVER (ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) FROM transactions</code></li> <li><code>SELECT date, sales, SUM(sales) OVER (PARTITION BY date ORDER BY sales) FROM transactions</code></li> <li><code>SELECT date, sales, CUMSUM(sales ORDER BY date) FROM transactions</code></li> <li><code>SELECT date, sales, SUM(sales) OVER (ORDER BY date DESC) FROM transactions</code></li> </ol> Show Answer <p>The correct answer is A.</p> <p>The ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW clause explicitly defines the window frame to include all rows from the start up to the current row, creating a running total. Option B uses PARTITION BY which resets the sum for each date group. Option C uses incorrect syntax; CUMSUM is not a standard SQL function. Option D calculates the total for all rows in reverse order, not a running total.</p> <p>Concept: Window Function Frames</p>"},{"location":"chapters/01-python-sql-foundations/quiz/#5-what-is-the-primary-difference-between-a-common-table-expression-cte-and-a-subquery","title":"5. What is the primary difference between a Common Table Expression (CTE) and a subquery?","text":"<ol> <li>CTEs are stored in the database while subqueries are temporary</li> <li>CTEs are more readable and can be referenced multiple times in the same query</li> <li>Subqueries are faster because they are optimized differently</li> <li>CTEs only work with SELECT statements, not with INSERT or UPDATE</li> </ol> Show Answer <p>The correct answer is B.</p> <p>CTEs (WITH clause) improve query readability and can be referenced multiple times, reducing repetition. While functionally similar, CTEs provide better code organization. Option A is incorrect; both are temporary. Option C is misleading; performance depends on the optimizer, not the structure. Option D is false; CTEs work with INSERT, UPDATE, and DELETE statements.</p> <p>Concept: Common Table Expressions</p>"},{"location":"chapters/01-python-sql-foundations/quiz/#6-a-list-comprehension-x2-for-x-in-range1000000-if-x-2-0-is-used-to-process-a-large-dataset-what-is-a-potential-issue-with-this-approach","title":"6. A list comprehension <code>[x**2 for x in range(1000000) if x % 2 == 0]</code> is used to process a large dataset. What is a potential issue with this approach?","text":"<ol> <li>It will take significantly longer than a traditional for loop</li> <li>It creates the entire list in memory at once, potentially causing memory issues</li> <li>It cannot handle the condition <code>x % 2 == 0</code> properly</li> <li>It will skip every other number due to the if clause</li> </ol> Show Answer <p>The correct answer is B.</p> <p>List comprehensions create the complete list immediately in memory. For 1,000,000 elements, this could consume significant memory. Using a generator expression with parentheses instead would use lazy evaluation. Option A is incorrect; list comprehensions are typically as fast or faster than loops. Option C is incorrect; the if clause works correctly. Option D misunderstands the if clause; it filters, not skips alternating positions.</p> <p>Concept: List Comprehensions vs Generators</p>"},{"location":"chapters/01-python-sql-foundations/quiz/#7-why-might-you-choose-denormalized-data-structures-in-a-data-engineering-context","title":"7. Why might you choose denormalized data structures in a data engineering context?","text":"<ol> <li>Because normalized databases are always slower and less reliable</li> <li>To reduce query complexity and improve read performance at the cost of storage</li> <li>Because denormalized data is always easier to maintain</li> <li>To eliminate the need for indexes and query optimization</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Denormalization is a performance optimization technique that reduces join operations and simplifies queries, though it increases storage and complicates updates. This trade-off is often justified in data warehousing contexts. Option A is an overstatement; normalization has benefits. Option C is false; denormalized data is often harder to maintain due to update anomalies. Option D is incorrect; denormalization doesn't eliminate the need for optimization.</p> <p>Concept: Denormalization Trade-offs</p>"},{"location":"chapters/01-python-sql-foundations/quiz/#8-given-a-dataset-with-columns-user_id-purchase_date-and-amount-how-would-you-calculate-the-average-purchase-amount-per-user","title":"8. Given a dataset with columns <code>user_id</code>, <code>purchase_date</code>, and <code>amount</code>, how would you calculate the average purchase amount per user?","text":"<ol> <li><code>SELECT user_id, AVG(amount) FROM purchases GROUP BY user_id</code></li> <li><code>SELECT user_id, amount / COUNT(*) FROM purchases</code></li> <li><code>SELECT AVG(amount) FROM purchases WHERE user_id IS NOT NULL</code></li> <li><code>SELECT user_id, AVG(DISTINCT amount) FROM purchases GROUP BY user_id</code></li> </ol> Show Answer <p>The correct answer is A.</p> <p>Using GROUP BY with AVG() correctly calculates the average amount per user. Option B divides each amount by the total count, which is incorrect. Option C calculates the global average, not per-user averages. Option D uses AVG(DISTINCT amount), which would only average unique values, not all purchases\u2014an incorrect interpretation of the requirement.</p> <p>Concept: SQL Aggregation Functions</p>"},{"location":"chapters/01-python-sql-foundations/quiz/#9-what-does-a-decorator-in-python-do","title":"9. What does a decorator in Python do?","text":"<ol> <li>It decorates the code with comments to improve readability</li> <li>It modifies the behavior of a function or class without changing its source code</li> <li>It creates a visual representation of code structure</li> <li>It automatically generates documentation for functions</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Decorators wrap functions to modify their behavior (e.g., adding logging, caching, or authentication) without altering the original function code. They use the @ syntax and are applied at definition time. Option A confuses decorators with comments. Option C is incorrect; decorators don't create visualizations. Option D is incorrect; that's the purpose of docstrings or documentation tools.</p> <p>Concept: Decorators</p>"},{"location":"chapters/01-python-sql-foundations/quiz/#10-how-would-you-optimize-a-slow-sql-query-that-returns-results-in-5-seconds","title":"10. How would you optimize a slow SQL query that returns results in 5 seconds?","text":"<ol> <li>Always add an index on every column used in WHERE clauses</li> <li>Rewrite subqueries as CTEs and add appropriate indexes based on execution plan analysis</li> <li>Remove all JOINs and denormalize the tables</li> <li>Increase the database server's RAM to speed up query execution</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Effective optimization involves analyzing the execution plan to identify bottlenecks, then adding targeted indexes and restructuring queries as needed. Option A is excessive; not all columns benefit from indexes. Option C is premature optimization that sacrifices data integrity. Option D doesn't address the root cause if the issue is poor query structure or missing indexes.</p> <p>Concept: Query Optimization</p>"},{"location":"chapters/01-python-sql-foundations/quiz/#question-distribution-summary","title":"Question Distribution Summary","text":"<p>Bloom's Taxonomy: - Remember (25%): Questions 1, 3 = 20% - Understand (30%): Questions 2, 5, 7 = 30% - Apply (30%): Questions 4, 6, 8, 9 = 40% - Analyze (15%): Question 10 = 10%</p> <p>Answer Distribution: - A: 20% (2 questions) - B: 50% (5 questions) - C: 20% (2 questions) - D: 10% (1 question)</p> <p>Note: Answer distribution will be balanced across all 4 quizzes (40 questions total) to achieve target percentages.</p>"},{"location":"chapters/02-git-docker/","title":"Chapter 2: DevOps Foundations - Git &amp; Docker","text":""},{"location":"chapters/02-git-docker/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this chapter, you will be able to: 1. Implement Git branching strategies and resolve merge conflicts safely in team environments 2. Evaluate when to use merge vs. rebase for integrating code changes 3. Build containerized applications using Docker with appropriate base images and multi-stage builds 4. Create multi-container environments with Docker Compose for local development and testing</p>"},{"location":"chapters/02-git-docker/#introduction","title":"Introduction","text":"<p>It's Thursday afternoon. You and Sarah are both working on the same data pipeline. You're adding a new data source. She's fixing a bug in the transformation logic. You both modify <code>etl_pipeline.py</code>. You both push your changes.</p> <pre><code>! [rejected] main -&gt; main (non-fast-forward)\nerror: failed to push some refs to 'origin'\n</code></pre> <p>Your heart sinks. What now? Do you force push and overwrite Sarah's work? Do you manually copy-paste your changes into her version? Do you give up and become a goat farmer?</p> <p>Twenty minutes later, you've successfully merged your changes. But your pipeline that worked perfectly on your laptop throws errors in production: <code>ModuleNotFoundError: No module named 'pandas'</code>. You check\u2014pandas is definitely in <code>requirements.txt</code>. Your DevOps engineer sighs and says the words every developer dreads: \"Works on my machine.\"</p> <p>These aren't just annoyances. They're reliability risks. A production data pipeline that breaks because of conflicting changes or environment differences can cost your company thousands of dollars per hour. This chapter is about making your code collaborative (Git) and reproducible (Docker).</p> <p>By the end, you'll understand not just how to use these tools, but when and why different approaches matter. Let's start with Git\u2014the tool that prevents you from accidentally destroying your teammate's work.</p>"},{"location":"chapters/02-git-docker/#section-1-git-branching-safe-parallel-development","title":"Section 1: Git Branching - Safe Parallel Development","text":""},{"location":"chapters/02-git-docker/#key-idea","title":"Key Idea","text":"<p>Branches let multiple people work on the same codebase simultaneously without interfering with each other. They're essential for team collaboration and safe deployment workflows.</p>"},{"location":"chapters/02-git-docker/#example-feature-development-in-a-team-environment","title":"Example: Feature Development in a Team Environment","text":"<p>You're on a data engineering team building an ETL pipeline. The current production pipeline is in the <code>main</code> branch. You need to add a new feature: loading data from Snowflake. Meanwhile, your teammate needs to fix a bug in the PostgreSQL connector.</p> <p>Without Branches (Dangerous): <pre><code># Both of you work directly on main\n# You modify db_connectors.py\ngit add db_connectors.py\ngit commit -m \"Add Snowflake connector\"\ngit push\n\n# Your teammate tries to push their PostgreSQL fix\ngit push\n# Error! Conflict! Now you need to coordinate who pushes first\n</code></pre></p> <p>With Branches (Safe): <pre><code># You create a feature branch\ngit checkout -b feature/snowflake-connector\n\n# Make your changes\nvim db_connectors.py\n\n# Commit to YOUR branch\ngit add db_connectors.py\ngit commit -m \"Add Snowflake connector with connection pooling\"\ngit push -u origin feature/snowflake-connector\n\n# Meanwhile, your teammate works on THEIR branch\n# (on their machine)\ngit checkout -b fix/postgres-timeout\n# ... make changes ...\ngit push -u origin fix/postgres-timeout\n</code></pre></p> <p>Now you both have isolated workspaces. Your teammate's changes don't affect yours until you explicitly merge them.</p>"},{"location":"chapters/02-git-docker/#branching-strategy-feature-branch-workflow","title":"Branching Strategy: Feature Branch Workflow","text":"<p>A common pattern in data engineering teams:</p> <pre><code>main (production-ready code)\n\u251c\u2500\u2500 develop (integration branch)\n\u2502   \u251c\u2500\u2500 feature/snowflake-connector (you)\n\u2502   \u251c\u2500\u2500 fix/postgres-timeout (teammate 1)\n\u2502   \u2514\u2500\u2500 feature/data-quality-checks (teammate 2)\n</code></pre> <p>The Workflow:</p> <pre><code># 1. Start from develop\ngit checkout develop\ngit pull origin develop\n\n# 2. Create feature branch\ngit checkout -b feature/snowflake-connector\n\n# 3. Work on your feature (many commits over several days)\ngit add src/connectors/snowflake.py\ngit commit -m \"Add basic Snowflake connector\"\n\ngit add tests/test_snowflake.py\ngit commit -m \"Add tests for Snowflake connector\"\n\ngit add src/connectors/snowflake.py\ngit commit -m \"Add connection pooling to Snowflake\"\n\n# 4. Push your branch\ngit push -u origin feature/snowflake-connector\n\n# 5. Open a Pull Request (PR) on GitHub/GitLab\n# Team reviews your code\n\n# 6. After approval, merge to develop\ngit checkout develop\ngit merge feature/snowflake-connector\n\n# 7. Delete feature branch (clean up)\ngit branch -d feature/snowflake-connector\ngit push origin --delete feature/snowflake-connector\n</code></pre>"},{"location":"chapters/02-git-docker/#why-this-matters","title":"Why This Matters","text":"<p>Branches enable: - Parallel work: Multiple features in progress simultaneously - Code review: PRs let teammates review before merging - Safe experimentation: Try ideas without breaking production - Easy rollback: If a feature breaks, just remove the branch</p> <p>In data engineering, this is critical because: - Pipelines run on schedules\u2014you can't break production at 3 AM - Data transformations are complex\u2014code review catches logic errors - Schema changes are risky\u2014branches let you test migrations safely</p>"},{"location":"chapters/02-git-docker/#try-it","title":"Try It","text":"<p>Create a simple project with a branch:</p> <pre><code># Initialize a repo\nmkdir my-pipeline &amp;&amp; cd my-pipeline\ngit init\n\n# Create initial file\necho \"# ETL Pipeline\" &gt; README.md\ngit add README.md\ngit commit -m \"Initial commit\"\n\n# Create a feature branch\ngit checkout -b feature/add-logging\n\n# Make changes\necho \"import logging\" &gt; pipeline.py\ngit add pipeline.py\ngit commit -m \"Add logging module\"\n\n# Switch back to main\ngit checkout main\n\n# Notice pipeline.py doesn't exist here\nls  # Only README.md\n\n# Switch back to feature branch\ngit checkout feature/add-logging\nls  # Now pipeline.py exists\n</code></pre> <p>See how branches create isolated workspaces? Your changes in one branch don't affect other branches until you merge.</p>"},{"location":"chapters/02-git-docker/#section-2-merge-vs-rebase-two-ways-to-integrate-changes","title":"Section 2: Merge vs. Rebase - Two Ways to Integrate Changes","text":""},{"location":"chapters/02-git-docker/#key-idea_1","title":"Key Idea","text":"<p>Both merge and rebase integrate changes from one branch to another, but they create different commit histories. Understanding when to use each is crucial for maintaining a clean, readable Git history.</p>"},{"location":"chapters/02-git-docker/#example-integrating-your-feature-into-main","title":"Example: Integrating Your Feature into Main","text":"<p>You've been working on <code>feature/snowflake-connector</code> for three days. Meanwhile, your team has merged five other PRs into <code>main</code>. Now you need to integrate your work.</p> <p>Your branch history: <pre><code>A---B---C---D  main\n     \\\n      E---F---G  feature/snowflake-connector\n</code></pre></p> <p>Option 1: Merge (Preserves History)</p> <pre><code>git checkout main\ngit merge feature/snowflake-connector\n</code></pre> <p>Result: <pre><code>A---B---C---D---H  main\n     \\         /\n      E---F---G  feature/snowflake-connector\n</code></pre></p> <p>Creates a merge commit (H) that ties the histories together. The history shows exactly when and how branches were integrated.</p> <p>Option 2: Rebase (Linear History)</p> <pre><code>git checkout feature/snowflake-connector\ngit rebase main\n</code></pre> <p>Result: <pre><code>A---B---C---D---E'---F'---G'  feature/snowflake-connector\n</code></pre></p> <p>Replays your commits (E, F, G) on top of the latest main. The prime marks (E', F', G') indicate these are new commits with different hashes\u2014they have the same changes but different parent commits.</p>"},{"location":"chapters/02-git-docker/#visual-comparison-with-real-code","title":"Visual Comparison with Real Code","text":"<p>Imagine this scenario:</p> <p>Initial state: <pre><code># main branch: pipeline.py\ndef load_data():\n    conn = connect_postgres()\n    return conn.query(\"SELECT * FROM users\")\n</code></pre></p> <p>Your feature branch: <pre><code># feature/snowflake-connector: pipeline.py\ndef load_data(source='postgres'):\n    if source == 'postgres':\n        conn = connect_postgres()\n    elif source == 'snowflake':\n        conn = connect_snowflake()  # Your addition\n    return conn.query(\"SELECT * FROM users\")\n</code></pre></p> <p>Meanwhile, main branch evolved: <pre><code># main branch: pipeline.py (someone added error handling)\ndef load_data():\n    try:\n        conn = connect_postgres()\n        return conn.query(\"SELECT * FROM users\")\n    except ConnectionError as e:\n        log_error(e)\n        raise\n</code></pre></p> <p>With merge: - Git creates a merge commit that combines both changes - Both histories are preserved - The merge commit shows the integration point</p> <p>With rebase: - Your commits are replayed on top of the new main - Git tries to apply your changes to the new code - The history looks like you made your changes AFTER the error handling was added</p>"},{"location":"chapters/02-git-docker/#when-to-use-each","title":"When to Use Each","text":"<p>Use Merge When: - Working on a team with many contributors - You want to preserve the exact timeline of when changes were made - Working on long-lived feature branches - The branch has already been pushed and others might be using it</p> <pre><code># Safe for shared branches\ngit checkout main\ngit merge feature/snowflake-connector\n</code></pre> <p>Use Rebase When: - Updating your local feature branch with latest main before pushing - You want a clean, linear history - Working on a short-lived personal branch - The branch has NOT been shared with others yet</p> <pre><code># Good for local work-in-progress\ngit checkout feature/snowflake-connector\ngit rebase main\n# Now your changes are on top of latest main\n</code></pre> <p>Golden Rule of Rebasing:</p> <p>Never rebase commits that have been pushed to a shared branch and that others may have based work on.</p> <p>Why? Rebase rewrites history (creates new commits). If someone else has pulled your old commits and built on top of them, rebasing causes chaos.</p>"},{"location":"chapters/02-git-docker/#handling-conflicts","title":"Handling Conflicts","text":"<p>Both merge and rebase can cause conflicts. Here's how to handle them:</p> <p>Merge Conflict: <pre><code>git checkout main\ngit merge feature/snowflake-connector\n\n# Conflict!\nAuto-merging pipeline.py\nCONFLICT (content): Merge conflict in pipeline.py\nAutomatic merge failed; fix conflicts and then commit the result.\n\n# Check conflicting file\ncat pipeline.py\n</code></pre></p> <pre><code>def load_data():\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n    # Current main branch code\n    try:\n        conn = connect_postgres()\n=======\n    # Your feature branch code\n    if source == 'postgres':\n        conn = connect_postgres()\n    elif source == 'snowflake':\n        conn = connect_snowflake()\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature/snowflake-connector\n    return conn.query(\"SELECT * FROM users\")\n</code></pre> <p>Resolution: <pre><code># Edit file to combine both changes\ndef load_data(source='postgres'):\n    try:\n        if source == 'postgres':\n            conn = connect_postgres()\n        elif source == 'snowflake':\n            conn = connect_snowflake()\n        return conn.query(\"SELECT * FROM users\")\n    except ConnectionError as e:\n        log_error(e)\n        raise\n</code></pre></p> <pre><code># Mark as resolved\ngit add pipeline.py\ngit commit -m \"Merge feature/snowflake-connector with error handling\"\n</code></pre>"},{"location":"chapters/02-git-docker/#why-this-matters_1","title":"Why This Matters","text":"<p>In data engineering: - Rebase keeps your ETL pipeline's Git history clean and readable\u2014important when debugging production issues - Merge preserves the exact history of when data schema changes were integrated\u2014crucial for audit trails - Conflicts happen frequently with SQL query changes or schema migrations\u2014knowing how to resolve them prevents deployment delays</p>"},{"location":"chapters/02-git-docker/#try-it_1","title":"Try It","text":"<p>Create a conflict and resolve it:</p> <pre><code># On main branch\necho \"version = 1.0\" &gt; config.py\ngit add config.py\ngit commit -m \"Set version 1.0\"\n\n# Create feature branch\ngit checkout -b feature/add-config\necho \"version = 1.0\\ndatabase = 'postgres'\" &gt; config.py\ngit add config.py\ngit commit -m \"Add database config\"\n\n# Back to main, make conflicting change\ngit checkout main\necho \"version = 1.1\" &gt; config.py\ngit add config.py\ngit commit -m \"Bump version to 1.1\"\n\n# Now try to merge\ngit merge feature/add-config\n# You'll get a conflict! Resolve it by combining both changes\n</code></pre>"},{"location":"chapters/02-git-docker/#section-3-docker-basics-reproducible-environments","title":"Section 3: Docker Basics - Reproducible Environments","text":""},{"location":"chapters/02-git-docker/#key-idea_2","title":"Key Idea","text":"<p>Containers package your application with all its dependencies, ensuring it runs the same way everywhere. This eliminates \"works on my machine\" problems and makes deployments predictable.</p>"},{"location":"chapters/02-git-docker/#example-the-works-on-my-machine-problem","title":"Example: The \"Works on My Machine\" Problem","text":"<p>You've built a data pipeline:</p> <pre><code># pipeline.py\nimport pandas as pd\nimport psycopg2\nfrom snowflake.connector import connect\n\ndef extract_data():\n    # Extract from PostgreSQL\n    conn = psycopg2.connect(\"postgresql://localhost/mydb\")\n    df = pd.read_sql(\"SELECT * FROM orders\", conn)\n    return df\n\ndef transform_data(df):\n    # Data transformations\n    return df.groupby('customer_id').agg({'amount': 'sum'})\n\ndef load_data(df):\n    # Load to Snowflake\n    sf_conn = connect(user='user', password='pass', account='account')\n    # ...\n</code></pre> <p>On your laptop: <pre><code>python pipeline.py\n# Works perfectly!\n</code></pre></p> <p>On the production server: <pre><code>python pipeline.py\n# ImportError: No module named 'pandas'\n# Oh, right, need to install dependencies\n\npip install pandas psycopg2 snowflake-connector-python\npython pipeline.py\n# ModuleNotFoundError: No module named 'psycopg2'\n# Wait, psycopg2 needs PostgreSQL client libraries\n\napt-get install libpq-dev\npip install psycopg2\npython pipeline.py\n# ImportError: cannot import name 'connect' from 'snowflake.connector'\n# Different version of snowflake-connector installed\n</code></pre></p> <p>This is a nightmare. What if you could package your entire environment\u2014Python version, dependencies, system libraries\u2014into a single deployable unit?</p>"},{"location":"chapters/02-git-docker/#enter-docker-your-environment-in-a-box","title":"Enter Docker: Your Environment in a Box","text":"<p>Dockerfile - A recipe for building your environment:</p> <pre><code># Start with Python 3.10 on Ubuntu\nFROM python:3.10-slim\n\n# Install system dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    libpq-dev \\\n    gcc \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Set working directory\nWORKDIR /app\n\n# Copy requirements first (Docker caching optimization)\nCOPY requirements.txt .\n\n# Install Python dependencies\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY pipeline.py .\nCOPY config.py .\n\n# Set environment variables\nENV PYTHONUNBUFFERED=1\n\n# Run the pipeline\nCMD [\"python\", \"pipeline.py\"]\n</code></pre> <p>Build and run: <pre><code># Build the container image\ndocker build -t my-pipeline:v1 .\n\n# Run it\ndocker run my-pipeline:v1\n# Runs exactly the same on any machine with Docker installed\n</code></pre></p>"},{"location":"chapters/02-git-docker/#understanding-docker-concepts","title":"Understanding Docker Concepts","text":"<p>Image vs. Container: - Image: A blueprint (like a class in OOP)\u2014immutable, shareable - Container: A running instance (like an object)\u2014temporary, isolated</p> <pre><code># Build image (once)\ndocker build -t my-pipeline:v1 .\n\n# Run multiple containers from same image\ndocker run --name pipeline-run-1 my-pipeline:v1\ndocker run --name pipeline-run-2 my-pipeline:v1\ndocker run --name pipeline-run-3 my-pipeline:v1\n</code></pre> <p>Layers and Caching:</p> <p>Each line in a Dockerfile creates a layer. Docker caches layers that haven't changed:</p> <pre><code>FROM python:3.10-slim          # Layer 1 (cached if not changed)\nRUN apt-get update ...         # Layer 2 (cached if not changed)\nCOPY requirements.txt .        # Layer 3 (cached if requirements.txt unchanged)\nRUN pip install -r ...         # Layer 4 (cached if Layer 3 cached)\nCOPY pipeline.py .             # Layer 5 (rebuilt if pipeline.py changed)\n</code></pre> <p>This is why we copy <code>requirements.txt</code> before <code>pipeline.py</code>\u2014if you only change your code, Docker doesn't need to reinstall dependencies.</p>"},{"location":"chapters/02-git-docker/#multi-stage-builds-for-efficiency","title":"Multi-Stage Builds for Efficiency","text":"<p>Problem: Your Docker image is 2 GB because it includes build tools you don't need at runtime.</p> <p>Solution: Multi-stage builds.</p> <pre><code># Stage 1: Build environment (includes compilers, build tools)\nFROM python:3.10 AS builder\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --user --no-cache-dir -r requirements.txt\n\n# Stage 2: Runtime environment (minimal)\nFROM python:3.10-slim\n\n# Copy only the installed packages from builder\nCOPY --from=builder /root/.local /root/.local\n\nWORKDIR /app\nCOPY pipeline.py .\n\n# Make sure scripts in .local are usable\nENV PATH=/root/.local/bin:$PATH\n\nCMD [\"python\", \"pipeline.py\"]\n</code></pre> <p>Result: Image size reduced from 2 GB to 400 MB. Faster deployments, lower storage costs.</p>"},{"location":"chapters/02-git-docker/#volumes-persisting-data","title":"Volumes: Persisting Data","text":"<p>Containers are ephemeral\u2014when they stop, data inside them is lost. For data pipelines, you need persistent storage.</p> <pre><code># Run pipeline with volume mount\ndocker run -v /local/data:/app/data my-pipeline:v1\n\n# Now the container can read/write to /app/data\n# And the data persists in /local/data on your host machine\n</code></pre> <p>In Python: <pre><code># pipeline.py\ndef save_results(df):\n    # This saves to /app/data inside container\n    # Which maps to /local/data on host\n    df.to_csv('/app/data/results.csv', index=False)\n</code></pre></p>"},{"location":"chapters/02-git-docker/#environment-variables-for-configuration","title":"Environment Variables for Configuration","text":"<p>Don't hardcode credentials in Dockerfiles:</p> <pre><code># Bad: Credentials in image (security risk!)\nENV DB_PASSWORD=secret123\n\n# Good: Pass at runtime\nENV DB_PASSWORD=\n</code></pre> <pre><code># Pass secrets at runtime\ndocker run \\\n    -e DB_HOST=postgres.example.com \\\n    -e DB_PASSWORD=secret123 \\\n    my-pipeline:v1\n</code></pre> <p>Or use environment file: <pre><code># .env file\nDB_HOST=postgres.example.com\nDB_USER=data_engineer\nDB_PASSWORD=secret123\n\n# Run with env file\ndocker run --env-file .env my-pipeline:v1\n</code></pre></p>"},{"location":"chapters/02-git-docker/#why-this-matters_2","title":"Why This Matters","text":"<p>For data engineers: - Reproducibility: Your pipeline runs identically in dev, staging, and production - Isolation: Different pipelines can use different Python versions without conflicts - Portability: Ship your pipeline to any cloud (AWS, GCP, Azure) or on-prem - Scalability: Orchestrators like Kubernetes run containers\u2014containerizing is the first step to scaling</p>"},{"location":"chapters/02-git-docker/#try-it_2","title":"Try It","text":"<p>Create a simple containerized ETL script:</p> <pre><code># Create project directory\nmkdir docker-pipeline &amp;&amp; cd docker-pipeline\n\n# Create a simple pipeline\ncat &gt; pipeline.py &lt;&lt; 'EOF'\nimport pandas as pd\nimport sys\n\nprint(f\"Python version: {sys.version}\")\nprint(f\"Pandas version: {pd.__version__}\")\n\ndata = {'name': ['Alice', 'Bob'], 'age': [25, 30]}\ndf = pd.DataFrame(data)\nprint(df)\nEOF\n\n# Create requirements\necho \"pandas==2.0.0\" &gt; requirements.txt\n\n# Create Dockerfile\ncat &gt; Dockerfile &lt;&lt; 'EOF'\nFROM python:3.10-slim\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY pipeline.py .\nCMD [\"python\", \"pipeline.py\"]\nEOF\n\n# Build and run\ndocker build -t simple-pipeline .\ndocker run simple-pipeline\n</code></pre>"},{"location":"chapters/02-git-docker/#section-4-docker-compose-multi-container-orchestration","title":"Section 4: Docker Compose - Multi-Container Orchestration","text":""},{"location":"chapters/02-git-docker/#key-idea_3","title":"Key Idea","text":"<p>Real-world applications involve multiple services (database, app, cache). Docker Compose lets you define and run multi-container setups with a single command.</p>"},{"location":"chapters/02-git-docker/#example-etl-pipeline-with-postgresql-database","title":"Example: ETL Pipeline with PostgreSQL Database","text":"<p>Your pipeline needs a PostgreSQL database. Instead of installing PostgreSQL on your laptop, you can run it in a container alongside your pipeline.</p> <p>docker-compose.yml: <pre><code>version: '3.8'\n\nservices:\n  # PostgreSQL database\n  postgres:\n    image: postgres:14\n    environment:\n      POSTGRES_USER: dataeng\n      POSTGRES_PASSWORD: secret\n      POSTGRES_DB: analytics\n    volumes:\n      # Persist data even when container stops\n      - postgres_data:/var/lib/postgresql/data\n      # Load initial schema\n      - ./init.sql:/docker-entrypoint-initdb.d/init.sql\n    ports:\n      - \"5432:5432\"\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U dataeng\"]\n      interval: 5s\n      timeout: 5s\n      retries: 5\n\n  # ETL pipeline\n  pipeline:\n    build: .\n    environment:\n      DB_HOST: postgres  # Docker Compose creates network, use service name\n      DB_USER: dataeng\n      DB_PASSWORD: secret\n      DB_NAME: analytics\n    depends_on:\n      postgres:\n        condition: service_healthy\n    volumes:\n      - ./output:/app/output\n\nvolumes:\n  postgres_data:  # Named volume for persistence\n</code></pre></p> <p>pipeline.py (updated to use environment variables): <pre><code>import os\nimport psycopg2\nimport pandas as pd\n\ndef get_db_connection():\n    return psycopg2.connect(\n        host=os.getenv('DB_HOST', 'localhost'),\n        user=os.getenv('DB_USER', 'dataeng'),\n        password=os.getenv('DB_PASSWORD'),\n        database=os.getenv('DB_NAME', 'analytics')\n    )\n\ndef extract_data():\n    conn = get_db_connection()\n    query = \"\"\"\n        SELECT\n            customer_id,\n            SUM(amount) as total_spent,\n            COUNT(*) as order_count\n        FROM orders\n        WHERE order_date &gt;= CURRENT_DATE - INTERVAL '30 days'\n        GROUP BY customer_id\n    \"\"\"\n    df = pd.read_sql(query, conn)\n    conn.close()\n    return df\n\ndef transform_data(df):\n    # Add customer segment\n    df['segment'] = pd.cut(\n        df['total_spent'],\n        bins=[0, 100, 500, float('inf')],\n        labels=['low', 'medium', 'high']\n    )\n    return df\n\ndef load_data(df):\n    # Save to CSV in mounted volume\n    output_path = '/app/output/customer_segments.csv'\n    df.to_csv(output_path, index=False)\n    print(f\"Results saved to {output_path}\")\n\nif __name__ == \"__main__\":\n    print(\"Starting ETL pipeline...\")\n    raw_data = extract_data()\n    transformed_data = transform_data(raw_data)\n    load_data(transformed_data)\n    print(\"Pipeline completed successfully!\")\n</code></pre></p> <p>init.sql (creates initial schema): <pre><code>CREATE TABLE orders (\n    order_id SERIAL PRIMARY KEY,\n    customer_id INT NOT NULL,\n    amount DECIMAL(10, 2) NOT NULL,\n    order_date DATE NOT NULL\n);\n\n-- Insert sample data\nINSERT INTO orders (customer_id, amount, order_date) VALUES\n(1, 150.00, CURRENT_DATE - INTERVAL '5 days'),\n(1, 200.00, CURRENT_DATE - INTERVAL '3 days'),\n(2, 50.00, CURRENT_DATE - INTERVAL '10 days'),\n(2, 75.00, CURRENT_DATE - INTERVAL '2 days'),\n(3, 1000.00, CURRENT_DATE - INTERVAL '1 day');\n</code></pre></p> <p>Run everything: <pre><code># Start all services\ndocker-compose up\n\n# Run in background\ndocker-compose up -d\n\n# View logs\ndocker-compose logs -f pipeline\n\n# Stop everything\ndocker-compose down\n\n# Stop and remove volumes (fresh start)\ndocker-compose down -v\n</code></pre></p>"},{"location":"chapters/02-git-docker/#docker-compose-features","title":"Docker Compose Features","text":"<p>Networking: Services can communicate using service names: <pre><code># In pipeline.py, use 'postgres' as hostname\nconn = psycopg2.connect(host='postgres', ...)\n</code></pre></p> <p>Docker Compose creates a network where <code>postgres</code> resolves to the database container's IP.</p> <p>Dependency Management: <pre><code>pipeline:\n  depends_on:\n    postgres:\n      condition: service_healthy\n</code></pre></p> <p>This ensures the pipeline doesn't start until PostgreSQL is ready to accept connections.</p> <p>Environment Files: <pre><code>services:\n  pipeline:\n    env_file:\n      - .env.dev      # For development\n      # - .env.prod   # For production\n</code></pre></p> <p>Scaling: <pre><code># Run 3 instances of pipeline\ndocker-compose up --scale pipeline=3\n</code></pre></p>"},{"location":"chapters/02-git-docker/#real-world-example-complete-data-stack","title":"Real-World Example: Complete Data Stack","text":"<p>A more complex setup with multiple data tools:</p> <pre><code>version: '3.8'\n\nservices:\n  # Source database (PostgreSQL)\n  postgres:\n    image: postgres:14\n    environment:\n      POSTGRES_DB: source_db\n      POSTGRES_USER: app\n      POSTGRES_PASSWORD: secret\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n\n  # Data warehouse (PostgreSQL on different port)\n  warehouse:\n    image: postgres:14\n    environment:\n      POSTGRES_DB: warehouse\n      POSTGRES_USER: analyst\n      POSTGRES_PASSWORD: secret\n    ports:\n      - \"5433:5432\"\n    volumes:\n      - warehouse_data:/var/lib/postgresql/data\n\n  # Redis cache\n  redis:\n    image: redis:7-alpine\n    ports:\n      - \"6379:6379\"\n\n  # ETL pipeline\n  etl:\n    build: .\n    environment:\n      SOURCE_DB_HOST: postgres\n      WAREHOUSE_DB_HOST: warehouse\n      REDIS_HOST: redis\n    depends_on:\n      - postgres\n      - warehouse\n      - redis\n\n  # Jupyter for analysis\n  jupyter:\n    image: jupyter/datascience-notebook\n    ports:\n      - \"8888:8888\"\n    environment:\n      JUPYTER_ENABLE_LAB: \"yes\"\n    volumes:\n      - ./notebooks:/home/jovyan/work\n    depends_on:\n      - warehouse\n\nvolumes:\n  postgres_data:\n  warehouse_data:\n</code></pre> <p>Start the entire data platform: <pre><code>docker-compose up -d\n</code></pre></p> <p>Now you have: - Source database (localhost:5432) - Data warehouse (localhost:5433) - Redis cache (localhost:6379) - Jupyter notebook (localhost:8888) - ETL pipeline connecting them all</p>"},{"location":"chapters/02-git-docker/#why-this-matters_3","title":"Why This Matters","text":"<p>Docker Compose enables: - Local development that mirrors production: Same services, same versions - Onboarding: New team members run <code>docker-compose up</code> and have a working environment - Integration testing: Test your pipeline against real databases, not mocks - Experimentation: Try new tools (Redis, MongoDB) without installing them system-wide</p> <p>For data engineers, this means: - Test schema migrations locally before running in production - Develop pipelines that interact with multiple systems (source DB, warehouse, cache) - Share reproducible development environments with your team</p>"},{"location":"chapters/02-git-docker/#try-it_3","title":"Try It","text":"<p>Set up a complete pipeline with database:</p> <pre><code># Create project\nmkdir compose-pipeline &amp;&amp; cd compose-pipeline\n\n# Create docker-compose.yml (use example above)\n# Create pipeline.py (use example above)\n# Create init.sql (use example above)\n# Create Dockerfile\n\ncat &gt; Dockerfile &lt;&lt; 'EOF'\nFROM python:3.10-slim\nRUN apt-get update &amp;&amp; apt-get install -y libpq-dev gcc &amp;&amp; rm -rf /var/lib/apt/lists/*\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY pipeline.py .\nCMD [\"python\", \"pipeline.py\"]\nEOF\n\n# Create requirements.txt\ncat &gt; requirements.txt &lt;&lt; 'EOF'\npandas==2.0.0\npsycopg2-binary==2.9.6\nEOF\n\n# Create output directory\nmkdir output\n\n# Run everything\ndocker-compose up\n\n# Check output\ncat output/customer_segments.csv\n</code></pre>"},{"location":"chapters/02-git-docker/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"chapters/02-git-docker/#1-rebasing-shared-branches","title":"1. Rebasing Shared Branches","text":"<p>Problem: <pre><code># On shared feature branch\ngit checkout feature/shared-work\ngit rebase main  # Rewrites history\ngit push --force  # Breaks everyone else's local copy\n</code></pre></p> <p>Fix: Only rebase local branches that haven't been pushed. For shared branches, use merge.</p>"},{"location":"chapters/02-git-docker/#2-forgetting-dockerignore","title":"2. Forgetting .dockerignore","text":"<p>Problem: <pre><code>COPY . /app  # Copies EVERYTHING, including node_modules, .git, __pycache__\n</code></pre></p> <p>Fix: Create <code>.dockerignore</code>: <pre><code>__pycache__/\n*.pyc\n.git/\n.env\nnode_modules/\n*.log\n.DS_Store\n</code></pre></p>"},{"location":"chapters/02-git-docker/#3-running-containers-as-root","title":"3. Running Containers as Root","text":"<p>Problem: <pre><code># Runs as root user (security risk)\nCMD [\"python\", \"pipeline.py\"]\n</code></pre></p> <p>Fix: Create a non-root user: <pre><code>RUN useradd -m -u 1000 appuser\nUSER appuser\nCMD [\"python\", \"pipeline.py\"]\n</code></pre></p>"},{"location":"chapters/02-git-docker/#4-hardcoding-localhost-in-database-connections","title":"4. Hardcoding Localhost in Database Connections","text":"<p>Problem: <pre><code># Won't work inside Docker container\nconn = psycopg2.connect(host='localhost', ...)\n</code></pre></p> <p>Fix: Use environment variables and service names: <pre><code>conn = psycopg2.connect(\n    host=os.getenv('DB_HOST', 'localhost'),\n    ...\n)\n</code></pre></p>"},{"location":"chapters/02-git-docker/#reflection-questions","title":"Reflection Questions","text":"<ol> <li>When would merge be better than rebase?</li> </ol> <p>Consider:    - You're working on a shared feature branch with 3 other engineers    - You want to preserve the exact timeline of when a critical bug fix was integrated    - You need to demonstrate to auditors exactly when certain data transformations were added</p> <p>What's the cost of having a non-linear history? What do you gain from preserving it?</p> <ol> <li>Your Docker image is 3 GB and takes 10 minutes to build. How would you optimize it?</li> </ol> <p>Think about:    - Are you using the smallest appropriate base image? (alpine vs slim vs full)    - Are you leveraging layer caching properly?    - Could you use multi-stage builds?    - Are you installing development dependencies in production images?</p> <ol> <li>You have a merge conflict in a SQL migration file that alters a production table schema. How do you resolve it safely?</li> </ol> <p>Considerations:    - Both changes might be valid but incompatible    - The order of migrations matters    - Testing the merged result is critical    - You might need to coordinate with the other developer</p> <ol> <li>When would you use Docker Compose vs. Kubernetes?</li> </ol> <p>Docker Compose is great for:    - Local development    - Small deployments (single server)    - Simple multi-container apps</p> <p>Kubernetes is better for:    - Production at scale    - Auto-scaling based on load    - High availability across multiple servers    - Complex orchestration requirements</p> <p>Where does your data pipeline fall on this spectrum?</p>"},{"location":"chapters/02-git-docker/#summary","title":"Summary","text":"<ul> <li> <p>Git branches enable safe parallel development, allowing multiple team members to work on features simultaneously without interfering with each other or breaking production code.</p> </li> <li> <p>Merge preserves history and is safe for shared branches, while rebase creates linear history and is best for local cleanup before pushing. Never rebase shared branches.</p> </li> <li> <p>Merge conflicts are inevitable in team environments. Understanding how to resolve them\u2014especially in data pipeline code and SQL migrations\u2014is a critical skill.</p> </li> <li> <p>Docker containers package your application with all dependencies, eliminating \"works on my machine\" problems and ensuring your pipeline runs identically everywhere.</p> </li> <li> <p>Docker Compose orchestrates multi-container environments, making it easy to run complex data platforms (databases, caches, pipelines) with a single command.</p> </li> <li> <p>Dockerfile best practices\u2014layer caching, multi-stage builds, .dockerignore, non-root users\u2014reduce image size, improve security, and speed up deployments.</p> </li> </ul>"},{"location":"chapters/02-git-docker/#next-steps","title":"Next Steps","text":"<p>You now have the tools for collaborative development (Git) and reproducible environments (Docker). In Chapter 3, we'll dive into database design and modeling. You'll learn:</p> <ul> <li>How to normalize data to prevent anomalies and redundancy</li> <li>When to denormalize for performance</li> <li>Index selection strategies for query optimization</li> <li>Designing schemas that scale from thousands to millions of rows</li> </ul> <p>The Git and Docker skills you learned here will be essential as you work with database migrations, test schema changes locally with Docker Compose, and collaborate with teammates on data model designs. Your ability to safely experiment (branches) and reproduce environments (containers) will make you a more effective database designer.</p>"},{"location":"chapters/02-git-docker/quiz/","title":"Chapter 2 Quiz: DevOps Foundations - Git &amp; Docker","text":""},{"location":"chapters/02-git-docker/quiz/#1-what-is-a-docker-container","title":"1. What is a Docker container?","text":"<ol> <li>A virtual machine that requires a full operating system</li> <li>A lightweight, standalone, executable package containing application code and dependencies</li> <li>A storage mechanism for Docker images</li> <li>A configuration file that defines how to build applications</li> </ol> Show Answer <p>The correct answer is B.</p> <p>A Docker container is a lightweight, isolated runtime environment that includes the application and its dependencies. Unlike virtual machines, containers share the host OS kernel, making them more efficient. Option A describes virtual machines, not containers. Option C describes registries or storage systems. Option D describes a Dockerfile.</p> <p>Concept: Docker Containers</p>"},{"location":"chapters/02-git-docker/quiz/#2-explain-the-relationship-between-a-dockerfile-and-a-docker-image","title":"2. Explain the relationship between a Dockerfile and a Docker image.","text":"<ol> <li>They are the same thing with different names</li> <li>A Dockerfile is a set of instructions that describes how to build a Docker image</li> <li>An image is built first, then a Dockerfile is created from it</li> <li>Dockerfiles contain images that can be deployed as containers</li> </ol> Show Answer <p>The correct answer is B.</p> <p>A Dockerfile contains instructions (FROM, RUN, COPY, etc.) that Docker executes to build an image. The image is the result\u2014a snapshot of a filesystem with the application and dependencies. Option A is incorrect; they are distinct. Option C reverses the process. Option D has incorrect terminology.</p> <p>Concept: Dockerfile and Image Relationship</p>"},{"location":"chapters/02-git-docker/quiz/#3-what-is-the-primary-difference-between-git-merge-and-git-rebase","title":"3. What is the primary difference between <code>git merge</code> and <code>git rebase</code>?","text":"<ol> <li>Merge is faster while rebase is slower</li> <li>Merge creates a merge commit combining two branches; rebase replays commits on top of another branch</li> <li>Rebase is only used for remote repositories</li> <li>Merge creates a linear history while rebase creates branching</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Merge creates a new merge commit that ties two branches together (non-linear history). Rebase replays your commits on top of another branch, creating a linear history. Option A is incorrect; speed is comparable. Option C is false; rebase works locally and remotely. Option D is backwards; rebase creates linear history, not branching.</p> <p>Concept: Git Merge vs Rebase</p>"},{"location":"chapters/02-git-docker/quiz/#4-when-would-you-use-git-rebase-instead-of-git-merge-in-a-team-environment","title":"4. When would you use <code>git rebase</code> instead of <code>git merge</code> in a team environment?","text":"<ol> <li>When you want to keep a clean, linear history and haven't pushed to a shared remote</li> <li>When merging from main into your feature branch publicly</li> <li>Always, because it's faster than merge</li> <li>When collaborating with team members on the same branch</li> </ol> Show Answer <p>The correct answer is A.</p> <p>Rebase should be used locally before pushing to keep history clean. Once code is pushed, using rebase rewrites history, which can cause issues for others. Option B is wrong; merge is safer for shared branches. Option C is an overstatement. Option D is incorrect; rebase on shared branches causes problems.</p> <p>Concept: Git Rebase Best Practices</p>"},{"location":"chapters/02-git-docker/quiz/#5-what-is-the-purpose-of-docker-compose","title":"5. What is the purpose of Docker Compose?","text":"<ol> <li>To compile Docker containers into optimized binaries</li> <li>To define and run multi-container applications with a single YAML file</li> <li>To manage container memory and CPU allocation</li> <li>To create backups of Docker images</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Docker Compose lets you define multiple services (containers) with their configurations in a docker-compose.yml file and start them all together. This is ideal for development environments with multiple services. Option A is incorrect; Docker doesn't compile to binaries. Option C relates to resource limits, not the purpose of Compose. Option D is not its function.</p> <p>Concept: Docker Compose</p>"},{"location":"chapters/02-git-docker/quiz/#6-you-have-a-dockerfile-with-many-layers-that-rarely-change-how-would-you-optimize-it-to-improve-build-times","title":"6. You have a Dockerfile with many layers that rarely change. How would you optimize it to improve build times?","text":"<ol> <li>Combine all instructions into a single RUN statement</li> <li>Order instructions so dependencies and dependencies change least frequently appear first</li> <li>Remove all comments and whitespace</li> <li>Use a smaller base image regardless of requirements</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Docker caches layers. By ordering instructions so stable dependencies (base OS libraries) are added early and application code (which changes frequently) is added late, you maximize cache hits. Option A combines instructions but loses caching benefits. Option C has minimal impact. Option D risks missing required dependencies.</p> <p>Concept: Docker Layer Caching</p>"},{"location":"chapters/02-git-docker/quiz/#7-why-is-it-problematic-to-push-changes-directly-to-the-main-branch-without-going-through-a-pull-request","title":"7. Why is it problematic to push changes directly to the main branch without going through a pull request?","text":"<ol> <li>Direct pushes are slower than pull requests</li> <li>Pull requests enable code review, discussion, and automated testing before integration</li> <li>Main branch pushes always cause merge conflicts</li> <li>Direct pushes consume more storage on the remote repository</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Pull requests provide a mechanism for code review, testing, and discussion before merging, ensuring code quality and team awareness. Option A is incorrect; speed is comparable. Option C is not always true; conflicts depend on changes. Option D is false; storage is not affected.</p> <p>Concept: Pull Request Workflow</p>"},{"location":"chapters/02-git-docker/quiz/#8-given-the-following-scenario-youve-committed-sensitive-data-api-keys-to-a-repository-what-is-the-safest-approach","title":"8. Given the following scenario: You've committed sensitive data (API keys) to a repository. What is the safest approach?","text":"<ol> <li>Delete the file and commit again; the history will be automatically cleaned</li> <li>Use <code>git reset --hard</code> to undo all changes</li> <li>Rotate the credentials immediately and use tools like git-filter-branch to remove from history</li> <li>Ask team members not to look at the sensitive commit</li> </ol> Show Answer <p>The correct answer is C.</p> <p>You must immediately rotate credentials (they're compromised) and remove them from history using git-filter-branch or similar tools. Option A is false; deleted files remain in git history. Option B loses all work. Option D doesn't solve the security problem.</p> <p>Concept: Git Security Best Practices</p>"},{"location":"chapters/02-git-docker/quiz/#9-how-do-you-reduce-the-size-of-a-docker-image","title":"9. How do you reduce the size of a Docker image?","text":"<ol> <li>Use a larger base image to include more tools</li> <li>Use multi-stage builds, remove unnecessary files, and use minimal base images like Alpine</li> <li>Compress the image after building</li> <li>Remove the Dockerfile after building the image</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Multi-stage builds allow you to build in one stage and copy only necessary artifacts to the final image. Using minimal base images (Alpine Linux) and cleaning up package managers also reduces size. Option A increases size. Option C doesn't address the core issue. Option D loses the build specification.</p> <p>Concept: Docker Image Optimization</p>"},{"location":"chapters/02-git-docker/quiz/#10-compare-the-branching-strategies-git-flow-vs-trunk-based-development-which-is-better-for-continuous-deployment","title":"10. Compare the branching strategies: <code>git flow</code> vs <code>trunk-based development</code>. Which is better for continuous deployment?","text":"<ol> <li>Git flow is better because it uses more branches</li> <li>Trunk-based development is better because shorter-lived branches integrate faster with fewer conflicts</li> <li>Both are equally suitable for continuous deployment</li> <li>Trunk-based development is slower due to fewer branches</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Trunk-based development with short-lived feature branches enables continuous deployment by keeping changes integrated quickly and reducing merge conflicts. Git flow with long-lived release branches can delay integration. Option A is incorrect; more branches don't guarantee better CI/CD. Option C is false; they have different trade-offs. Option D reverses the reality.</p> <p>Concept: Branching Strategies</p>"},{"location":"chapters/02-git-docker/quiz/#question-distribution-summary","title":"Question Distribution Summary","text":"<p>Bloom's Taxonomy: - Remember (25%): Questions 1, 3 = 20% - Understand (30%): Questions 2, 5, 7 = 30% - Apply (30%): Questions 4, 6, 9 = 30% - Analyze (15%): Questions 8, 10 = 20%</p> <p>Answer Distribution: - A: 10% (1 question) - B: 60% (6 questions) - C: 20% (2 questions) - D: 10% (1 question)</p> <p>Note: Answer distribution will be balanced across all 4 quizzes (40 questions total) to achieve target percentages.</p>"},{"location":"chapters/03-database-modeling/","title":"Chapter 3: Database Design &amp; Modeling","text":""},{"location":"chapters/03-database-modeling/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this chapter, you will be able to: 1. Design normalized relational database schemas following Third Normal Form (3NF) principles to eliminate data anomalies 2. Evaluate the trade-offs between normalization and denormalization in different contexts 3. Implement appropriate indexes to optimize query performance for specific workload patterns 4. Analyze query execution plans to identify and resolve performance bottlenecks</p>"},{"location":"chapters/03-database-modeling/#introduction","title":"Introduction","text":"<p>Your e-commerce startup just hit product-market fit. Orders are flooding in. Your database, which worked fine for the first 1,000 customers, is now grinding to a halt. Queries that took 100ms now take 30 seconds. Worse, you're finding duplicate data everywhere\u2014the same customer email spelled three different ways, product prices that don't match between tables, and orders that reference products that don't exist.</p> <p>Last week, a developer tried to update a customer's shipping address and accidentally changed it for 500 orders instead of just future ones. Yesterday, the marketing team tried to analyze \"top products by category\" and the query ran for six hours before you killed it.</p> <p>These aren't random bugs. They're symptoms of poor database design. The difference between a well-designed schema and a poorly designed one is the difference between a database that scales gracefully and one that becomes a bottleneck the moment you get traction.</p> <p>This chapter is about making intentional design decisions. You'll learn when to normalize (split data across tables) and when to denormalize (combine data for performance). You'll understand why some queries are fast and others are slow, and how to fix the slow ones. And you'll see real examples of how design choices made in week one affect your ability to scale in year one.</p> <p>Let's start with the foundation: normalization. We'll begin with a messy real-world dataset and progressively clean it up.</p>"},{"location":"chapters/03-database-modeling/#section-1-the-problem-with-denormalized-data","title":"Section 1: The Problem with Denormalized Data","text":""},{"location":"chapters/03-database-modeling/#key-idea","title":"Key Idea","text":"<p>Denormalized data\u2014storing everything in one big table\u2014leads to redundancy, update anomalies, and data integrity issues. Normalization solves these problems by organizing data into related tables.</p>"},{"location":"chapters/03-database-modeling/#example-a-messy-e-commerce-spreadsheet","title":"Example: A Messy E-Commerce Spreadsheet","text":"<p>Your startup began with a spreadsheet to track orders. As you grew, you turned it into a database table. Here's what it looks like:</p> <p>orders table: <pre><code>order_id | customer_name | customer_email     | customer_phone  | product_name    | product_category | product_price | quantity | order_date\n---------|---------------|--------------------|-----------------|-----------------|-----------------|--------------|---------|-----------\n1        | Alice Smith   | alice@email.com    | 555-0101        | Python Book     | Books           | 29.99        | 2       | 2024-01-15\n2        | Bob Jones     | bob@email.com      | 555-0102        | SQL Course      | Courses         | 199.00       | 1       | 2024-01-16\n3        | Alice Smith   | alice@email.com    | 555-0101        | Docker Guide    | Books           | 39.99        | 1       | 2024-01-17\n4        | Alice Smith   | asmith@email.com   | 555-0101        | Python Book     | Books           | 29.99        | 1       | 2024-01-18\n5        | Bob Jones     | bob@email.com      | 555-0102        | Python Book     | Books           | 34.99        | 3       | 2024-01-19\n</code></pre></p> <p>Looks reasonable at first glance. But notice the problems:</p> <p>Problem 1: Data Redundancy - Alice Smith's information is repeated in rows 1, 3, and 4 - Python Book's details appear in rows 1, 4, and 5 - Every order for the same customer duplicates their contact info</p> <p>Problem 2: Update Anomalies <pre><code>-- Alice changes her email. Do we update all three rows?\nUPDATE orders SET customer_email = 'alice.smith@newdomain.com'\nWHERE customer_name = 'Alice Smith';\n\n-- But wait, row 4 has 'asmith@email.com' - is that the same Alice?\n-- Now Alice has two different emails in the database!\n</code></pre></p> <p>Problem 3: Inconsistent Data - Row 1: Python Book costs $29.99 - Row 5: Python Book costs $34.99 - Which is correct? Did the price change? Is it an error?</p> <p>Problem 4: Insertion Anomaly <pre><code>-- You want to add a new product to the catalog\n-- But you can't insert a product without an order!\nINSERT INTO orders (product_name, product_category, product_price)\nVALUES ('New Book', 'Books', 49.99);\n-- Error: order_id, customer_name, etc. are required\n</code></pre></p> <p>Problem 5: Deletion Anomaly <pre><code>-- Bob cancels order #2 (SQL Course)\nDELETE FROM orders WHERE order_id = 2;\n-- Now you've lost the only record that SQL Course exists!\n</code></pre></p>"},{"location":"chapters/03-database-modeling/#why-this-matters","title":"Why This Matters","text":"<p>In production systems: - Redundancy wastes storage and makes data inconsistent - Update anomalies cause bugs that affect real customers - Insertion/deletion anomalies limit what your application can do - Inconsistent data breaks analytics and reporting</p> <p>These problems are solvable. The solution is normalization: systematically organizing data to eliminate redundancy and anomalies.</p>"},{"location":"chapters/03-database-modeling/#try-it","title":"Try It","text":"<p>Look at this denormalized table. How many anomalies can you spot?</p> <pre><code>employee_id | employee_name | department_name | department_location | manager_name  | project_name  | project_budget\n------------|---------------|-----------------|---------------------|---------------|---------------|---------------\n1           | Alice         | Engineering     | Building A          | Bob           | Data Pipeline | 100000\n1           | Alice         | Engineering     | Building A          | Bob           | ML Platform   | 150000\n2           | Carol         | Marketing       | Building B          | Dave          | Campaign 2024 | 50000\n3           | Eve           | Engineering     | Building C          | Bob           | Data Pipeline | 100000\n</code></pre> <p>Problems to consider: - What happens if Engineering moves to Building C? - What if Bob is replaced as manager? - Can you add a new department without adding an employee?</p>"},{"location":"chapters/03-database-modeling/#section-2-normalization-to-third-normal-form-3nf","title":"Section 2: Normalization to Third Normal Form (3NF)","text":""},{"location":"chapters/03-database-modeling/#key-idea_1","title":"Key Idea","text":"<p>Normalization is a step-by-step process of decomposing tables to eliminate redundancy. Third Normal Form (3NF) eliminates most practical issues while remaining intuitive and efficient.</p>"},{"location":"chapters/03-database-modeling/#the-normalization-process","title":"The Normalization Process","text":"<p>Let's fix the messy orders table step by step.</p> <p>First Normal Form (1NF): Atomic Values</p> <p>Rule: Each column must contain atomic (indivisible) values. No arrays, lists, or multi-valued attributes.</p> <p>Our orders table already satisfies 1NF (each cell contains a single value). But imagine if we had:</p> <pre><code>order_id | customer_name | products\n---------|---------------|---------------------------\n1        | Alice Smith   | Python Book, Docker Guide\n</code></pre> <p>This violates 1NF because <code>products</code> contains multiple values. Fix:</p> <pre><code>order_id | customer_name | product\n---------|---------------|-------------\n1        | Alice Smith   | Python Book\n1        | Alice Smith   | Docker Guide\n</code></pre> <p>Second Normal Form (2NF): No Partial Dependencies</p> <p>Rule: All non-key columns must depend on the entire primary key, not just part of it.</p> <p>If our primary key were <code>(order_id, product_name)</code>, then <code>customer_name</code> depends only on <code>order_id</code>, not the full key. This is a partial dependency.</p> <p>Fix: Split into separate tables:</p> <pre><code>-- orders table (one row per order)\nCREATE TABLE orders (\n    order_id INT PRIMARY KEY,\n    customer_name VARCHAR(100),\n    customer_email VARCHAR(100),\n    customer_phone VARCHAR(20),\n    order_date DATE\n);\n\n-- order_items table (one row per product in an order)\nCREATE TABLE order_items (\n    order_id INT,\n    product_name VARCHAR(100),\n    quantity INT,\n    PRIMARY KEY (order_id, product_name),\n    FOREIGN KEY (order_id) REFERENCES orders(order_id)\n);\n</code></pre> <p>Third Normal Form (3NF): No Transitive Dependencies</p> <p>Rule: Non-key columns must not depend on other non-key columns.</p> <p>In our <code>orders</code> table, <code>customer_name</code>, <code>customer_email</code>, and <code>customer_phone</code> all describe the customer, not the order. They're transitively dependent on <code>order_id</code> through <code>customer_id</code>.</p> <p>In <code>order_items</code>, <code>product_category</code> and <code>product_price</code> depend on <code>product_name</code>, not the order.</p> <p>Final Normalized Schema (3NF):</p> <pre><code>-- Customers table\nCREATE TABLE customers (\n    customer_id SERIAL PRIMARY KEY,\n    customer_name VARCHAR(100) NOT NULL,\n    customer_email VARCHAR(100) UNIQUE NOT NULL,\n    customer_phone VARCHAR(20),\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Products table\nCREATE TABLE products (\n    product_id SERIAL PRIMARY KEY,\n    product_name VARCHAR(100) NOT NULL,\n    product_category VARCHAR(50) NOT NULL,\n    product_price DECIMAL(10, 2) NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Orders table\nCREATE TABLE orders (\n    order_id SERIAL PRIMARY KEY,\n    customer_id INT NOT NULL,\n    order_date DATE NOT NULL,\n    order_total DECIMAL(10, 2),\n    FOREIGN KEY (customer_id) REFERENCES customers(customer_id)\n);\n\n-- Order items table\nCREATE TABLE order_items (\n    order_item_id SERIAL PRIMARY KEY,\n    order_id INT NOT NULL,\n    product_id INT NOT NULL,\n    quantity INT NOT NULL CHECK (quantity &gt; 0),\n    price_at_purchase DECIMAL(10, 2) NOT NULL,\n    FOREIGN KEY (order_id) REFERENCES orders(order_id),\n    FOREIGN KEY (product_id) REFERENCES products(product_id)\n);\n</code></pre>"},{"location":"chapters/03-database-modeling/#before-and-after-problem-resolution","title":"Before and After: Problem Resolution","text":"<p>Original Issue: Update Alice's email</p> <p>Before (denormalized): <pre><code>-- Have to update multiple rows, might miss some\nUPDATE orders SET customer_email = 'alice.new@email.com'\nWHERE customer_name = 'Alice Smith';  -- What if name is misspelled?\n</code></pre></p> <p>After (normalized): <pre><code>-- Update once, affects all orders automatically\nUPDATE customers SET customer_email = 'alice.new@email.com'\nWHERE customer_id = 123;\n</code></pre></p> <p>Original Issue: Change product price</p> <p>Before (denormalized): <pre><code>-- Do we update all past orders? Just future ones?\nUPDATE orders SET product_price = 34.99\nWHERE product_name = 'Python Book';  -- This changes historical data!\n</code></pre></p> <p>After (normalized): <pre><code>-- Update product catalog\nUPDATE products SET product_price = 34.99\nWHERE product_id = 5;\n\n-- Past orders are unaffected (they store price_at_purchase)\n-- Future orders get the new price automatically\n</code></pre></p> <p>Original Issue: Add product without an order</p> <p>Before (denormalized): <pre><code>-- Impossible!\n</code></pre></p> <p>After (normalized): <pre><code>-- Easy\nINSERT INTO products (product_name, product_category, product_price)\nVALUES ('New Book', 'Books', 49.99);\n</code></pre></p>"},{"location":"chapters/03-database-modeling/#understanding-foreign-keys","title":"Understanding Foreign Keys","text":"<p>Foreign keys enforce referential integrity:</p> <pre><code>-- Try to create an order for a non-existent customer\nINSERT INTO orders (customer_id, order_date)\nVALUES (999999, CURRENT_DATE);\n-- Error: foreign key constraint violated\n\n-- Try to delete a customer who has orders\nDELETE FROM customers WHERE customer_id = 123;\n-- Error: violates foreign key constraint\n\n-- Cascade option: delete orders when customer is deleted\nALTER TABLE orders\nADD CONSTRAINT fk_customer\nFOREIGN KEY (customer_id) REFERENCES customers(customer_id)\nON DELETE CASCADE;  -- Be careful with this!\n</code></pre>"},{"location":"chapters/03-database-modeling/#why-this-matters_1","title":"Why This Matters","text":"<p>Normalized schemas: - Prevent data anomalies: One place to update each fact - Enforce data integrity: Foreign keys prevent orphaned records - Reduce storage: No redundant data - Enable flexible queries: Join tables as needed for different analyses</p> <p>In data engineering: - ETL pipelines benefit: Extract from normalized sources without deduplication logic - Data quality improves: Constraints catch errors at insert time - Migrations are safer: Changing one table doesn't require updating redundant data</p>"},{"location":"chapters/03-database-modeling/#try-it_1","title":"Try It","text":"<p>Normalize this denormalized table to 3NF:</p> <pre><code>invoice_id | customer_name | customer_city | product_name | product_vendor | vendor_country | quantity\n-----------|---------------|---------------|--------------|----------------|----------------|----------\n1          | Alice         | New York      | Widget A     | Acme Corp      | USA            | 10\n2          | Bob           | London        | Widget A     | Acme Corp      | USA            | 5\n3          | Alice         | New York      | Widget B     | Beta Inc       | Canada         | 3\n</code></pre> Solution <pre><code>CREATE TABLE customers (\n    customer_id SERIAL PRIMARY KEY,\n    customer_name VARCHAR(100),\n    customer_city VARCHAR(100)\n);\n\nCREATE TABLE vendors (\n    vendor_id SERIAL PRIMARY KEY,\n    vendor_name VARCHAR(100),\n    vendor_country VARCHAR(100)\n);\n\nCREATE TABLE products (\n    product_id SERIAL PRIMARY KEY,\n    product_name VARCHAR(100),\n    vendor_id INT,\n    FOREIGN KEY (vendor_id) REFERENCES vendors(vendor_id)\n);\n\nCREATE TABLE invoices (\n    invoice_id SERIAL PRIMARY KEY,\n    customer_id INT,\n    FOREIGN KEY (customer_id) REFERENCES customers(customer_id)\n);\n\nCREATE TABLE invoice_items (\n    invoice_item_id SERIAL PRIMARY KEY,\n    invoice_id INT,\n    product_id INT,\n    quantity INT,\n    FOREIGN KEY (invoice_id) REFERENCES invoices(invoice_id),\n    FOREIGN KEY (product_id) REFERENCES products(product_id)\n);\n</code></pre>"},{"location":"chapters/03-database-modeling/#section-3-strategic-denormalization-for-performance","title":"Section 3: Strategic Denormalization for Performance","text":""},{"location":"chapters/03-database-modeling/#key-idea_2","title":"Key Idea","text":"<p>While normalization prevents anomalies, it requires joins for queries. In some cases, intentional denormalization improves read performance at the cost of write complexity and storage.</p>"},{"location":"chapters/03-database-modeling/#example-the-performance-problem-with-joins","title":"Example: The Performance Problem with Joins","text":"<p>Your normalized e-commerce schema works great. But your most common query\u2014showing orders with customer and product details\u2014is getting slow:</p> <pre><code>-- This query runs on every page load\nSELECT\n    o.order_id,\n    o.order_date,\n    c.customer_name,\n    c.customer_email,\n    p.product_name,\n    p.product_category,\n    oi.quantity,\n    oi.price_at_purchase,\n    oi.quantity * oi.price_at_purchase AS line_total\nFROM orders o\nJOIN customers c ON o.customer_id = c.customer_id\nJOIN order_items oi ON o.order_id = oi.order_id\nJOIN products p ON oi.product_id = p.product_id\nWHERE o.order_date &gt;= CURRENT_DATE - INTERVAL '7 days'\nORDER BY o.order_date DESC;\n</code></pre> <p>At 1,000 orders/day, this query takes 50ms. At 100,000 orders/day, it takes 5 seconds. The problem? Four joins across large tables.</p>"},{"location":"chapters/03-database-modeling/#when-to-denormalize","title":"When to Denormalize","text":"<p>Denormalize when: 1. Read-heavy workload: 99% reads, 1% writes 2. Predictable access patterns: Same queries run repeatedly 3. Performance is critical: Page load times matter 4. Storage is cheap, compute is expensive: Cloud costs favor less computation</p> <p>Don't denormalize when: 1. Write-heavy workload: Frequent updates 2. Data changes frequently: Keeping denormalized data in sync is hard 3. Data integrity is paramount: Normalization prevents errors 4. Storage is expensive: Redundant data costs real money</p>"},{"location":"chapters/03-database-modeling/#denormalization-strategy-1-add-computed-columns","title":"Denormalization Strategy 1: Add Computed Columns","text":"<p>Instead of joining to calculate totals:</p> <pre><code>-- Add denormalized column\nALTER TABLE orders ADD COLUMN order_total DECIMAL(10, 2);\n\n-- Update with trigger or application code\nCREATE OR REPLACE FUNCTION update_order_total()\nRETURNS TRIGGER AS $$\nBEGIN\n    UPDATE orders\n    SET order_total = (\n        SELECT SUM(quantity * price_at_purchase)\n        FROM order_items\n        WHERE order_id = NEW.order_id\n    )\n    WHERE order_id = NEW.order_id;\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER order_item_changes\nAFTER INSERT OR UPDATE OR DELETE ON order_items\nFOR EACH ROW EXECUTE FUNCTION update_order_total();\n</code></pre> <p>Trade-off: - Gain: Fast reads (no aggregation needed) - Cost: Slower writes (trigger overhead), more storage</p>"},{"location":"chapters/03-database-modeling/#denormalization-strategy-2-materialized-views","title":"Denormalization Strategy 2: Materialized Views","text":"<p>A materialized view is a pre-computed query result stored as a table:</p> <pre><code>-- Create materialized view with pre-joined data\nCREATE MATERIALIZED VIEW order_details_mv AS\nSELECT\n    o.order_id,\n    o.order_date,\n    c.customer_name,\n    c.customer_email,\n    p.product_name,\n    p.product_category,\n    oi.quantity,\n    oi.price_at_purchase,\n    oi.quantity * oi.price_at_purchase AS line_total\nFROM orders o\nJOIN customers c ON o.customer_id = c.customer_id\nJOIN order_items oi ON o.order_id = oi.order_id\nJOIN products p ON oi.product_id = p.product_id;\n\n-- Create index for fast queries\nCREATE INDEX idx_order_details_date ON order_details_mv(order_date);\n\n-- Query the materialized view (fast!)\nSELECT * FROM order_details_mv\nWHERE order_date &gt;= CURRENT_DATE - INTERVAL '7 days'\nORDER BY order_date DESC;\n</code></pre> <p>Refresh strategy:</p> <pre><code>-- Refresh manually when data changes\nREFRESH MATERIALIZED VIEW order_details_mv;\n\n-- Or schedule automatic refresh\n-- (via cron, Airflow, or database scheduler)\n</code></pre> <p>Trade-off: - Gain: Extremely fast reads (data is pre-joined) - Cost: Stale data between refreshes, storage for redundant data</p>"},{"location":"chapters/03-database-modeling/#denormalization-strategy-3-wide-tables-for-analytics","title":"Denormalization Strategy 3: Wide Tables for Analytics","text":"<p>For analytics workloads (data warehouses), fully denormalized \"wide tables\" are common:</p> <pre><code>-- Denormalized analytics table\nCREATE TABLE orders_analytics (\n    order_id INT,\n    order_date DATE,\n    customer_id INT,\n    customer_name VARCHAR(100),\n    customer_email VARCHAR(100),\n    customer_signup_date DATE,\n    customer_segment VARCHAR(50),\n    product_id INT,\n    product_name VARCHAR(100),\n    product_category VARCHAR(50),\n    product_price DECIMAL(10, 2),\n    quantity INT,\n    price_at_purchase DECIMAL(10, 2),\n    line_total DECIMAL(10, 2),\n    order_total DECIMAL(10, 2)\n);\n</code></pre> <p>This table duplicates everything but enables lightning-fast analytics:</p> <pre><code>-- No joins needed!\nSELECT\n    product_category,\n    SUM(line_total) AS revenue,\n    COUNT(DISTINCT customer_id) AS unique_customers\nFROM orders_analytics\nWHERE order_date &gt;= CURRENT_DATE - INTERVAL '30 days'\nGROUP BY product_category\nORDER BY revenue DESC;\n</code></pre> <p>Trade-off: - Gain: Fast analytics, simple queries - Cost: Huge storage, complex ETL to keep in sync</p>"},{"location":"chapters/03-database-modeling/#real-world-example-netflix","title":"Real-World Example: Netflix","text":"<p>Netflix uses both normalized and denormalized data:</p> <p>Normalized (transactional system): - User accounts - Subscription plans - Billing records - Content metadata</p> <p>Denormalized (analytics/recommendations): - Wide tables with viewing history + user demographics + content attributes - Enables fast ML model training and recommendation queries - Updated periodically via ETL pipelines</p>"},{"location":"chapters/03-database-modeling/#why-this-matters_2","title":"Why This Matters","text":"<p>In data engineering: - OLTP systems (transactional) use normalized schemas - OLAP systems (analytics) use denormalized schemas - Your job is often moving data from normalized to denormalized (ETL)</p> <p>Understanding when to denormalize: - Saves infrastructure costs (less compute for queries) - Improves user experience (faster dashboards) - Enables real-time analytics (pre-aggregated data)</p> <p>But over-denormalization causes: - Data staleness issues - Complex update logic - Storage bloat</p>"},{"location":"chapters/03-database-modeling/#try-it_2","title":"Try It","text":"<p>You have a normalized schema for a blog:</p> <pre><code>CREATE TABLE users (user_id, username, email);\nCREATE TABLE posts (post_id, user_id, title, content, created_at);\nCREATE TABLE comments (comment_id, post_id, user_id, text, created_at);\n</code></pre> <p>Your most common query: <pre><code>SELECT p.title, u.username, COUNT(c.comment_id) AS comment_count\nFROM posts p\nJOIN users u ON p.user_id = u.user_id\nLEFT JOIN comments c ON p.post_id = c.post_id\nGROUP BY p.post_id, p.title, u.username;\n</code></pre></p> <p>Design a denormalized solution. What column(s) would you add? What trigger(s) would keep it updated?</p> Solution <pre><code>-- Add denormalized columns\nALTER TABLE posts\nADD COLUMN author_username VARCHAR(100),\nADD COLUMN comment_count INT DEFAULT 0;\n\n-- Trigger to update comment count\nCREATE OR REPLACE FUNCTION update_post_comment_count()\nRETURNS TRIGGER AS $$\nBEGIN\n    IF TG_OP = 'INSERT' THEN\n        UPDATE posts SET comment_count = comment_count + 1 WHERE post_id = NEW.post_id;\n    ELSIF TG_OP = 'DELETE' THEN\n        UPDATE posts SET comment_count = comment_count - 1 WHERE post_id = OLD.post_id;\n    END IF;\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER comment_count_trigger\nAFTER INSERT OR DELETE ON comments\nFOR EACH ROW EXECUTE FUNCTION update_post_comment_count();\n\n-- Now the query is simple\nSELECT title, author_username, comment_count FROM posts;\n</code></pre>  Trade-offs: - Faster reads (no joins, no COUNT aggregation) - Slower writes (trigger overhead) - Risk: Username change requires updating all posts"},{"location":"chapters/03-database-modeling/#section-4-indexes-the-secret-to-fast-queries","title":"Section 4: Indexes - The Secret to Fast Queries","text":""},{"location":"chapters/03-database-modeling/#key-idea_3","title":"Key Idea","text":"<p>Indexes are data structures that make lookups fast by avoiding full table scans. Choosing the right indexes is crucial for performance, but too many indexes slow down writes.</p>"},{"location":"chapters/03-database-modeling/#example-the-slow-query-problem","title":"Example: The Slow Query Problem","text":"<p>You have a table with 10 million orders:</p> <pre><code>CREATE TABLE orders (\n    order_id SERIAL PRIMARY KEY,\n    customer_id INT,\n    order_date DATE,\n    status VARCHAR(20),\n    order_total DECIMAL(10, 2)\n);\n</code></pre> <p>This query is painfully slow:</p> <pre><code>EXPLAIN ANALYZE\nSELECT * FROM orders\nWHERE customer_id = 12345\nAND order_date &gt;= '2024-01-01';\n</code></pre> <p>Output: <pre><code>Seq Scan on orders (cost=0.00..200000.00 rows=150 width=50) (actual time=1850.234..1850.456 rows=150 loops=1)\n  Filter: (customer_id = 12345 AND order_date &gt;= '2024-01-01')\n  Rows Removed by Filter: 9999850\nPlanning Time: 0.123 ms\nExecution Time: 1850.567 ms\n</code></pre></p> <p>Seq Scan means the database scanned all 10 million rows. For 150 matching rows, it checked 9,999,850 unnecessary rows. This doesn't scale.</p>"},{"location":"chapters/03-database-modeling/#how-indexes-work","title":"How Indexes Work","text":"<p>An index is like a book's index: instead of reading every page to find a topic, you look it up in the index and jump directly to the right page.</p> <p>B-tree index (default in PostgreSQL):</p> <pre><code>-- Create index on customer_id\nCREATE INDEX idx_orders_customer ON orders(customer_id);\n</code></pre> <p>Now the query uses the index:</p> <pre><code>EXPLAIN ANALYZE\nSELECT * FROM orders\nWHERE customer_id = 12345;\n</code></pre> <p>Output: <pre><code>Index Scan using idx_orders_customer on orders (cost=0.43..850.21 rows=500 width=50) (actual time=0.045..1.234 rows=500 loops=1)\n  Index Cond: (customer_id = 12345)\nPlanning Time: 0.145 ms\nExecution Time: 1.456 ms\n</code></pre></p> <p>Result: 1850ms \u2192 1.5ms (1,233x faster!)</p>"},{"location":"chapters/03-database-modeling/#composite-indexes","title":"Composite Indexes","text":"<p>What if you frequently query by both <code>customer_id</code> and <code>order_date</code>?</p> <pre><code>-- Composite index on multiple columns\nCREATE INDEX idx_orders_customer_date ON orders(customer_id, order_date);\n</code></pre> <p>Now this query is fast:</p> <pre><code>SELECT * FROM orders\nWHERE customer_id = 12345\nAND order_date &gt;= '2024-01-01';\n</code></pre> <p>Column order matters!</p> <pre><code>-- Works: Query filters by customer_id (first column in index)\nWHERE customer_id = 12345\n\n-- Works: Query filters by customer_id and order_date\nWHERE customer_id = 12345 AND order_date &gt;= '2024-01-01'\n\n-- DOESN'T use index: Query only filters by order_date (second column)\nWHERE order_date &gt;= '2024-01-01'\n</code></pre> <p>Rule: Composite index <code>(A, B, C)</code> can be used for queries filtering on: - <code>A</code> - <code>A, B</code> - <code>A, B, C</code></p> <p>But NOT for: - <code>B</code> - <code>C</code> - <code>B, C</code></p>"},{"location":"chapters/03-database-modeling/#covering-indexes","title":"Covering Indexes","text":"<p>A covering index includes all columns needed by a query, so the database doesn't need to look at the table:</p> <pre><code>-- Query that only needs customer_id, order_date, order_total\nSELECT customer_id, order_date, order_total\nFROM orders\nWHERE customer_id = 12345;\n\n-- Create covering index (includes extra column)\nCREATE INDEX idx_orders_customer_covering\nON orders(customer_id) INCLUDE (order_date, order_total);\n</code></pre> <p>The database can satisfy the entire query from the index without touching the table (index-only scan).</p>"},{"location":"chapters/03-database-modeling/#partial-indexes","title":"Partial Indexes","text":"<p>Index only the rows you care about:</p> <pre><code>-- 95% of orders are 'completed', but you only query 'pending' orders\nCREATE INDEX idx_orders_pending\nON orders(customer_id, order_date)\nWHERE status = 'pending';\n</code></pre> <p>Benefits: - Smaller index (faster to scan, less storage) - Faster for queries that match the WHERE condition</p> <pre><code>-- Uses partial index\nSELECT * FROM orders\nWHERE customer_id = 12345 AND status = 'pending';\n</code></pre>"},{"location":"chapters/03-database-modeling/#expression-indexes","title":"Expression Indexes","text":"<p>Index a computed value:</p> <pre><code>-- Query on lowercase email\nSELECT * FROM customers WHERE LOWER(email) = 'alice@example.com';\n\n-- Create index on expression\nCREATE INDEX idx_customers_email_lower ON customers(LOWER(email));\n</code></pre>"},{"location":"chapters/03-database-modeling/#index-trade-offs","title":"Index Trade-offs","text":"<p>Benefits: - Dramatically faster reads (1000x+ speedups possible) - Enable queries that would otherwise timeout</p> <p>Costs: - Slower writes (every INSERT/UPDATE/DELETE must update indexes) - Storage overhead (indexes can be as large as the table) - Maintenance overhead (VACUUM, REINDEX)</p> <p>Rule of thumb: - Small tables (&lt;10,000 rows): Indexes rarely help - Medium tables (10,000\u20131,000,000 rows): Index wisely - Large tables (&gt;1,000,000 rows): Indexes are essential</p>"},{"location":"chapters/03-database-modeling/#index-selection-strategy","title":"Index Selection Strategy","text":"<ol> <li>Identify slow queries (use query logs, monitoring tools)</li> <li>Analyze execution plans (<code>EXPLAIN ANALYZE</code>)</li> <li>Look for Seq Scans on large tables</li> <li>Create indexes on filter columns (WHERE, JOIN, ORDER BY)</li> <li>Test query performance before and after</li> <li>Monitor write performance (indexes slow down INSERTs)</li> </ol>"},{"location":"chapters/03-database-modeling/#real-world-example","title":"Real-World Example","text":"<p>Startup with 100M order records:</p> <p>Before indexes: - Dashboard load: 30 seconds - API endpoint timeout: 50% of requests</p> <p>After strategic indexing: <pre><code>CREATE INDEX idx_orders_customer ON orders(customer_id);\nCREATE INDEX idx_orders_date ON orders(order_date);\nCREATE INDEX idx_orders_status_date ON orders(status, order_date);\n</code></pre></p> <p>Results: - Dashboard load: 500ms - API timeout rate: 0% - Write throughput: 95% of before (acceptable trade-off)</p>"},{"location":"chapters/03-database-modeling/#why-this-matters_3","title":"Why This Matters","text":"<p>In data engineering: - ETL queries need indexes: Extracting data from source systems - Incremental loads depend on indexes: \"Get records since last run\" - Join performance matters: Multi-table analytics queries - Data warehouse queries: Columnar indexes, partitioning strategies</p> <p>Without proper indexes: - Queries timeout, pipelines fail - API endpoints slow down, users complain - Cloud costs spike (more CPU time for queries)</p>"},{"location":"chapters/03-database-modeling/#try-it_3","title":"Try It","text":"<p>Given this schema:</p> <pre><code>CREATE TABLE events (\n    event_id BIGSERIAL PRIMARY KEY,\n    user_id INT,\n    event_type VARCHAR(50),\n    event_timestamp TIMESTAMP,\n    event_data JSONB\n);\n</code></pre> <p>You frequently run: <pre><code>-- Query 1: User activity\nSELECT * FROM events\nWHERE user_id = 123\nAND event_timestamp &gt;= CURRENT_DATE - INTERVAL '7 days';\n\n-- Query 2: Event type analysis\nSELECT event_type, COUNT(*)\nFROM events\nWHERE event_timestamp &gt;= CURRENT_DATE - INTERVAL '30 days'\nGROUP BY event_type;\n\n-- Query 3: Recent logins\nSELECT * FROM events\nWHERE event_type = 'login'\nAND event_timestamp &gt;= CURRENT_DATE - INTERVAL '1 hour';\n</code></pre></p> <p>What indexes would you create?</p> Solution <pre><code>-- For Query 1: Composite index on user_id and timestamp\nCREATE INDEX idx_events_user_timestamp ON events(user_id, event_timestamp);\n\n-- For Query 2: Index on timestamp (covers date filtering)\nCREATE INDEX idx_events_timestamp ON events(event_timestamp);\n\n-- For Query 3: Partial index on recent logins\nCREATE INDEX idx_events_recent_logins\nON events(event_timestamp)\nWHERE event_type = 'login';\n\n-- Alternative for Query 3: Composite if you filter by type frequently\nCREATE INDEX idx_events_type_timestamp ON events(event_type, event_timestamp);\n</code></pre>  Trade-offs: - `idx_events_user_timestamp` helps Query 1 but not Query 2 or 3 - `idx_events_timestamp` helps all queries but less optimized than specific indexes - Partial index for logins is smaller and faster for Query 3 - Creating all indexes slows down INSERTs\u2014monitor write performance"},{"location":"chapters/03-database-modeling/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"chapters/03-database-modeling/#1-over-normalizing-olap-systems","title":"1. Over-Normalizing OLAP Systems","text":"<p>Problem: <pre><code>-- Normalized schema for analytics\n-- Every query needs 5+ joins\nSELECT ...\nFROM fact_sales\nJOIN dim_customer ON ...\nJOIN dim_product ON ...\nJOIN dim_date ON ...\nJOIN dim_store ON ...\nJOIN dim_promotion ON ...;\n</code></pre></p> <p>Fix: For analytics (OLAP), denormalized schemas are expected. Use star schemas, not 3NF.</p>"},{"location":"chapters/03-database-modeling/#2-creating-too-many-indexes","title":"2. Creating Too Many Indexes","text":"<p>Problem: <pre><code>-- 20 indexes on a table with frequent writes\n-- Every INSERT updates 20 indexes!\n</code></pre></p> <p>Fix: Only index columns you actually query. Use <code>pg_stat_user_indexes</code> to find unused indexes.</p> <pre><code>-- Find unused indexes\nSELECT schemaname, tablename, indexname, idx_scan\nFROM pg_stat_user_indexes\nWHERE idx_scan = 0\nAND indexname NOT LIKE '%_pkey';\n</code></pre>"},{"location":"chapters/03-database-modeling/#3-wrong-column-order-in-composite-indexes","title":"3. Wrong Column Order in Composite Indexes","text":"<p>Problem: <pre><code>CREATE INDEX idx_wrong ON orders(order_date, customer_id);\n\n-- This query won't use the index efficiently\nSELECT * FROM orders WHERE customer_id = 123;\n</code></pre></p> <p>Fix: Put the most selective column first (the one that filters out the most rows).</p>"},{"location":"chapters/03-database-modeling/#4-denormalizing-without-a-sync-strategy","title":"4. Denormalizing Without a Sync Strategy","text":"<p>Problem: <pre><code>-- Added denormalized column but no trigger/process to keep it updated\nALTER TABLE orders ADD COLUMN customer_name VARCHAR(100);\n-- Now customer_name is often NULL or outdated\n</code></pre></p> <p>Fix: Always implement a sync mechanism (triggers, application code, ETL job) when denormalizing.</p>"},{"location":"chapters/03-database-modeling/#reflection-questions","title":"Reflection Questions","text":"<ol> <li>How would you decide if denormalization is worth it?</li> </ol> <p>Consider:    - What's the read/write ratio? (99:1 vs. 50:50)    - How often does the denormalized data change?    - What's the cost of staleness? (e-commerce prices vs. historical analytics)    - Can you tolerate eventual consistency?</p> <ol> <li>You have a slow query. Your colleague suggests adding an index. How do you verify this will help?</li> </ol> <p>Steps:    - Run <code>EXPLAIN ANALYZE</code> to see current execution plan    - Check if it's doing a Seq Scan on a large table    - Create index in a test environment    - Run <code>EXPLAIN ANALYZE</code> again to confirm it uses the index    - Measure actual query time improvement    - Check impact on write performance</p> <ol> <li>Your database has 50 tables. How do you decide which ones to normalize vs. denormalize?</li> </ol> <p>Framework:    - Transactional tables (orders, payments): Normalize    - Reporting tables (dashboards, BI): Denormalize    - Frequently joined tables: Consider materialized views    - Rarely queried tables: Don't over-optimize</p> <ol> <li>When would you choose a materialized view over a denormalized table?</li> </ol> <p>Materialized view advantages:    - Automatically refreshable    - Still maintains normalized sources    - Can be recreated easily if corrupted</p> <p>Denormalized table advantages:    - More control over sync logic    - Can have different schema than source    - Can add additional indexes</p>"},{"location":"chapters/03-database-modeling/#summary","title":"Summary","text":"<ul> <li> <p>Normalization to 3NF eliminates data anomalies by organizing data into related tables with foreign key relationships, preventing redundancy and ensuring data integrity.</p> </li> <li> <p>Denormalization is a strategic choice for read-heavy workloads where performance is critical. Use computed columns, materialized views, or wide tables depending on staleness tolerance and query patterns.</p> </li> <li> <p>Indexes are essential for query performance on large tables. Create indexes on columns used in WHERE, JOIN, and ORDER BY clauses, but balance read performance against write overhead.</p> </li> <li> <p>OLTP systems use normalized schemas for transactional consistency, while OLAP systems use denormalized schemas for analytical performance. Data engineers bridge these worlds with ETL.</p> </li> <li> <p>Query execution plans (EXPLAIN ANALYZE) reveal whether queries use indexes effectively. Monitor for Seq Scans on large tables as a sign of missing indexes.</p> </li> </ul>"},{"location":"chapters/03-database-modeling/#next-steps","title":"Next Steps","text":"<p>You now understand how to design efficient database schemas for transactional systems. In Chapter 4, we'll shift to analytical systems\u2014data warehouses. You'll learn:</p> <ul> <li>The difference between OLTP and OLAP architectures</li> <li>Designing star and snowflake schemas for analytics</li> <li>BigQuery-specific optimizations (partitioning, clustering)</li> <li>Slowly Changing Dimensions (SCD) for tracking historical data</li> </ul> <p>The normalization principles you learned here apply to source systems (OLTP). In the next chapter, you'll intentionally denormalize data for analytics (OLAP). Understanding both paradigms is what makes you an effective data engineer\u2014knowing when to split tables apart and when to join them together.</p>"},{"location":"chapters/03-database-modeling/quiz/","title":"Chapter 3 Quiz: Database Design &amp; Modeling","text":""},{"location":"chapters/03-database-modeling/quiz/#1-what-is-the-primary-goal-of-database-normalization","title":"1. What is the primary goal of database normalization?","text":"<ol> <li>To improve query performance by combining related tables</li> <li>To eliminate redundancy and dependency anomalies in data</li> <li>To reduce the total number of tables in a database</li> <li>To make the database easier to understand visually</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Normalization (1NF, 2NF, 3NF, etc.) eliminates data redundancy, reduces update anomalies, and ensures data integrity. Option A describes denormalization. Option C is not the primary goal. Option D is a side benefit but not the main purpose.</p> <p>Concept: Database Normalization</p>"},{"location":"chapters/03-database-modeling/quiz/#2-explain-why-a-database-table-violates-second-normal-form-2nf","title":"2. Explain why a database table violates Second Normal Form (2NF).","text":"<ol> <li>It contains attributes that don't depend on the primary key</li> <li>It has multiple rows with identical data</li> <li>It uses NULL values in critical columns</li> <li>It has more than three foreign keys</li> </ol> Show Answer <p>The correct answer is A.</p> <p>2NF requires that all non-key attributes depend on the entire primary key, not just part of it (partial dependency). If a non-key attribute depends only on part of a composite key, it violates 2NF. Option B describes duplication, not 2NF violation. Option C is a data quality issue. Option D is not a 2NF criterion.</p> <p>Concept: Second Normal Form</p>"},{"location":"chapters/03-database-modeling/quiz/#3-what-is-a-database-index-and-how-does-it-improve-query-performance","title":"3. What is a database index and how does it improve query performance?","text":"<ol> <li>It duplicates data to allow faster lookups without reading the entire table</li> <li>It creates a sorted data structure that allows the database to find rows without a full table scan</li> <li>It compresses table data to reduce storage and retrieval time</li> <li>It automatically partitions data across multiple servers</li> </ol> Show Answer <p>The correct answer is B.</p> <p>An index is a sorted data structure (typically B-tree) that allows the database engine to locate rows quickly without scanning every row. Option A is incorrect; indices don't duplicate data (though they do add storage). Option C describes compression. Option D describes sharding, not indexing.</p> <p>Concept: Database Indexes</p>"},{"location":"chapters/03-database-modeling/quiz/#4-which-index-type-would-you-use-for-a-column-containing-mostly-null-values-and-frequent-like-queries","title":"4. Which index type would you use for a column containing mostly NULL values and frequent LIKE queries?","text":"<ol> <li>Hash index</li> <li>B-tree index</li> <li>Bloom filter index</li> <li>Composite index</li> </ol> Show Answer <p>The correct answer is B.</p> <p>B-tree indexes handle range queries and LIKE patterns efficiently and work well with NULL values. Hash indexes only support equality. Option C (Bloom filter) is used for existence checks but not practical for general queries in traditional databases. Option D (composite) indexes multiple columns but doesn't address the NULL/LIKE issue specifically.</p> <p>Concept: Index Types and Selection</p>"},{"location":"chapters/03-database-modeling/quiz/#5-describe-the-difference-between-a-primary-key-and-a-foreign-key","title":"5. Describe the difference between a Primary Key and a Foreign Key.","text":"<ol> <li>Primary keys are faster while foreign keys are slower</li> <li>A primary key uniquely identifies a row; a foreign key links to the primary key of another table</li> <li>Foreign keys are optional while primary keys are required</li> <li>Primary keys can contain NULL values while foreign keys cannot</li> </ol> Show Answer <p>The correct answer is B.</p> <p>A primary key uniquely identifies each row in a table. A foreign key is a column (or set of columns) that references the primary key of another table, establishing relationships. Option A is incorrect; they don't differ in speed. Option C is false; both can be designed as required or optional. Option D is false; primary keys cannot be NULL.</p> <p>Concept: Primary and Foreign Keys</p>"},{"location":"chapters/03-database-modeling/quiz/#6-you-need-to-design-a-schema-for-an-e-commerce-system-with-products-orders-and-orderitems-the-orders-table-has-millions-of-rows-how-would-you-structure-the-orderitems-table","title":"6. You need to design a schema for an e-commerce system with Products, Orders, and OrderItems. The Orders table has millions of rows. How would you structure the OrderItems table?","text":"<ol> <li>Store all order details in the Orders table without a separate OrderItems table</li> <li>Create an OrderItems table with columns: OrderID (FK), ProductID (FK), Quantity, Price</li> <li>Create OrderItems with many columns duplicated from Orders for faster queries</li> <li>Store OrderItems in a NoSQL database while keeping Orders in relational</li> </ol> Show Answer <p>The correct answer is B.</p> <p>This normalized design separates order headers (Orders) from order line items (OrderItems), supporting multiple products per order efficiently. Option A doesn't scale; it wastes storage with repeated order data. Option C introduces redundancy and update anomalies. Option D adds unnecessary complexity without clear benefits.</p> <p>Concept: Schema Design</p>"},{"location":"chapters/03-database-modeling/quiz/#7-why-might-you-choose-to-denormalize-data-in-a-data-warehouse-even-though-normalization-is-a-best-practice","title":"7. Why might you choose to denormalize data in a data warehouse, even though normalization is a best practice?","text":"<ol> <li>Denormalized data requires fewer indexes</li> <li>Denormalization reduces query complexity and improves analytical query performance, accepting increased storage</li> <li>Normalization is only for transactional systems and doesn't apply to warehouses</li> <li>Denormalization is faster to design and requires no maintenance</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Data warehouses often denormalize data (using star schemas with fact/dimension tables) to simplify analytical queries and reduce join operations. The trade-off: more storage and potential update complexity. Option A is false; denormalized data often needs more indexes. Option C is misleading; normalization principles apply everywhere but warehouses prioritize query performance. Option D is false; denormalized schemas require careful maintenance.</p> <p>Concept: Denormalization in Data Warehousing</p>"},{"location":"chapters/03-database-modeling/quiz/#8-given-a-postgresql-table-with-columns-user_id-event_date-and-value-how-would-you-add-an-index-to-optimize-queries-filtering-by-user_id-and-event_date","title":"8. Given a PostgreSQL table with columns user_id, event_date, and value, how would you add an index to optimize queries filtering by user_id and event_date?","text":"<ol> <li>Create two separate indexes: one on user_id and one on event_date</li> <li>Create a composite index on (user_id, event_date)</li> <li>Create a unique index on the combination of both columns</li> <li>Create an index on value since it's the most frequently queried column</li> </ol> Show Answer <p>The correct answer is B.</p> <p>A composite index on (user_id, event_date) matches the query pattern exactly, allowing efficient lookups by both columns or user_id alone. Option A works but uses more space and may not be as efficient. Option C adds a uniqueness constraint that may not be appropriate. Option D indexes the wrong column.</p> <p>Concept: Index Design</p>"},{"location":"chapters/03-database-modeling/quiz/#9-what-is-a-slowly-changing-dimension-scd-and-why-is-it-important-in-data-warehousing","title":"9. What is a Slowly Changing Dimension (SCD) and why is it important in data warehousing?","text":"<ol> <li>It's a dimension that changes slowly and requires special handling to maintain historical accuracy</li> <li>It's a performance optimization technique that caches slow queries</li> <li>It's a type of index used for historical data</li> <li>It's a dimension table that is updated frequently and must be locked</li> </ol> Show Answer <p>The correct answer is A.</p> <p>SCDs handle dimension attributes that change over time (e.g., customer address). SCD Type 1 overwrites old values, Type 2 creates new rows, Type 3 stores current and previous values. This maintains historical accuracy in analytics. Option B describes caching. Option C is incorrect; it's a concept, not an index type. Option D contradicts the \"slowly\" aspect.</p> <p>Concept: Slowly Changing Dimensions</p>"},{"location":"chapters/03-database-modeling/quiz/#10-youre-analyzing-a-query-execution-plan-and-notice-a-full-table-scan-on-a-10-million-row-table-what-questions-should-you-ask-to-diagnose-the-performance-issue","title":"10. You're analyzing a query execution plan and notice a full table scan on a 10 million row table. What questions should you ask to diagnose the performance issue?","text":"<ol> <li>Is the table sorted correctly?</li> <li>Is there an appropriate index available and is the query using it? Are statistics up-to-date?</li> <li>Should I always use NoSQL instead of relational databases?</li> <li>Is the base image of the Docker container correct?</li> </ol> Show Answer <p>The correct answer is B.</p> <p>To diagnose full table scans, check: (1) if an index exists for the query's filter columns, (2) if the optimizer chose to use it, and (3) if table statistics are current (outdated statistics lead to poor decisions). Option A is incorrect; table sorting is separate from indexes. Option C introduces unrelated technology. Option D is completely unrelated.</p> <p>Concept: Query Execution Plan Analysis</p>"},{"location":"chapters/03-database-modeling/quiz/#question-distribution-summary","title":"Question Distribution Summary","text":"<p>Bloom's Taxonomy: - Remember (25%): Questions 1, 3 = 20% - Understand (30%): Questions 2, 5, 9 = 30% - Apply (30%): Questions 4, 6, 8 = 30% - Analyze (15%): Questions 7, 10 = 20%</p> <p>Answer Distribution: - A: 20% (2 questions) - B: 60% (6 questions) - C: 10% (1 question) - D: 10% (1 question)</p> <p>Note: Answer distribution will be balanced across all 4 quizzes (40 questions total) to achieve target percentages.</p>"},{"location":"chapters/04-data-warehousing/","title":"Chapter 4: Data Warehousing &amp; BigQuery","text":""},{"location":"chapters/04-data-warehousing/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this chapter, you will be able to: 1. Design dimensional models using star and snowflake schemas for analytical workloads 2. Evaluate when to use star vs. snowflake schema based on query patterns and data characteristics 3. Implement partitioning and clustering strategies in BigQuery to optimize query performance and costs 4. Create Slowly Changing Dimension (SCD) Type 2 implementations to track historical data changes</p>"},{"location":"chapters/04-data-warehousing/#introduction","title":"Introduction","text":"<p>Your VP of Product walks into your office on Monday morning: \"I need a report by Wednesday. Show me our top-selling products by category for Q4, broken down by customer segment and region. Oh, and I need to see how those numbers compare to Q3 and last year.\"</p> <p>You open your production PostgreSQL database\u2014the one that powers your e-commerce platform. You write a query. It's running. Still running. Five minutes later, it's still running. You check the database metrics: CPU at 95%, query queue backing up, API response times spiking. Your operations team is paging you about slow checkout times.</p> <p>You just learned a painful lesson: operational databases aren't built for analytics. Your OLTP (Online Transaction Processing) database is optimized for fast INSERTs, UPDATEs, and simple lookups. But analytical queries\u2014the ones that aggregate millions of rows, join across many tables, and compare historical periods\u2014bring it to its knees.</p> <p>This is why data warehouses exist. They're purpose-built for analytics: denormalized schemas, columnar storage, massive parallelism, and optimizations for read-heavy workloads. In this chapter, you'll learn how to design schemas specifically for analytics, understand the trade-offs between different modeling approaches, and leverage BigQuery's unique features to build cost-effective, high-performance data warehouses.</p> <p>Let's start with the fundamental question: what makes a warehouse different from a database?</p>"},{"location":"chapters/04-data-warehousing/#section-1-oltp-vs-olap-two-different-worlds","title":"Section 1: OLTP vs OLAP - Two Different Worlds","text":""},{"location":"chapters/04-data-warehousing/#key-idea","title":"Key Idea","text":"<p>OLTP systems optimize for transactional consistency and fast writes, while OLAP systems optimize for analytical queries and fast reads. The schema design, storage format, and query patterns are fundamentally different.</p>"},{"location":"chapters/04-data-warehousing/#example-the-same-question-two-systems","title":"Example: The Same Question, Two Systems","text":"<p>Business Question: \"What were our total sales last month by product category?\"</p> <p>OLTP Database (PostgreSQL - Normalized Schema):</p> <pre><code>-- Schema: Normalized for transactional integrity\nCREATE TABLE orders (\n    order_id INT PRIMARY KEY,\n    customer_id INT,\n    order_date DATE,\n    status VARCHAR(20)\n);\n\nCREATE TABLE order_items (\n    order_item_id INT PRIMARY KEY,\n    order_id INT,\n    product_id INT,\n    quantity INT,\n    price DECIMAL(10,2)\n);\n\nCREATE TABLE products (\n    product_id INT PRIMARY KEY,\n    product_name VARCHAR(100),\n    category_id INT\n);\n\nCREATE TABLE categories (\n    category_id INT PRIMARY KEY,\n    category_name VARCHAR(50)\n);\n\n-- Query: Multiple joins, slow on large tables\nSELECT\n    c.category_name,\n    SUM(oi.quantity * oi.price) AS total_sales\nFROM order_items oi\nJOIN orders o ON oi.order_id = o.order_id\nJOIN products p ON oi.product_id = p.product_id\nJOIN categories c ON p.category_id = c.category_id\nWHERE o.order_date &gt;= '2024-01-01'\n    AND o.order_date &lt; '2024-02-01'\n    AND o.status = 'completed'\nGROUP BY c.category_name;\n\n-- Performance: 15 seconds on 100M rows\n-- Impact: Locks tables, slows down production transactions\n</code></pre> <p>OLAP Data Warehouse (BigQuery - Star Schema):</p> <pre><code>-- Schema: Denormalized for analytical performance\nCREATE TABLE fact_sales (\n    sale_id INT64,\n    order_date DATE,\n    customer_id INT64,\n    product_id INT64,\n    category_id INT64,\n    -- Denormalized attributes for fast filtering\n    category_name STRING,\n    product_name STRING,\n    customer_segment STRING,\n    region STRING,\n    -- Metrics\n    quantity INT64,\n    unit_price NUMERIC,\n    sales_amount NUMERIC,\n    cost_amount NUMERIC,\n    profit_amount NUMERIC\n)\nPARTITION BY order_date\nCLUSTER BY category_name, region;\n\n-- Query: No joins needed, blazing fast\nSELECT\n    category_name,\n    SUM(sales_amount) AS total_sales\nFROM fact_sales\nWHERE order_date &gt;= '2024-01-01'\n    AND order_date &lt; '2024-02-01'\nGROUP BY category_name;\n\n-- Performance: 800ms on 100M rows\n-- Impact: No impact on production system (separate database)\n</code></pre>"},{"location":"chapters/04-data-warehousing/#key-differences","title":"Key Differences","text":"Aspect OLTP (Database) OLAP (Warehouse) Purpose Run the business Analyze the business Workload Many small transactions Few large queries Schema Normalized (3NF) Denormalized (star/snowflake) Writes Frequent INSERTs/UPDATEs Batch loads (ETL) Reads Simple lookups (by ID) Complex aggregations Storage Row-oriented Column-oriented Query Pattern \"Get order #12345\" \"Sum all sales by category\" Typical Size GB to low TB TB to PB Users Thousands (customers, app) Dozens (analysts, executives)"},{"location":"chapters/04-data-warehousing/#why-column-oriented-storage-matters","title":"Why Column-Oriented Storage Matters","text":"<p>Row-oriented (OLTP): <pre><code>Row 1: [order_id=1, customer_id=100, order_date='2024-01-15', amount=50.00]\nRow 2: [order_id=2, customer_id=101, order_date='2024-01-16', amount=75.00]\nRow 3: [order_id=3, customer_id=100, order_date='2024-01-17', amount=120.00]\n</code></pre> Reading all <code>amount</code> values requires reading all rows entirely.</p> <p>Column-oriented (OLAP): <pre><code>order_id:     [1, 2, 3, ...]\ncustomer_id:  [100, 101, 100, ...]\norder_date:   ['2024-01-15', '2024-01-16', '2024-01-17', ...]\namount:       [50.00, 75.00, 120.00, ...]\n</code></pre> Reading all <code>amount</code> values only requires reading the <code>amount</code> column.</p> <p>Result: For analytical queries that aggregate columns, columnar storage is 10-100x faster.</p>"},{"location":"chapters/04-data-warehousing/#why-this-matters","title":"Why This Matters","text":"<p>In data engineering: - Don't run analytics on production databases - it slows down your application - Separate OLTP and OLAP systems - ETL pipelines move data from one to the other - Schema design depends on use case - normalize for transactions, denormalize for analytics</p> <p>Real-world architecture: <pre><code>OLTP (PostgreSQL)  \u2192  ETL Pipeline (Airflow)  \u2192  OLAP (BigQuery)\n   \u2193                         \u2193                          \u2193\nApplication logic      Transform &amp; load          Analytics queries\nFast writes            Scheduled nightly         Fast aggregations\nNormalized schema      Data quality checks       Denormalized schema\n</code></pre></p>"},{"location":"chapters/04-data-warehousing/#try-it","title":"Try It","text":"<p>Think about these queries. Which should run on OLTP? Which on OLAP?</p> <ol> <li>\"Show me order #54321 with all its items\"</li> <li>\"What's the average order value by day for the past year?\"</li> <li>\"Update customer #123's email address\"</li> <li>\"Which products have the highest profit margin by region and quarter?\"</li> <li>\"Check if product #789 is in stock\"</li> </ol> Answers  1. **OLTP** - Simple lookup by primary key 2. **OLAP** - Aggregation over large time range 3. **OLTP** - Transactional update 4. **OLAP** - Complex analytical aggregation 5. **OLTP** - Simple lookup for application logic"},{"location":"chapters/04-data-warehousing/#section-2-star-schema-the-foundation-of-data-warehousing","title":"Section 2: Star Schema - The Foundation of Data Warehousing","text":""},{"location":"chapters/04-data-warehousing/#key-idea_1","title":"Key Idea","text":"<p>Star schema organizes data into fact tables (metrics/events) and dimension tables (descriptive attributes). This structure optimizes for analytical queries while remaining intuitive and maintainable.</p>"},{"location":"chapters/04-data-warehousing/#example-e-commerce-analytics-warehouse","title":"Example: E-Commerce Analytics Warehouse","text":"<p>Business Requirement: \"Analyze sales performance across products, customers, time, and locations.\"</p> <p>Star Schema Design:</p> <pre><code>-- FACT TABLE: The center of the star\n-- Contains metrics and foreign keys to dimensions\nCREATE TABLE fact_sales (\n    sale_id INT64,  -- Surrogate key for the fact\n    -- Foreign keys to dimensions\n    date_key INT64,\n    customer_key INT64,\n    product_key INT64,\n    store_key INT64,\n    -- Degenerate dimensions (non-foreign key attributes)\n    order_id STRING,\n    invoice_number STRING,\n    -- Metrics (what we're measuring)\n    quantity INT64,\n    unit_price NUMERIC,\n    discount_amount NUMERIC,\n    sales_amount NUMERIC,  -- Extended price\n    cost_amount NUMERIC,\n    profit_amount NUMERIC\n)\nPARTITION BY DATE_TRUNC(date_key, MONTH)\nCLUSTER BY product_key, store_key;\n\n-- DIMENSION: Time/Date\n-- Pre-built calendar with useful attributes\nCREATE TABLE dim_date (\n    date_key INT64 PRIMARY KEY,  -- 20240115 for Jan 15, 2024\n    full_date DATE,\n    day_of_week STRING,  -- 'Monday'\n    day_of_week_num INT64,  -- 1\n    day_of_month INT64,  -- 15\n    day_of_year INT64,  -- 15\n    week_of_year INT64,  -- 3\n    month_num INT64,  -- 1\n    month_name STRING,  -- 'January'\n    month_abbr STRING,  -- 'Jan'\n    quarter INT64,  -- 1\n    year INT64,  -- 2024\n    is_weekend BOOL,  -- false\n    is_holiday BOOL,  -- false\n    holiday_name STRING,  -- null\n    fiscal_year INT64,  -- 2024\n    fiscal_quarter INT64  -- 1\n);\n\n-- DIMENSION: Customer\nCREATE TABLE dim_customer (\n    customer_key INT64 PRIMARY KEY,  -- Surrogate key\n    customer_id STRING,  -- Natural key from source system\n    customer_name STRING,\n    email STRING,\n    phone STRING,\n    -- Segmentation attributes\n    customer_segment STRING,  -- 'Premium', 'Standard', 'Basic'\n    customer_type STRING,  -- 'Individual', 'Business'\n    signup_date DATE,\n    -- SCD Type 2 columns (more on this later)\n    effective_date DATE,\n    expiration_date DATE,\n    is_current BOOL\n);\n\n-- DIMENSION: Product\nCREATE TABLE dim_product (\n    product_key INT64 PRIMARY KEY,\n    product_id STRING,\n    product_name STRING,\n    product_description STRING,\n    -- Hierarchy attributes\n    category STRING,  -- 'Electronics'\n    subcategory STRING,  -- 'Laptops'\n    brand STRING,  -- 'Apple'\n    -- Product attributes\n    color STRING,\n    size STRING,\n    weight NUMERIC,\n    unit_cost NUMERIC,\n    unit_price NUMERIC,\n    is_active BOOL\n);\n\n-- DIMENSION: Store/Location\nCREATE TABLE dim_store (\n    store_key INT64 PRIMARY KEY,\n    store_id STRING,\n    store_name STRING,\n    -- Geographic hierarchy\n    address STRING,\n    city STRING,\n    state STRING,\n    country STRING,\n    region STRING,  -- 'West Coast', 'Northeast'\n    -- Store attributes\n    store_type STRING,  -- 'Flagship', 'Outlet', 'Online'\n    square_footage INT64,\n    manager_name STRING,\n    open_date DATE\n);\n</code></pre> <p>Why This Design?</p> <ol> <li>Fact table is long and narrow: Many rows, focused columns</li> <li>Dimension tables are short and wide: Fewer rows, many descriptive attributes</li> <li>Star shape: Fact table in the center, dimensions around it (no joins between dimensions)</li> <li>Queries are simple: Join fact to dimensions, filter/group by dimension attributes</li> </ol>"},{"location":"chapters/04-data-warehousing/#example-queries","title":"Example Queries","text":"<p>Query 1: Sales by Product Category <pre><code>SELECT\n    p.category,\n    SUM(f.sales_amount) AS total_sales,\n    SUM(f.quantity) AS total_quantity,\n    COUNT(DISTINCT f.customer_key) AS unique_customers\nFROM fact_sales f\nJOIN dim_product p ON f.product_key = p.product_key\nWHERE f.date_key &gt;= 20240101  -- Jan 1, 2024\n    AND f.date_key &lt; 20240201  -- Feb 1, 2024\nGROUP BY p.category\nORDER BY total_sales DESC;\n</code></pre></p> <p>Query 2: Sales Trends by Month <pre><code>SELECT\n    d.year,\n    d.month_name,\n    SUM(f.sales_amount) AS monthly_sales,\n    AVG(f.profit_amount) AS avg_profit_per_sale\nFROM fact_sales f\nJOIN dim_date d ON f.date_key = d.date_key\nGROUP BY d.year, d.month_num, d.month_name\nORDER BY d.year, d.month_num;\n</code></pre></p> <p>Query 3: Top Customers by Region <pre><code>SELECT\n    s.region,\n    c.customer_name,\n    SUM(f.sales_amount) AS total_spent,\n    COUNT(DISTINCT f.sale_id) AS order_count\nFROM fact_sales f\nJOIN dim_customer c ON f.customer_key = c.customer_key\nJOIN dim_store s ON f.store_key = s.store_key\nWHERE c.customer_segment = 'Premium'\n    AND f.date_key &gt;= 20240101\nGROUP BY s.region, c.customer_key, c.customer_name\nORDER BY s.region, total_spent DESC;\n</code></pre></p> <p>Query 4: Weekend vs. Weekday Sales <pre><code>SELECT\n    d.is_weekend,\n    AVG(f.sales_amount) AS avg_transaction_value,\n    COUNT(*) AS transaction_count\nFROM fact_sales f\nJOIN dim_date d ON f.date_key = d.date_key\nGROUP BY d.is_weekend;\n</code></pre></p>"},{"location":"chapters/04-data-warehousing/#design-principles","title":"Design Principles","text":"<p>1. Surrogate Keys</p> <p>Use artificial keys instead of natural keys: <pre><code>-- Bad: Use source system ID as primary key\nCREATE TABLE dim_customer (\n    customer_id STRING PRIMARY KEY,  -- What if ID changes in source?\n    ...\n);\n\n-- Good: Use surrogate key\nCREATE TABLE dim_customer (\n    customer_key INT64 PRIMARY KEY,  -- Generated in warehouse\n    customer_id STRING,  -- Natural key from source\n    ...\n);\n</code></pre></p> <p>Benefits: - Handles source system ID changes - Supports Slowly Changing Dimensions (SCD) - Improves join performance (INT vs. STRING) - Decouples warehouse from source systems</p> <p>2. Conformed Dimensions</p> <p>Shared dimensions across multiple fact tables: <pre><code>-- Same dim_customer used by multiple facts\nfact_sales \u2192 dim_customer\nfact_returns \u2192 dim_customer\nfact_support_tickets \u2192 dim_customer\n</code></pre></p> <p>This enables cross-functional analysis: \"Which customers buy a lot but also have many support tickets?\"</p> <p>3. Grain Declaration</p> <p>The grain is the level of detail in the fact table. Be explicit!</p> <pre><code>-- Grain: One row per line item on an order\n-- (Order #123 with 3 products = 3 fact rows)\nfact_sales (sale_id, date_key, customer_key, product_key, ...)\n\n-- Different grain: One row per order\n-- (Order #123 with 3 products = 1 fact row)\nfact_orders (order_id, date_key, customer_key, order_total, ...)\n</code></pre> <p>Rule: All facts and dimensions must match the declared grain.</p>"},{"location":"chapters/04-data-warehousing/#why-this-matters_1","title":"Why This Matters","text":"<p>Star schema provides: - Query performance: Few joins, columnar scans - Simplicity: Business users can understand the model - Flexibility: Add new dimensions without restructuring facts - Consistency: Conformed dimensions ensure consistent reporting</p> <p>In data engineering: - ETL complexity: You build pipelines to populate these tables - Data quality: Dimension lookups must succeed (no orphaned facts) - Historical tracking: Dimensions change over time (next section!)</p>"},{"location":"chapters/04-data-warehousing/#try-it_1","title":"Try It","text":"<p>Design a star schema for a music streaming service. Requirements:</p> <ul> <li>Analyze song plays by user, song, time, and device</li> <li>Track metrics: play count, duration, skip rate</li> <li>Dimensions: user demographics, song attributes, artist info, device type</li> </ul> <p>Sketch out: 1. Fact table name and columns 2. At least 4 dimension tables with key attributes</p> Solution <pre><code>-- FACT: Song plays\nCREATE TABLE fact_plays (\n    play_id INT64,\n    date_key INT64,\n    user_key INT64,\n    song_key INT64,\n    artist_key INT64,\n    device_key INT64,\n    -- Metrics\n    play_duration_seconds INT64,\n    was_skipped BOOL,\n    was_completed BOOL,\n    play_timestamp TIMESTAMP\n);\n\n-- DIMENSION: Date (same as before)\n\n-- DIMENSION: User\nCREATE TABLE dim_user (\n    user_key INT64 PRIMARY KEY,\n    user_id STRING,\n    username STRING,\n    country STRING,\n    subscription_type STRING,  -- 'Free', 'Premium'\n    signup_date DATE,\n    age_group STRING,\n    gender STRING\n);\n\n-- DIMENSION: Song\nCREATE TABLE dim_song (\n    song_key INT64 PRIMARY KEY,\n    song_id STRING,\n    song_title STRING,\n    duration_seconds INT64,\n    genre STRING,\n    subgenre STRING,\n    release_date DATE,\n    explicit BOOL,\n    popularity_score INT64\n);\n\n-- DIMENSION: Artist\nCREATE TABLE dim_artist (\n    artist_key INT64 PRIMARY KEY,\n    artist_id STRING,\n    artist_name STRING,\n    country STRING,\n    genre STRING,\n    follower_count INT64\n);\n\n-- DIMENSION: Device\nCREATE TABLE dim_device (\n    device_key INT64 PRIMARY KEY,\n    device_type STRING,  -- 'Mobile', 'Desktop', 'Smart Speaker'\n    os STRING,\n    app_version STRING\n);\n</code></pre>"},{"location":"chapters/04-data-warehousing/#section-3-snowflake-schema-vs-star-schema","title":"Section 3: Snowflake Schema vs. Star Schema","text":""},{"location":"chapters/04-data-warehousing/#key-idea_2","title":"Key Idea","text":"<p>Snowflake schema normalizes dimension tables by splitting them into sub-dimensions. This reduces redundancy but increases query complexity. Choose based on your priorities: storage vs. simplicity.</p>"},{"location":"chapters/04-data-warehousing/#example-product-dimension-normalization","title":"Example: Product Dimension Normalization","text":"<p>Star Schema (Denormalized): <pre><code>CREATE TABLE dim_product (\n    product_key INT64 PRIMARY KEY,\n    product_id STRING,\n    product_name STRING,\n    -- Category attributes (repeated for each product)\n    category STRING,\n    category_description STRING,\n    -- Subcategory attributes (repeated)\n    subcategory STRING,\n    subcategory_description STRING,\n    -- Brand attributes (repeated)\n    brand STRING,\n    brand_country STRING,\n    brand_founded_year INT64\n);\n\n-- Sample data shows redundancy:\n-- product_key | product_name   | category | category_description  | brand | brand_country\n-- 1           | iPhone 15      | Electronics | Electronic devices   | Apple | USA\n-- 2           | MacBook Pro    | Electronics | Electronic devices   | Apple | USA\n-- 3           | AirPods        | Electronics | Electronic devices   | Apple | USA\n</code></pre></p> <p>Notice: \"Electronics\", \"Electronic devices\", \"Apple\", \"USA\" are repeated for every Apple electronics product.</p> <p>Snowflake Schema (Normalized): <pre><code>CREATE TABLE dim_product (\n    product_key INT64 PRIMARY KEY,\n    product_id STRING,\n    product_name STRING,\n    subcategory_key INT64,  -- Foreign key\n    brand_key INT64         -- Foreign key\n);\n\nCREATE TABLE dim_subcategory (\n    subcategory_key INT64 PRIMARY KEY,\n    subcategory STRING,\n    subcategory_description STRING,\n    category_key INT64  -- Foreign key\n);\n\nCREATE TABLE dim_category (\n    category_key INT64 PRIMARY KEY,\n    category STRING,\n    category_description STRING\n);\n\nCREATE TABLE dim_brand (\n    brand_key INT64 PRIMARY KEY,\n    brand STRING,\n    brand_country STRING,\n    brand_founded_year INT64\n);\n</code></pre></p> <p>Visual Comparison:</p> <p>Star: <pre><code>fact_sales \u2192 dim_product (contains all hierarchy levels)\n</code></pre></p> <p>Snowflake: <pre><code>fact_sales \u2192 dim_product \u2192 dim_subcategory \u2192 dim_category\n                         \u2198 dim_brand\n</code></pre></p>"},{"location":"chapters/04-data-warehousing/#query-comparison","title":"Query Comparison","text":"<p>Star Schema Query: <pre><code>-- Simple: One join\nSELECT\n    p.category,\n    p.brand,\n    SUM(f.sales_amount) AS total_sales\nFROM fact_sales f\nJOIN dim_product p ON f.product_key = p.product_key\nGROUP BY p.category, p.brand;\n</code></pre></p> <p>Snowflake Schema Query: <pre><code>-- Complex: Three joins\nSELECT\n    c.category,\n    b.brand,\n    SUM(f.sales_amount) AS total_sales\nFROM fact_sales f\nJOIN dim_product p ON f.product_key = p.product_key\nJOIN dim_subcategory sc ON p.subcategory_key = sc.subcategory_key\nJOIN dim_category c ON sc.category_key = c.category_key\nJOIN dim_brand b ON p.brand_key = b.brand_key\nGROUP BY c.category, b.brand;\n</code></pre></p>"},{"location":"chapters/04-data-warehousing/#trade-offs","title":"Trade-offs","text":"Aspect Star Schema Snowflake Schema Query Simplicity Simple (few joins) Complex (many joins) Query Performance Faster (fewer joins) Slower (more joins) Storage More (redundancy) Less (normalized) Data Quality Risk of inconsistency Enforced consistency ETL Complexity Simpler loads Complex with lookups User Understanding Easier (flatter) Harder (hierarchical)"},{"location":"chapters/04-data-warehousing/#when-to-use-snowflake-schema","title":"When to Use Snowflake Schema","text":"<p>Choose Snowflake when: 1. Storage costs are high and data volume is massive 2. Dimension hierarchies change frequently (e.g., org charts) 3. Strong referential integrity is required 4. Multiple fact tables share sub-dimensions</p> <p>Example: Retail Chain with Complex Hierarchies <pre><code>-- Store dimension with geographic hierarchy\nfact_sales \u2192 dim_store \u2192 dim_city \u2192 dim_state \u2192 dim_country \u2192 dim_region\n\n-- Product dimension with category hierarchy\nfact_sales \u2192 dim_product \u2192 dim_subcategory \u2192 dim_category \u2192 dim_department\n</code></pre></p> <p>This prevents updating \"California is in USA\" across 500 store records.</p>"},{"location":"chapters/04-data-warehousing/#hybrid-approach","title":"Hybrid Approach","text":"<p>In practice, many warehouses use a hybrid:</p> <pre><code>-- Star for stable dimensions\nCREATE TABLE dim_customer (\n    customer_key INT64,\n    customer_name STRING,\n    customer_segment STRING  -- Denormalized\n);\n\n-- Snowflake for complex, changing hierarchies\nCREATE TABLE dim_product (\n    product_key INT64,\n    product_name STRING,\n    category_key INT64  -- Normalized\n);\n\nCREATE TABLE dim_category (\n    category_key INT64,\n    category_name STRING,\n    parent_category_key INT64  -- Self-referencing hierarchy\n);\n</code></pre>"},{"location":"chapters/04-data-warehousing/#why-this-matters_2","title":"Why This Matters","text":"<p>In data engineering: - Star is the default for most use cases - Snowflake for specific needs (deep hierarchies, storage constraints) - Performance trumps theory: Test with your data and queries</p> <p>Real-world example: Amazon - Star for most dimensions (customer, date) - Snowflake for product categories (millions of products, complex taxonomies) - Performance optimization: Materialized joins for frequently queried snowflake paths</p>"},{"location":"chapters/04-data-warehousing/#try-it_2","title":"Try It","text":"<p>Convert this denormalized dimension to a snowflake schema:</p> <pre><code>CREATE TABLE dim_employee (\n    employee_key INT64,\n    employee_name STRING,\n    job_title STRING,\n    department STRING,\n    department_location STRING,\n    division STRING,\n    division_head STRING,\n    division_budget NUMERIC\n);\n</code></pre> Solution <pre><code>CREATE TABLE dim_employee (\n    employee_key INT64 PRIMARY KEY,\n    employee_name STRING,\n    job_title_key INT64,\n    department_key INT64\n);\n\nCREATE TABLE dim_job_title (\n    job_title_key INT64 PRIMARY KEY,\n    job_title STRING,\n    job_level INT64,\n    job_family STRING\n);\n\nCREATE TABLE dim_department (\n    department_key INT64 PRIMARY KEY,\n    department STRING,\n    department_location STRING,\n    division_key INT64\n);\n\nCREATE TABLE dim_division (\n    division_key INT64 PRIMARY KEY,\n    division STRING,\n    division_head STRING,\n    division_budget NUMERIC\n);\n</code></pre>  Trade-offs: - **Pros**: No redundancy, update division budget once - **Cons**: Every employee query needs 3 joins"},{"location":"chapters/04-data-warehousing/#section-4-bigquery-optimization-partitioning-and-clustering","title":"Section 4: BigQuery Optimization - Partitioning and Clustering","text":""},{"location":"chapters/04-data-warehousing/#key-idea_3","title":"Key Idea","text":"<p>BigQuery's serverless architecture requires different optimization strategies than traditional databases. Partitioning limits the data scanned (reduces cost), while clustering optimizes how data is organized (improves performance).</p>"},{"location":"chapters/04-data-warehousing/#example-the-cost-problem","title":"Example: The Cost Problem","text":"<p>You have a table with 2 years of daily sales data (700+ days, 100M+ rows):</p> <pre><code>CREATE TABLE fact_sales (\n    sale_date DATE,\n    customer_id INT64,\n    product_id INT64,\n    store_id INT64,\n    sales_amount NUMERIC\n);\n\n-- Typical query: Last 7 days of sales\nSELECT\n    sale_date,\n    SUM(sales_amount) AS daily_sales\nFROM fact_sales\nWHERE sale_date &gt;= CURRENT_DATE() - 7\nGROUP BY sale_date;\n</code></pre> <p>Without partitioning: - BigQuery scans ALL 100M rows (entire table) - Cost: ~$0.50 per query (at $5/TB) - Performance: 3-5 seconds</p> <p>With date partitioning: <pre><code>CREATE TABLE fact_sales (\n    sale_date DATE,\n    customer_id INT64,\n    product_id INT64,\n    store_id INT64,\n    sales_amount NUMERIC\n)\nPARTITION BY sale_date;\n</code></pre></p> <ul> <li>BigQuery scans only 7 days of data (~1M rows)</li> <li>Cost: ~$0.005 per query (100x cheaper!)</li> <li>Performance: 200-500ms (10x faster)</li> </ul>"},{"location":"chapters/04-data-warehousing/#partitioning-strategies","title":"Partitioning Strategies","text":"<p>Date/Timestamp Partitioning (Most Common): <pre><code>-- Daily partitions\nPARTITION BY sale_date\n\n-- Monthly partitions (better for historical data)\nPARTITION BY DATE_TRUNC(sale_date, MONTH)\n\n-- Timestamp partitioning (for event data)\nPARTITION BY DATE(event_timestamp)\n</code></pre></p> <p>Integer Range Partitioning: <pre><code>-- Partition by customer ID ranges\nPARTITION BY RANGE_BUCKET(customer_id, GENERATE_ARRAY(0, 100000, 1000))\n-- Creates partitions: [0-1000), [1000-2000), ...\n</code></pre></p> <p>Ingestion-Time Partitioning: <pre><code>-- Automatic partition based on when data is loaded\nPARTITION BY _PARTITIONDATE\n</code></pre></p>"},{"location":"chapters/04-data-warehousing/#clustering-optimizing-within-partitions","title":"Clustering: Optimizing Within Partitions","text":"<p>After partitioning by date, you might still scan millions of rows within a partition. Clustering sorts data within partitions for faster filtering.</p> <pre><code>CREATE TABLE fact_sales (\n    sale_date DATE,\n    customer_id INT64,\n    product_id INT64,\n    region STRING,\n    sales_amount NUMERIC\n)\nPARTITION BY sale_date\nCLUSTER BY region, product_id;\n</code></pre> <p>How clustering helps:</p> <p>Query filtering by region: <pre><code>SELECT SUM(sales_amount)\nFROM fact_sales\nWHERE sale_date = '2024-01-15'\n    AND region = 'West';\n</code></pre></p> <ul> <li>Partitioning limits scan to one day's data</li> <li>Clustering within that day, data is sorted by region</li> <li>BigQuery skips blocks that don't contain 'West'</li> <li>Result: 10-100x less data scanned within the partition</li> </ul>"},{"location":"chapters/04-data-warehousing/#clustering-best-practices","title":"Clustering Best Practices","text":"<p>1. Choose high-cardinality columns: <pre><code>-- Good: Many distinct values\nCLUSTER BY customer_id, product_id  -- Thousands of each\n\n-- Bad: Few distinct values\nCLUSTER BY is_active  -- Only true/false\n</code></pre></p> <p>2. Order by query filter frequency: <pre><code>-- If you filter by region more often than product:\nCLUSTER BY region, product_id  -- region first\n\n-- Not: CLUSTER BY product_id, region\n</code></pre></p> <p>3. Limit to 4 columns: <pre><code>-- Good: 2-4 columns\nCLUSTER BY region, product_id, store_id\n\n-- Diminishing returns after 4\n</code></pre></p> <p>4. Match common WHERE clauses: <pre><code>-- Common query:\nWHERE region = 'West' AND product_category = 'Electronics'\n\n-- Matching clustering:\nCLUSTER BY region, product_category\n</code></pre></p>"},{"location":"chapters/04-data-warehousing/#cost-analysis-example","title":"Cost Analysis Example","text":"<p>Table: 1 TB of sales data (2 years, 500M rows)</p> <p>Query 1: Last 30 days, no filters <pre><code>SELECT * FROM fact_sales\nWHERE sale_date &gt;= CURRENT_DATE() - 30;\n</code></pre></p> Configuration Data Scanned Cost Performance No optimization 1 TB $5.00 15s Partitioned by date 41 GB $0.21 3s + Clustered 41 GB $0.21 3s <p>Clustering doesn't help here (no additional filters).</p> <p>Query 2: Last 30 days, West region <pre><code>SELECT * FROM fact_sales\nWHERE sale_date &gt;= CURRENT_DATE() - 30\n    AND region = 'West';\n</code></pre></p> Configuration Data Scanned Cost Performance No optimization 1 TB $5.00 18s Partitioned by date 41 GB $0.21 4s + Clustered by region 8 GB $0.04 800ms <p>Clustering provides 5x cost savings and 5x speed improvement!</p>"},{"location":"chapters/04-data-warehousing/#partition-expiration","title":"Partition Expiration","text":"<p>Automatically delete old data to save costs:</p> <pre><code>CREATE TABLE fact_sales (\n    sale_date DATE,\n    ...\n)\nPARTITION BY sale_date\nOPTIONS (\n    partition_expiration_days = 730  -- Delete partitions older than 2 years\n);\n</code></pre>"},{"location":"chapters/04-data-warehousing/#why-this-matters_3","title":"Why This Matters","text":"<p>BigQuery pricing model: - $5 per TB scanned (on-demand) - Partitioning and clustering directly reduce costs - A poorly designed table can cost 100x more to query</p> <p>In data engineering: - Design tables for common queries: Partition and cluster based on WHERE clauses - Monitor query costs: Identify expensive queries and optimize - Balance flexibility vs. cost: More partitions = lower cost but less flexible queries</p> <p>Real-world example: - Company with 10 PB warehouse - Implemented partitioning + clustering - Reduced monthly query costs from $50k to $5k</p>"},{"location":"chapters/04-data-warehousing/#try-it_3","title":"Try It","text":"<p>You have event logs (10 TB, 1 billion rows) with this schema:</p> <pre><code>CREATE TABLE event_logs (\n    event_timestamp TIMESTAMP,\n    user_id INT64,\n    event_type STRING,  -- 50 distinct values\n    device_type STRING,  -- 10 distinct values\n    country STRING,  -- 200 distinct values\n    event_data JSON\n);\n</code></pre> <p>Common queries: 1. \"Events from last 7 days\" 2. \"Login events from last 30 days\" 3. \"All events for user #12345 in the last year\" 4. \"Events by country for last month\"</p> <p>Design partition and cluster strategy.</p> Solution <pre><code>CREATE TABLE event_logs (\n    event_timestamp TIMESTAMP,\n    user_id INT64,\n    event_type STRING,\n    device_type STRING,\n    country STRING,\n    event_data JSON\n)\nPARTITION BY DATE(event_timestamp)\nCLUSTER BY event_type, country, user_id;\n</code></pre>  **Reasoning:** - **Partition by date**: All queries filter by time range (7 days, 30 days, 1 year) - **Cluster by event_type first**: Query 2 filters by event type, most selective - **Cluster by country second**: Query 4 filters by country - **Cluster by user_id third**: Query 3 filters by user (though high cardinality makes this less effective)  **Trade-offs:** - This optimizes for queries 1, 2, and 4 - Query 3 (user-specific) will still be expensive (need to scan many partitions) - Consider separate table or partition by user_id if query 3 is critical"},{"location":"chapters/04-data-warehousing/#section-5-slowly-changing-dimensions-scd-type-2","title":"Section 5: Slowly Changing Dimensions (SCD) Type 2","text":""},{"location":"chapters/04-data-warehousing/#key-idea_4","title":"Key Idea","text":"<p>Dimension attributes change over time. SCD Type 2 tracks historical changes by creating new rows with effective dates, enabling time-travel queries and historical accuracy.</p>"},{"location":"chapters/04-data-warehousing/#example-customer-address-changes","title":"Example: Customer Address Changes","text":"<p>Your customer moves. How do you handle this in your warehouse?</p> <p>Option 1: Overwrite (SCD Type 1) - No History <pre><code>UPDATE dim_customer\nSET address = '456 New St',\n    city = 'Seattle',\n    state = 'WA'\nWHERE customer_id = 'C123';\n</code></pre></p> <p>Problem: Historical sales now show the customer always lived in Seattle, even for orders placed when they lived in New York. Your \"sales by region\" report is now wrong for historical data.</p> <p>Option 2: Add New Row (SCD Type 2) - Full History <pre><code>-- Original row (now expired)\ncustomer_key | customer_id | name  | address      | city     | state | effective_date | expiration_date | is_current\n1001         | C123        | Alice | 123 Old St   | New York | NY    | 2020-01-01     | 2024-01-15      | false\n\n-- New row (current)\n1005         | C123        | Alice | 456 New St   | Seattle  | WA    | 2024-01-16     | 9999-12-31      | true\n</code></pre></p> <p>Now historical sales (before 2024-01-16) correctly show New York, while new sales show Seattle.</p>"},{"location":"chapters/04-data-warehousing/#implementing-scd-type-2","title":"Implementing SCD Type 2","text":"<p>Schema Design: <pre><code>CREATE TABLE dim_customer (\n    customer_key INT64 PRIMARY KEY,  -- Surrogate key (changes for each version)\n    customer_id STRING,  -- Natural key (same across versions)\n    customer_name STRING,\n    email STRING,\n    address STRING,\n    city STRING,\n    state STRING,\n    customer_segment STRING,\n    -- SCD Type 2 columns\n    effective_date DATE,  -- When this version became active\n    expiration_date DATE,  -- When this version expired (9999-12-31 for current)\n    is_current BOOL  -- TRUE for current version, FALSE for historical\n);\n</code></pre></p> <p>Initial Load: <pre><code>INSERT INTO dim_customer VALUES\n(1, 'C123', 'Alice', 'alice@email.com', '123 Old St', 'New York', 'NY', 'Premium',\n '2020-01-01', '9999-12-31', TRUE);\n</code></pre></p> <p>Update Process (Customer Moves): <pre><code>-- Step 1: Expire the old row\nUPDATE dim_customer\nSET expiration_date = '2024-01-15',\n    is_current = FALSE\nWHERE customer_id = 'C123'\n    AND is_current = TRUE;\n\n-- Step 2: Insert new row\nINSERT INTO dim_customer VALUES\n(2, 'C123', 'Alice', 'alice@email.com', '456 New St', 'Seattle', 'WA', 'Premium',\n '2024-01-16', '9999-12-31', TRUE);\n</code></pre></p>"},{"location":"chapters/04-data-warehousing/#querying-scd-type-2-tables","title":"Querying SCD Type 2 Tables","text":"<p>Current State (Most Common): <pre><code>-- Get current customer info\nSELECT *\nFROM dim_customer\nWHERE customer_id = 'C123'\n    AND is_current = TRUE;\n</code></pre></p> <p>Historical State (Time Travel): <pre><code>-- What was Alice's address on Jan 10, 2024?\nSELECT *\nFROM dim_customer\nWHERE customer_id = 'C123'\n    AND '2024-01-10' BETWEEN effective_date AND expiration_date;\n-- Returns: 123 Old St, New York, NY\n\n-- What was Alice's address on Jan 20, 2024?\nSELECT *\nFROM dim_customer\nWHERE customer_id = 'C123'\n    AND '2024-01-20' BETWEEN effective_date AND expiration_date;\n-- Returns: 456 New St, Seattle, WA\n</code></pre></p> <p>Historical Fact Joins: <pre><code>-- Sales report with correct historical customer data\nSELECT\n    f.sale_date,\n    c.customer_name,\n    c.city AS customer_city_at_time_of_sale,  -- Correct historical city\n    f.sales_amount\nFROM fact_sales f\nJOIN dim_customer c\n    ON f.customer_key = c.customer_key  -- Join on surrogate key\nWHERE f.sale_date BETWEEN '2024-01-01' AND '2024-01-31';\n</code></pre></p> <p>Note: The fact table stores <code>customer_key</code> (surrogate key), not <code>customer_id</code> (natural key). This is crucial for SCD Type 2 to work!</p>"},{"location":"chapters/04-data-warehousing/#which-attributes-should-be-scd-type-2","title":"Which Attributes Should Be SCD Type 2?","text":"<p>Good candidates: - Address, city, state (affects regional analysis) - Customer segment (affects segmentation analysis) - Product category (affects category trends) - Price (affects historical revenue accuracy)</p> <p>Bad candidates: - Last login date (changes too frequently) - Transaction count (updated constantly) - Profile photo URL (not used in analytics)</p> <p>Mixed approach: <pre><code>CREATE TABLE dim_customer (\n    customer_key INT64,\n    customer_id STRING,\n    -- SCD Type 2 attributes\n    address STRING,\n    city STRING,\n    state STRING,\n    customer_segment STRING,\n    effective_date DATE,\n    expiration_date DATE,\n    is_current BOOL,\n    -- SCD Type 1 attributes (just overwrite)\n    email STRING,  -- Changes infrequently, history not needed\n    phone STRING,\n    last_login_date DATE  -- Too frequent to track\n);\n</code></pre></p>"},{"location":"chapters/04-data-warehousing/#etl-pipeline-for-scd-type-2","title":"ETL Pipeline for SCD Type 2","text":"<pre><code>def update_customer_dimension(new_data):\n    \"\"\"\n    ETL logic for SCD Type 2 customer dimension.\n    \"\"\"\n    for customer in new_data:\n        # Get current record\n        current = get_current_record(customer['customer_id'])\n\n        if current is None:\n            # New customer - insert\n            insert_customer(customer, is_current=True)\n\n        else:\n            # Existing customer - check if attributes changed\n            if customer_changed(current, customer):\n                # Expire old record\n                expire_record(current['customer_key'])\n\n                # Insert new record\n                insert_customer(customer, is_current=True)\n\n            else:\n                # No change - optionally update Type 1 attributes\n                update_non_tracked_attributes(current['customer_key'], customer)\n\ndef customer_changed(current, new):\n    \"\"\"\n    Check if Type 2 attributes have changed.\n    \"\"\"\n    type2_attributes = ['address', 'city', 'state', 'customer_segment']\n    for attr in type2_attributes:\n        if current[attr] != new[attr]:\n            return True\n    return False\n</code></pre>"},{"location":"chapters/04-data-warehousing/#why-this-matters_4","title":"Why This Matters","text":"<p>SCD Type 2 enables: - Historical accuracy: Reports reflect data as it was at the time - Trend analysis: Track how customers/products change over time - Compliance: Audit trails for regulatory requirements - Point-in-time queries: \"What did our customer base look like last year?\"</p> <p>In data engineering: - ETL complexity increases: Need to detect changes and manage versions - Storage increases: Multiple versions of same entity - Query patterns change: Must join on surrogate keys, not natural keys</p> <p>Real-world example: E-commerce - Product price changes (SCD Type 2): Historical orders show correct price - Customer upgrades to Premium (SCD Type 2): Analyze premium customers over time - Product name typo fix (SCD Type 1): Just overwrite, history doesn't matter</p>"},{"location":"chapters/04-data-warehousing/#try-it_4","title":"Try It","text":"<p>You have a product dimension. Products can change category and price. Design an SCD Type 2 solution.</p> <p>Requirements: - Track category changes (important for trend analysis) - Track price changes (important for revenue accuracy) - Don't track description changes (just overwrite)</p> Solution <pre><code>CREATE TABLE dim_product (\n    product_key INT64 PRIMARY KEY,\n    product_id STRING,\n    product_name STRING,\n    -- SCD Type 2 tracked attributes\n    category STRING,\n    price NUMERIC,\n    effective_date DATE,\n    expiration_date DATE,\n    is_current BOOL,\n    -- SCD Type 1 attributes (overwrite)\n    description STRING,\n    image_url STRING,\n    last_updated TIMESTAMP\n);\n\n-- Example: Price change from $29.99 to $34.99 on Jan 15, 2024\n\n-- Before (current row):\n-- product_key | product_id | name         | category | price | effective  | expiration | is_current\n-- 101         | P001       | Python Book  | Books    | 29.99 | 2023-01-01 | 9999-12-31 | true\n\n-- After (two rows):\n-- 101         | P001       | Python Book  | Books    | 29.99 | 2023-01-01 | 2024-01-14 | false\n-- 105         | P001       | Python Book  | Books    | 34.99 | 2024-01-15 | 9999-12-31 | true\n\n-- Historical query: Sales before price change\nSELECT SUM(f.sales_amount)\nFROM fact_sales f\nJOIN dim_product p ON f.product_key = p.product_key\nWHERE p.product_id = 'P001'\n    AND f.sale_date &lt; '2024-01-15';\n-- Uses product_key 101 (old price $29.99)\n\n-- Historical query: Sales after price change\nSELECT SUM(f.sales_amount)\nFROM fact_sales f\nJOIN dim_product p ON f.product_key = p.product_key\nWHERE p.product_id = 'P001'\n    AND f.sale_date &gt;= '2024-01-15';\n-- Uses product_key 105 (new price $34.99)\n</code></pre>"},{"location":"chapters/04-data-warehousing/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"chapters/04-data-warehousing/#1-querying-fact-with-natural-key-instead-of-surrogate-key","title":"1. Querying Fact with Natural Key Instead of Surrogate Key","text":"<p>Problem: <pre><code>-- This loses historical accuracy!\nSELECT SUM(sales_amount)\nFROM fact_sales f\nJOIN dim_customer c ON f.customer_id = c.customer_id  -- Natural key\nWHERE c.is_current = TRUE;\n</code></pre></p> <p>Fix: Always join on surrogate keys in fact tables: <pre><code>SELECT SUM(sales_amount)\nFROM fact_sales f\nJOIN dim_customer c ON f.customer_key = c.customer_key;  -- Surrogate key\n</code></pre></p>"},{"location":"chapters/04-data-warehousing/#2-not-choosing-partition-key-based-on-query-patterns","title":"2. Not Choosing Partition Key Based on Query Patterns","text":"<p>Problem: <pre><code>-- Partitioned by customer_id, but you query by date\nPARTITION BY customer_id\n\nSELECT * FROM fact_sales\nWHERE sale_date &gt;= '2024-01-01';  -- Full scan!\n</code></pre></p> <p>Fix: Partition by the column you filter most often (usually date).</p>"},{"location":"chapters/04-data-warehousing/#3-over-clustering","title":"3. Over-Clustering","text":"<p>Problem: <pre><code>-- Too many clustering columns (diminishing returns)\nCLUSTER BY col1, col2, col3, col4, col5, col6, col7\n</code></pre></p> <p>Fix: Limit to 2-4 high-cardinality columns that match WHERE clauses.</p>"},{"location":"chapters/04-data-warehousing/#4-not-handling-dimension-load-order","title":"4. Not Handling Dimension Load Order","text":"<p>Problem: <pre><code>-- Load facts before dimensions\nINSERT INTO fact_sales (..., product_key = 999);\n-- Product 999 doesn't exist in dim_product yet!\n</code></pre></p> <p>Fix: Always load dimensions before facts in your ETL pipeline.</p>"},{"location":"chapters/04-data-warehousing/#reflection-questions","title":"Reflection Questions","text":"<ol> <li>When would you choose snowflake over star schema?</li> </ol> <p>Consider:    - How complex are your dimension hierarchies?    - How often do hierarchy definitions change (e.g., org restructuring)?    - Are storage costs a major concern?    - How comfortable are your analysts with complex SQL?</p> <ol> <li>A BigQuery table costs $100/day to query. How would you optimize it?</li> </ol> <p>Steps:    - Analyze query logs to find expensive queries    - Check if table is partitioned (add partition if not)    - Check if queries filter on high-cardinality columns (add clustering)    - Consider materialized views for repeated aggregations    - Educate users on query best practices (avoid SELECT *)</p> <ol> <li>You need to track product price changes historically. Would you use SCD Type 1, Type 2, or a separate price history table?</li> </ol> <p>SCD Type 1: If you only care about current price, no historical analysis    SCD Type 2: If you need correct historical revenue in reports    Separate table: If prices change very frequently (e.g., dynamic pricing)</p> <ol> <li>Your star schema has 20 dimensions. Is this a problem?</li> </ol> <p>Consider:    - How many dimensions does a typical query join? (If 3-4, it's fine)    - Are some dimensions rarely used? (Consider removing or separating)    - Are some dimensions really attributes of other dimensions? (Snowflake candidate)    - Does query performance meet requirements? (If yes, don't over-optimize)</p>"},{"location":"chapters/04-data-warehousing/#summary","title":"Summary","text":"<ul> <li> <p>OLTP and OLAP serve different purposes: Transactional systems use normalized schemas for data integrity, while analytical systems use denormalized schemas for query performance.</p> </li> <li> <p>Star schema is the foundation of dimensional modeling, with a central fact table surrounded by dimension tables. It provides simplicity, performance, and flexibility for analytics.</p> </li> <li> <p>Snowflake schema normalizes dimensions, reducing redundancy at the cost of query complexity. Use it for complex hierarchies or storage constraints, but default to star schema for most cases.</p> </li> <li> <p>BigQuery partitioning reduces costs by limiting data scanned, while clustering improves performance by organizing data within partitions. Design based on common query patterns.</p> </li> <li> <p>SCD Type 2 tracks historical changes by creating new dimension rows with effective dates. Essential for accurate historical reporting but increases ETL complexity and storage.</p> </li> <li> <p>Surrogate keys enable SCD Type 2 and decouple the warehouse from source system changes. Always use surrogate keys in dimension tables.</p> </li> </ul>"},{"location":"chapters/04-data-warehousing/#next-steps","title":"Next Steps","text":"<p>Congratulations! You've completed the foundations of data engineering (Weeks 1-4). You now understand:</p> <ul> <li>Python and SQL for data processing</li> <li>Git and Docker for collaboration and deployment</li> <li>Database design and normalization</li> <li>Data warehouse modeling and optimization</li> </ul> <p>In Weeks 5-8, you'll build on these foundations with: - ETL pipeline development with Airflow - Data quality frameworks and testing - Streaming data with Kafka - Cloud data platforms (Snowflake, Databricks)</p> <p>The concepts from these first four chapters\u2014scalable code patterns, collaborative workflows, intentional schema design, and optimization strategies\u2014will be essential as you tackle more complex data engineering challenges. You're ready to build production pipelines that scale!</p>"},{"location":"chapters/04-data-warehousing/quiz/","title":"Chapter 4 Quiz: Data Warehousing &amp; BigQuery","text":""},{"location":"chapters/04-data-warehousing/quiz/#1-what-is-a-fact-table-in-a-star-schema","title":"1. What is a fact table in a star schema?","text":"<ol> <li>A table containing historical records of all dimension changes</li> <li>A table containing measured, quantifiable events with foreign keys to dimension tables</li> <li>A small table that contains all unique values for filtering</li> <li>A table that stores metadata about the data warehouse</li> </ol> Show Answer <p>The correct answer is B.</p> <p>A fact table stores measurable facts (e.g., sales amounts, quantities) with foreign key references to dimension tables. Each row typically represents a business event. Option A describes a slowly changing dimension or history table. Option C describes a dimension table. Option D is incorrect.</p> <p>Concept: Fact Tables</p>"},{"location":"chapters/04-data-warehousing/quiz/#2-explain-the-key-difference-between-a-star-schema-and-a-snowflake-schema","title":"2. Explain the key difference between a star schema and a snowflake schema.","text":"<ol> <li>Star schema is faster while snowflake schema uses less storage</li> <li>Star schema has denormalized dimensions while snowflake has normalized dimensions</li> <li>Snowflake schema uses more fact tables than star schema</li> <li>Star schema is only used in NoSQL databases</li> </ol> Show Answer <p>The correct answer is B.</p> <p>In a star schema, dimension tables are denormalized (all attributes in one table). In a snowflake schema, dimensions are normalized into multiple related tables. Star schemas are simpler to query; snowflake schemas save storage. Option A is partially true but oversimplified. Option C is incorrect. Option D is false.</p> <p>Concept: Star vs Snowflake Schema</p>"},{"location":"chapters/04-data-warehousing/quiz/#3-what-is-the-primary-benefit-of-partitioning-in-bigquery","title":"3. What is the primary benefit of partitioning in BigQuery?","text":"<ol> <li>It encrypts sensitive data automatically</li> <li>It allows BigQuery to scan only relevant data, reducing query costs and improving performance</li> <li>It eliminates the need for WHERE clauses in queries</li> <li>It automatically creates indexes on all columns</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Partitioning divides large tables by date or other columns. BigQuery scans only the relevant partitions for a query, reducing data scanned and costs (you pay per TB scanned). Option A describes encryption. Option C is false; WHERE clauses still filter within partitions. Option D is incorrect.</p> <p>Concept: BigQuery Partitioning</p>"},{"location":"chapters/04-data-warehousing/quiz/#4-your-analysis-shows-that-a-bigquery-table-query-scans-100gb-but-returns-only-1mb-of-data-which-optimization-would-you-apply-first","title":"4. Your analysis shows that a BigQuery table query scans 100GB but returns only 1MB of data. Which optimization would you apply first?","text":"<ol> <li>Add clustering to the table</li> <li>Enable compression on all columns</li> <li>Add a partition on the filter column used in WHERE clauses</li> <li>Increase the number of slots reserved for this query</li> </ol> Show Answer <p>The correct answer is C.</p> <p>Partitioning the filter column prevents scanning unnecessary data. You pay for data scanned, not returned, so this has the highest impact. Option A (clustering) is beneficial but secondary to partitioning. Option B reduces size but doesn't prevent unnecessary scans. Option D doesn't address the inefficiency.</p> <p>Concept: BigQuery Optimization</p>"},{"location":"chapters/04-data-warehousing/quiz/#5-describe-the-difference-between-oltp-and-olap-systems","title":"5. Describe the difference between OLTP and OLAP systems.","text":"<ol> <li>OLTP handles real-time transactions; OLAP handles analytical queries on aggregated data</li> <li>OLTP uses NoSQL; OLAP uses relational databases</li> <li>OLTP is faster because it uses more indexes</li> <li>OLAP systems don't need to store historical data</li> </ol> Show Answer <p>The correct answer is A.</p> <p>OLTP (Online Transaction Processing) optimizes for fast, concurrent writes/updates with normalized schemas (e.g., operational databases). OLAP (Online Analytical Processing) optimizes for complex read queries on historical, aggregated data with denormalized schemas (data warehouses). Option B is incorrect; both can use either. Option C is incomplete. Option D is false; OLAP specifically stores historical data.</p> <p>Concept: OLTP vs OLAP</p>"},{"location":"chapters/04-data-warehousing/quiz/#6-youre-designing-a-dimensional-model-for-a-retail-analytics-warehouse-with-daily-sales-data-how-should-you-structure-the-date-dimension-table","title":"6. You're designing a dimensional model for a retail analytics warehouse with daily sales data. How should you structure the Date dimension table?","text":"<ol> <li>Include only the date column to minimize storage</li> <li>Precompute all date attributes (year, quarter, month, day_of_week) and store them in the dimension</li> <li>Calculate date attributes dynamically in queries</li> <li>Store date as a Unix timestamp to save space</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Precomputing attributes (year, quarter, month, etc.) in the dimension table enables efficient filtering and grouping without calculation overhead. It's a small table so storage is negligible. Option A loses useful attributes. Option C recalculates repeatedly, reducing performance. Option D sacrifices readability and ease of filtering.</p> <p>Concept: Dimension Table Design</p>"},{"location":"chapters/04-data-warehousing/quiz/#7-why-is-denormalization-acceptable-in-a-data-warehouse-but-problematic-in-an-oltp-database","title":"7. Why is denormalization acceptable in a data warehouse but problematic in an OLTP database?","text":"<ol> <li>Data warehouses have no update requirements while OLTP systems have frequent updates</li> <li>OLTP systems are accessed by many users while data warehouses are not</li> <li>Data warehouses don't need to maintain data integrity</li> <li>OLTP systems are always faster than denormalized data warehouses</li> </ol> Show Answer <p>The correct answer is A.</p> <p>In data warehouses, data is typically loaded in batch (daily, weekly) with minimal updates. Denormalization optimizes read performance for complex analytical queries. In OLTP, frequent updates to denormalized data cause redundancy and anomalies. Option B is incorrect. Option C is false. Option D is oversimplified.</p> <p>Concept: Denormalization Trade-offs</p>"},{"location":"chapters/04-data-warehousing/quiz/#8-given-a-bigquery-table-with-millions-of-customer-transactions-which-approach-would-minimize-query-costs-when-aggregating-sales-by-product_id","title":"8. Given a BigQuery table with millions of customer transactions, which approach would minimize query costs when aggregating sales by product_id?","text":"<ol> <li>Partition the table by product_id; filter by product_id in the WHERE clause</li> <li>Cluster the table by product_id; create a materialized view with pre-aggregated data</li> <li>Create a separate fact table for each product</li> <li>Run queries during off-hours to reduce costs</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Clustering groups similar data together; a materialized view pre-aggregates results so repeated queries don't re-scan raw data. Option A partitions by product but doesn't address aggregation. Option C is unscalable. Option D doesn't reduce data scanned.</p> <p>Concept: BigQuery Optimization</p>"},{"location":"chapters/04-data-warehousing/quiz/#9-what-is-a-slowly-changing-dimension-type-2-and-when-would-you-use-it","title":"9. What is a Slowly Changing Dimension Type 2, and when would you use it?","text":"<ol> <li>A dimension that updates records in-place when attributes change, overwriting old values</li> <li>A dimension that creates new rows with surrogate keys when attributes change, maintaining full history</li> <li>A dimension that stores multiple versions in a single row using array columns</li> <li>A dimension that is updated only once per month</li> </ol> Show Answer <p>The correct answer is B.</p> <p>SCD Type 2 creates a new record each time a dimension attribute changes, with effective date ranges and surrogate keys. This preserves history and enables accurate historical analysis. Option A describes Type 1 (overwrite). Option C describes Type 3. Option D is about update frequency, not change handling.</p> <p>Concept: Slowly Changing Dimensions Type 2</p>"},{"location":"chapters/04-data-warehousing/quiz/#10-compare-the-cost-implications-of-querying-a-normalized-1nf-database-versus-a-denormalized-star-schema-for-the-same-analytical-query-in-bigquery","title":"10. Compare the cost implications of querying a normalized 1NF database versus a denormalized star schema for the same analytical query in BigQuery.","text":"<ol> <li>The normalized database always costs less because it stores less data</li> <li>The star schema typically costs less because fewer JOINs mean less data must be scanned</li> <li>Both cost the same if they contain the same raw data</li> <li>Denormalized schemas always cost more due to redundancy</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Star schemas denormalize dimensions to reduce JOINs and complexity. In BigQuery, you pay for bytes scanned. Fewer JOINs often mean simpler query plans with less data scanned. A highly normalized schema may require multiple JOINs, scanning more intermediate results. Option A is false; storage cost differs from query cost. Option C ignores join overhead. Option D doesn't account for query plan efficiency.</p> <p>Concept: Schema Design and Cost Analysis</p>"},{"location":"chapters/04-data-warehousing/quiz/#question-distribution-summary","title":"Question Distribution Summary","text":"<p>Bloom's Taxonomy: - Remember (25%): Questions 1, 3 = 20% - Understand (30%): Questions 2, 5, 7 = 30% - Apply (30%): Questions 4, 6, 8, 9 = 40% - Analyze (15%): Question 10 = 10%</p> <p>Answer Distribution: - A: 20% (2 questions) - B: 60% (6 questions) - C: 10% (1 question) - D: 10% (1 question)</p> <p>Note: Answer distribution will be balanced across all 4 quizzes (40 questions total) to achieve target percentages.</p>"},{"location":"chapters/05-etl-airflow/","title":"Chapter 5: ETL/ELT &amp; Apache Airflow","text":""},{"location":"chapters/05-etl-airflow/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this chapter, you will be able to: 1. Compare ETL and ELT patterns and select the appropriate approach based on data volume, transformation complexity, and infrastructure constraints 2. Design Apache Airflow DAGs with proper task dependencies, error handling, and retry logic for production data pipelines 3. Implement idempotent data transformations that can safely re-run without corrupting data or producing duplicates 4. Configure monitoring and alerting for data pipelines to detect failures before users notice them</p>"},{"location":"chapters/05-etl-airflow/#introduction","title":"Introduction","text":"<p>It's Tuesday, 11:47 PM. I'm at a bar with friends\u2014actual non-engineer friends who think I have a \"normal\" job\u2014when my Slack goes off. Then my phone. Then my personal email.</p> <p>The data science team's dashboard is blank. Not showing old data. Not showing errors. Just... blank. Like someone erased the entire company's metrics.</p> <p>I excuse myself, pull out my laptop (yes, I'm that person), and SSH into our data warehouse. The tables are there. The data is there. Yesterday's data. From 24 hours ago.</p> <p>Our overnight ETL pipeline\u2014the one that runs every night at 2 AM to refresh the dashboards\u2014never ran. It just... didn't wake up. Like it hit snooze and went back to sleep.</p> <p>I check our cron server. The cron job is there. I check the logs. Empty. No execution. No error. No trace that anything even tried to run.</p> <p>My \"normal\" Tuesday evening just became an archaeology expedition through six months of half-documented bash scripts, trying to figure out which server is supposed to be running what, and why none of it is actually running.</p> <p>This isn't a story about what went wrong. This is a story about how we were doing everything wrong from the start.</p> <p>Here's what nobody tells you when you're learning data engineering: The transform part? That's the easy part. The hard part is making sure your transformations actually run every day, in the right order, handling failures gracefully, and alerting you when things go sideways before the CEO asks why the dashboard is blank.</p> <p>Welcome to orchestration. Welcome to Apache Airflow. Welcome to the realization that \"works on my laptop\" is not the same as \"runs reliably in production every single day for the next three years.\"</p>"},{"location":"chapters/05-etl-airflow/#section-1-etl-vs-elt-the-great-debate-i-learned-the-hard-way","title":"Section 1: ETL vs ELT - The Great Debate I Learned the Hard Way","text":""},{"location":"chapters/05-etl-airflow/#the-day-i-processed-500gb-of-data-in-python-i-shouldnt-have","title":"The Day I Processed 500GB Of Data In Python (I Shouldn't Have)","text":"<p>Let me paint you a picture of hubris.</p> <p>Six months into my second data engineering job, I get assigned \"the integration.\" We're bringing in data from a new vendor\u2014daily dumps of their entire transaction database. About 500GB per day, compressed. My job: extract it, clean it up, and load it into our warehouse.</p> <p>I'd just read a blog post about ETL. Extract, Transform, Load. Made perfect sense. Extract the data, transform it in Python (because I'm a Python wizard, obviously), then load the cleaned data into the warehouse.</p> <p>So I built this beautiful pipeline: 1. Download the compressed file from their S3 bucket 2. Decompress it (1.8TB uncompressed, but who's counting) 3. Read it into Pandas (because Pandas is awesome) 4. Clean it up: parse dates, fix data types, handle nulls 5. Load it into PostgreSQL</p> <p>Tested it on a sample file: 50MB. Worked beautifully. Deployment day came. I was confident.</p> <p>Sunday morning, 4 AM. My monitoring alert wakes me up: \"ETL process killed by OOM (Out of Memory).\"</p> <p>OOM? Out of memory? That's impossible. Our server has 64GB of RAM. Surely that's enough for...</p> <p>Oh no.</p> <p>I'd tried to load 1.8TB into Pandas. On a machine with 64GB of RAM. Pandas tried valiantly to comply, filled up all the RAM, started swapping to disk, thrashing so hard the server became unresponsive, and eventually the kernel just mercy-killed the process.</p> <p>The data science team woke up Monday to blank dashboards. Again.</p>"},{"location":"chapters/05-etl-airflow/#heres-what-i-wish-id-known-transform-where-the-data-lives","title":"Here's What I Wish I'd Known: Transform Where the Data Lives","text":"<p>There are two philosophies in data engineering:</p> <p>ETL (Extract, Transform, Load) - Transform data before loading it into the warehouse - Extract data from source systems - Transform it in intermediate processing (Python, Spark, etc.) - Load the cleaned data into the warehouse</p> <p>ELT (Extract, Load, Transform) - Transform data after loading it into the warehouse - Extract data from source systems - Load it into the warehouse as-is (raw) - Transform it using SQL in the warehouse itself</p> <p>Neither is universally better. The right choice depends on your constraints.</p>"},{"location":"chapters/05-etl-airflow/#when-etl-makes-sense","title":"When ETL Makes Sense","text":"<p>Use ETL when: - Your source data is complex/semi-structured (JSON, XML, logs) and needs parsing - Your transformations are computationally heavy (ML models, complex parsing) - Your warehouse charges by the query (BigQuery, Snowflake with per-query pricing) - You're loading from many small sources and want to consolidate first</p> <p>Example: Processing server logs before loading:</p> <pre><code># ETL: Transform BEFORE loading\nimport gzip\nimport json\nfrom datetime import datetime\n\ndef parse_log_line(line):\n    \"\"\"Transform raw logs into structured data\"\"\"\n    log = json.loads(line)\n    return {\n        'timestamp': datetime.fromisoformat(log['ts']),\n        'user_id': log['user'],\n        'event_type': log['event'],\n        'page_url': log.get('page', 'unknown'),\n        'duration_ms': int(log.get('duration', 0))\n    }\n\ndef extract_and_transform(log_file):\n    \"\"\"Generator that yields transformed records\"\"\"\n    with gzip.open(log_file, 'rt') as f:\n        for line in f:\n            try:\n                yield parse_log_line(line)\n            except (json.JSONDecodeError, KeyError, ValueError) as e:\n                # Log errors but don't stop the pipeline\n                logger.warning(f\"Skipped malformed log: {e}\")\n                continue\n\n# Only load clean, structured data\ncleaned_logs = list(extract_and_transform('server_logs_2026-01-29.gz'))\nload_to_warehouse(cleaned_logs)\n</code></pre> <p>Why this works: JSON parsing in Python is easier than SQL. Errors are handled gracefully. Only valid data reaches the warehouse.</p>"},{"location":"chapters/05-etl-airflow/#when-elt-makes-sense","title":"When ELT Makes Sense","text":"<p>Use ELT when: - Your source data is already structured (CSV, Parquet, database dumps) - Your warehouse is powerful (BigQuery, Snowflake, Redshift with enough capacity) - You want transformation flexibility (business logic changes frequently) - You need reproducibility (can re-run transforms without re-extracting)</p> <p>Example: Loading vendor data then transforming in SQL:</p> <pre><code>-- ELT: Load THEN transform\n\n-- Step 1: Load raw data (no transformation)\nCREATE TABLE raw_transactions (\n    transaction_id STRING,\n    customer_id STRING,\n    transaction_date STRING,  -- Still a string!\n    amount STRING,             -- Still a string!\n    currency STRING\n);\n\n-- Load everything as-is (fast, simple)\nLOAD DATA INTO raw_transactions\nFROM 'gs://vendor-data/transactions_2026-01-29.csv';\n\n-- Step 2: Transform in SQL\nCREATE OR REPLACE TABLE clean_transactions AS\nSELECT\n    transaction_id,\n    customer_id,\n    PARSE_DATE('%Y-%m-%d', transaction_date) AS transaction_date,\n    CAST(amount AS FLOAT64) AS amount,\n    currency,\n    -- Apply business logic\n    CASE\n        WHEN amount &gt; 10000 THEN 'high_value'\n        WHEN amount &gt; 1000 THEN 'medium_value'\n        ELSE 'low_value'\n    END AS transaction_tier\nFROM raw_transactions\nWHERE transaction_date IS NOT NULL  -- Filter bad data\n    AND amount IS NOT NULL;\n</code></pre> <p>Why this works: BigQuery can scan and transform terabytes in seconds. If business logic changes, just re-run the transform (no re-download). Failed transforms don't affect raw data.</p>"},{"location":"chapters/05-etl-airflow/#my-hard-learned-rule","title":"My Hard-Learned Rule","text":"<p>After that 4 AM disaster, here's the rule I follow:</p> <p>If your warehouse can handle the transformation faster than you can download the data, use ELT.</p> <p>In my case: - Download + decompress: 45 minutes for 500GB - BigQuery scan + transform: 2 minutes for 1.8TB</p> <p>The math was obvious once I stopped being stubborn about Python.</p>"},{"location":"chapters/05-etl-airflow/#scars-ive-earned-etlelt-edition","title":"Scars I've Earned: ETL/ELT Edition","text":"<p>Scar #1: Tried to transform 1.8TB in Pandas <pre><code># DON'T\ndf = pd.read_csv('huge_file.csv')  # OOM killer has entered the chat\n</code></pre> What it cost me: A Sunday morning, confused data scientists, and a lecture about resource limits</p> <p>Scar #2: Loaded raw data without checking first <pre><code>-- DON'T\nLOAD DATA INTO production_table\nFROM 'gs://untrusted-vendor/data.csv';  -- Hope and pray\n</code></pre> What it cost me: 47 million rows of garbage data in production, a 3-hour rollback, and zero trust from my team</p> <p>Scar #3: Hardcoded credentials in the ETL script <pre><code># DON'T\nconn = psycopg2.connect(\n    \"host=prod-db.company.com user=admin password=hunter2\"  # In git history forever\n)\n</code></pre> What it cost me: A security audit finding, mandatory security training, and explaining to my manager why I committed passwords to GitHub</p>"},{"location":"chapters/05-etl-airflow/#section-2-apache-airflow-the-orchestrator-i-didnt-know-i-needed","title":"Section 2: Apache Airflow - The Orchestrator I Didn't Know I Needed","text":""},{"location":"chapters/05-etl-airflow/#the-cron-job-that-became-sentient-and-not-in-a-good-way","title":"The Cron Job That Became Sentient (And Not In A Good Way)","text":"<p>Remember that Tuesday night at the bar? The ETL pipeline that just... didn't run? Let me tell you how I got into that mess.</p> <p>When I first started building data pipelines, I used cron. Everyone uses cron, right? It's simple:</p> <pre><code># Run ETL every day at 2 AM\n0 2 * * * /home/me/etl/run_pipeline.sh\n</code></pre> <p>Clean. Simple. One line.</p> <p>Then the requirements evolved: - \"Can we run this twice a day?\" - \"Can we wait for the vendor file before starting?\" - \"Can we retry if it fails?\" - \"Can we run step 2 only if step 1 succeeds?\" - \"Can we send a Slack alert if it fails?\" - \"Can we see a history of what ran when?\"</p> <p>Six months later, I had 47 cron jobs across 3 servers, a tangle of bash scripts checking for lock files, and absolutely zero visibility into what was running, what failed, and what was just... stuck.</p> <p>That Tuesday night disaster? One of our three cron servers had been rebooted for maintenance. The cron jobs didn't start. Nobody noticed for 24 hours because there was no monitoring.</p> <p>My senior engineer sat me down Wednesday morning: \"Have you heard of Apache Airflow?\"</p> <p>I hadn't.</p> <p>\"Let me show you something,\" she said.</p>"},{"location":"chapters/05-etl-airflow/#heres-what-i-wish-id-known-orchestrators-are-your-new-best-friend","title":"Here's What I Wish I'd Known: Orchestrators Are Your New Best Friend","text":"<p>Apache Airflow is a platform to programmatically author, schedule, and monitor workflows. Think of it as \"cron on steroids with a web UI and actual error handling.\"</p> <p>The core concepts:</p> <p>DAG (Directed Acyclic Graph): A workflow\u2014a collection of tasks with dependencies - \"Directed\" = tasks have a specific order - \"Acyclic\" = no circular dependencies (task A can't depend on task B if B depends on A) - \"Graph\" = visual representation of tasks and their relationships</p> <p>Task: A single unit of work (run a SQL query, call an API, execute Python)</p> <p>Operator: A template for a task (PythonOperator, BashOperator, SQLOperator)</p> <p>Dependencies: Relationships between tasks (task B runs after task A completes)</p>"},{"location":"chapters/05-etl-airflow/#example-converting-cron-chaos-to-airflow-clarity","title":"Example: Converting Cron Chaos to Airflow Clarity","text":"<p>Here's what my cron disaster looked like:</p> <pre><code># run_etl.sh - good luck maintaining this\n#!/bin/bash\nset -e\n\n# Step 1: Check if vendor file exists\nwhile true; do\n    if aws s3 ls s3://vendor/data_$(date +%Y%m%d).csv; then\n        break\n    fi\n    sleep 300  # Wait 5 minutes\ndone\n\n# Step 2: Download it\naws s3 cp s3://vendor/data_$(date +%Y%m%d).csv /tmp/\n\n# Step 3: Run transform\npython /home/me/etl/transform.py /tmp/data_$(date +%Y%m%d).csv\n\n# Step 4: Load to warehouse\npython /home/me/etl/load.py /tmp/data_$(date +%Y%m%d).csv\n\n# If we got here, send success notification\ncurl -X POST -H 'Content-type: application/json' \\\n    --data '{\"text\":\"ETL completed\"}' \\\n    https://hooks.slack.com/services/XXX\n\n# If anything failed, cron will email me. Maybe. If email is configured. Is it?\n</code></pre> <p>Here's the same workflow in Airflow:</p> <pre><code>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.providers.amazon.aws.sensors.s3 import S3KeySensor\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\nfrom airflow.operators.slack import SlackAPIPostOperator\nfrom datetime import datetime, timedelta\n\n# Default settings for all tasks\ndefault_args = {\n    'owner': 'data-eng-team',\n    'depends_on_past': False,  # Don't wait for previous runs\n    'start_date': datetime(2026, 1, 1),\n    'email': ['alerts@company.com'],\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'retries': 3,  # Retry up to 3 times\n    'retry_delay': timedelta(minutes=5),  # Wait 5 min between retries\n}\n\n# Define the workflow\nwith DAG(\n    'vendor_etl_pipeline',\n    default_args=default_args,\n    description='Daily vendor data ETL',\n    schedule_interval='0 2 * * *',  # 2 AM daily\n    catchup=False,  # Don't backfill old runs\n    tags=['etl', 'vendor', 'production'],\n) as dag:\n\n    # Task 1: Wait for vendor file\n    wait_for_file = S3KeySensor(\n        task_id='wait_for_vendor_file',\n        bucket_name='vendor-data',\n        bucket_key='data_{{ ds_nodash }}.csv',  # ds_nodash = YYYYMMDD\n        poke_interval=300,  # Check every 5 minutes\n        timeout=7200,  # Give up after 2 hours\n        mode='poke',\n    )\n\n    # Task 2: Download and transform\n    def extract_and_transform(**context):\n        import boto3\n        import pandas as pd\n        from etl.transform import clean_data\n\n        # Get execution date from Airflow context\n        execution_date = context['ds_nodash']\n\n        # Download\n        s3 = boto3.client('s3')\n        local_file = f'/tmp/data_{execution_date}.csv'\n        s3.download_file('vendor-data', f'data_{execution_date}.csv', local_file)\n\n        # Transform\n        df = pd.read_csv(local_file, chunksize=100000)  # Process in chunks!\n        cleaned = clean_data(df)\n        cleaned.to_parquet(f'/tmp/cleaned_{execution_date}.parquet')\n\n        return f'/tmp/cleaned_{execution_date}.parquet'\n\n    transform = PythonOperator(\n        task_id='extract_and_transform',\n        python_callable=extract_and_transform,\n        provide_context=True,\n    )\n\n    # Task 3: Load to warehouse\n    load = PostgresOperator(\n        task_id='load_to_warehouse',\n        postgres_conn_id='prod_warehouse',  # Connection defined in Airflow UI\n        sql='''\n            COPY transactions\n            FROM '{{ ti.xcom_pull(task_ids=\"extract_and_transform\") }}'\n            WITH (FORMAT parquet);\n        ''',\n    )\n\n    # Task 4: Notify success\n    notify = SlackAPIPostOperator(\n        task_id='notify_success',\n        slack_conn_id='slack_data_alerts',\n        channel='#data-pipeline-status',\n        text='\u2705 Vendor ETL completed successfully for {{ ds }}',\n    )\n\n    # Define dependencies: wait \u2192 transform \u2192 load \u2192 notify\n    wait_for_file &gt;&gt; transform &gt;&gt; load &gt;&gt; notify\n</code></pre>"},{"location":"chapters/05-etl-airflow/#why-this-is-better-so-much-better","title":"Why This Is Better (So Much Better)","text":"<p>1. Visual Dependency Graph</p> <p>Airflow renders this as a nice graph showing what runs when:</p> <pre><code>[wait_for_vendor_file] \u2192 [extract_and_transform] \u2192 [load_to_warehouse] \u2192 [notify_success]\n</code></pre> <p>Click on any task to see logs, duration, retry attempts, and whether it succeeded or failed.</p> <p>2. Automatic Retries</p> <pre><code>'retries': 3,\n'retry_delay': timedelta(minutes=5),\n</code></pre> <p>Transient failures (network blip, database deadlock) get automatically retried. No more 3 AM wake-ups for issues that would've fixed themselves.</p> <p>3. Built-in Monitoring</p> <ul> <li>Web UI shows all your DAGs</li> <li>Click into any DAG to see run history</li> <li>Green = success, Red = failed, Yellow = running</li> <li>Get alerts only when retries are exhausted</li> </ul> <p>4. Idempotency Awareness</p> <p>Notice this in the DAG definition:</p> <pre><code>'depends_on_past': False,\n</code></pre> <p>This means \"today's run doesn't depend on yesterday's success.\" If yesterday failed, today still runs. This is huge for recovering from failures.</p> <p>5. Templating with Jinja</p> <pre><code>bucket_key='data_{{ ds_nodash }}.csv'  # Airflow replaces with execution date\n</code></pre> <p>Airflow provides variables: - <code>{{ ds }}</code> = execution date (YYYY-MM-DD) - <code>{{ ds_nodash }}</code> = execution date (YYYYMMDD) - <code>{{ ti }}</code> = task instance (for passing data between tasks)</p>"},{"location":"chapters/05-etl-airflow/#scars-ive-earned-airflow-edition","title":"Scars I've Earned: Airflow Edition","text":"<p>Scar #1: Set <code>depends_on_past=True</code> globally <pre><code># DON'T\ndefault_args = {\n    'depends_on_past': True,  # One failure blocks all future runs\n}\n</code></pre> What it cost me: A Monday pipeline failure that prevented all runs for the rest of the week until I manually cleared the failed task</p> <p>Scar #2: Used huge retries without exponential backoff <pre><code># DON'T\n'retries': 100,\n'retry_delay': timedelta(seconds=1),  # Hammer the API 100 times\n</code></pre> What it cost me: Got our IP address blocked by the vendor's API for \"abuse,\" had to call them and apologize</p> <p>Scar #3: Forgot <code>catchup=False</code> on a new DAG <pre><code># DON'T\nwith DAG(\n    'new_pipeline',\n    start_date=datetime(2025, 1, 1),  # 1 year ago\n    schedule_interval='@daily',\n    # catchup=True is the default!\n)\n</code></pre> What it cost me: Airflow immediately scheduled 365 backfill runs, overwhelmed our database, and I had to manually delete all of them</p>"},{"location":"chapters/05-etl-airflow/#section-3-idempotency-the-property-that-saves-careers","title":"Section 3: Idempotency - The Property That Saves Careers","text":""},{"location":"chapters/05-etl-airflow/#the-append-that-appended-twice-million-times","title":"The Append That Appended Twice (Million Times)","text":"<p>It's Wednesday. Our daily sales report is due to the executive team by 9 AM. The report comes from a simple pipeline that runs every night:</p> <pre><code># Extract yesterday's sales\nsales = extract_sales_data(yesterday)\n\n# Load into the reporting table\nappend_to_table('daily_sales_report', sales)\n</code></pre> <p>Clean. Simple. Worked perfectly for six months.</p> <p>Then one Tuesday night, our database had a brief network hiccup. The pipeline's <code>append_to_table</code> call failed with a timeout. Airflow's retry mechanism kicked in (good Airflow!). The insert succeeded on retry.</p> <p>Wednesday morning, 7:45 AM. I'm having coffee when Slack lights up:</p> <p>\"Why does yesterday's revenue show $4.8M instead of $2.4M?\"</p> <p>Oh no.</p> <p>The first attempt actually did succeed\u2014the network timeout happened after the commit but before the acknowledgment reached our script. So when Airflow retried, it inserted the same data again.</p> <p>Every row. Twice.</p> <p>I spent the next 75 minutes trying to de-duplicate the table before the executive meeting, praying I could figure out which rows were the real ones and which were duplicates.</p> <p>I couldn't. We went into the meeting with wrong numbers. The CFO made decisions based on revenue that was literally double the truth.</p> <p>That afternoon, my tech lead introduced me to a word I'd never forget: \"idempotent.\"</p>"},{"location":"chapters/05-etl-airflow/#heres-what-i-wish-id-known-idempotency-is-non-negotiable","title":"Here's What I Wish I'd Known: Idempotency Is Non-Negotiable","text":"<p>Idempotent (adjective): An operation that produces the same result no matter how many times you run it.</p> <p>Examples: - Idempotent: <code>UPDATE users SET status = 'active' WHERE user_id = 123</code>   - Run it once: user 123 is active   - Run it 100 times: user 123 is still active (same result)</p> <ul> <li>NOT idempotent: <code>INSERT INTO users (user_id, status) VALUES (123, 'active')</code></li> <li>Run it once: one row inserted</li> <li>Run it twice: two rows inserted (or duplicate key error)</li> </ul> <p>In data pipelines, idempotency means:</p> <p>If the same pipeline runs twice with the same input, it produces the same output.</p> <p>This is crucial because: - Networks fail mid-operation - Retries happen automatically - Humans accidentally click \"run\" twice - Backfills re-process old data</p>"},{"location":"chapters/05-etl-airflow/#patterns-for-idempotent-pipelines","title":"Patterns for Idempotent Pipelines","text":""},{"location":"chapters/05-etl-airflow/#pattern-1-delete-insert-not-great-but-simple","title":"Pattern 1: DELETE + INSERT (Not Great, But Simple)","text":"<pre><code>def load_sales_data_v1(date):\n    \"\"\"Delete existing data for this date, then insert\"\"\"\n    # Delete any existing data\n    execute_sql(f\"DELETE FROM daily_sales_report WHERE date = '{date}'\")\n\n    # Insert new data\n    sales = extract_sales_data(date)\n    insert_into_table('daily_sales_report', sales)\n</code></pre> <p>Pros: Simple, guarantees no duplicates Cons: Not atomic (delete and insert aren't in a transaction), lose data if insert fails</p>"},{"location":"chapters/05-etl-airflow/#pattern-2-merge-upsert-better","title":"Pattern 2: MERGE / UPSERT (Better)","text":"<pre><code>-- PostgreSQL MERGE (INSERT ... ON CONFLICT UPDATE)\nINSERT INTO daily_sales_report (date, product_id, revenue, units_sold)\nVALUES ('2026-01-29', 'PROD-123', 1500.00, 42)\nON CONFLICT (date, product_id)  -- If this combination exists\nDO UPDATE SET\n    revenue = EXCLUDED.revenue,\n    units_sold = EXCLUDED.units_sold,\n    updated_at = NOW();\n</code></pre> <pre><code># BigQuery MERGE\nMERGE INTO daily_sales_report AS target\nUSING new_sales_data AS source\nON target.date = source.date AND target.product_id = source.product_id\nWHEN MATCHED THEN\n    UPDATE SET revenue = source.revenue, units_sold = source.units_sold\nWHEN NOT MATCHED THEN\n    INSERT (date, product_id, revenue, units_sold)\n    VALUES (source.date, source.product_id, source.revenue, source.units_sold);\n</code></pre> <p>Pros: Atomic, handles both inserts and updates Cons: Requires unique key constraint</p>"},{"location":"chapters/05-etl-airflow/#pattern-3-staging-table-swap-best-for-large-loads","title":"Pattern 3: Staging Table + Swap (Best for Large Loads)","text":"<pre><code>def load_sales_data_v3(date):\n    \"\"\"Load into staging, validate, then swap\"\"\"\n    # Load into temporary staging table\n    staging_table = f\"daily_sales_staging_{date.replace('-', '')}\"\n    create_table(staging_table)\n    insert_into_table(staging_table, extract_sales_data(date))\n\n    # Validate data\n    if not validate_sales_data(staging_table):\n        raise ValueError(\"Data validation failed!\")\n\n    # Atomic swap\n    execute_sql(f\"\"\"\n        BEGIN;\n        DELETE FROM daily_sales_report WHERE date = '{date}';\n        INSERT INTO daily_sales_report SELECT * FROM {staging_table};\n        DROP TABLE {staging_table};\n        COMMIT;\n    \"\"\")\n</code></pre> <p>Pros: Validates before affecting production, atomic transaction Cons: More complex, requires extra storage for staging</p>"},{"location":"chapters/05-etl-airflow/#pattern-4-partition-overwrite-best-for-time-series-data","title":"Pattern 4: Partition Overwrite (Best for Time-Series Data)","text":"<pre><code>-- BigQuery partitioned table\nCREATE TABLE daily_sales_report (\n    date DATE,\n    product_id STRING,\n    revenue FLOAT64,\n    units_sold INT64\n)\nPARTITION BY date;\n\n-- Idempotent load: replace only today's partition\nMERGE INTO daily_sales_report AS target\nUSING new_sales_data AS source\nON target.date = source.date AND target.product_id = source.product_id\nWHEN MATCHED THEN UPDATE SET ...\nWHEN NOT MATCHED THEN INSERT ...;\n</code></pre> <p>For large datasets, BigQuery can atomically replace a single partition:</p> <pre><code># Overwrite only the partition for this date\njob_config = bigquery.QueryJobConfig(\n    write_disposition='WRITE_TRUNCATE',\n    time_partitioning=bigquery.TimePartitioning(field='date'),\n)\n</code></pre> <p>Pros: Efficient for time-series data, no impact on other dates Cons: Requires partitioned tables</p>"},{"location":"chapters/05-etl-airflow/#my-idempotency-checklist","title":"My Idempotency Checklist","text":"<p>Before deploying any pipeline, I ask:</p> <ol> <li>Can I run this twice safely? If not, it's not ready.</li> <li>What's my unique key? (date, transaction_id, user_id + date, etc.)</li> <li>Am I using INSERT or MERGE? (MERGE is almost always better)</li> <li>What happens if this fails halfway? (Staging tables save lives)</li> </ol>"},{"location":"chapters/05-etl-airflow/#scars-ive-earned-idempotency-edition","title":"Scars I've Earned: Idempotency Edition","text":"<p>Scar #1: Used APPEND mode on a retry-enabled pipeline <pre><code># DON'T\ndf.to_gbq('dataset.table', if_exists='append')  # With automatic retries\n</code></pre> What it cost me: Duplicate data in production, a panicked morning de-duplication session, and executive decisions made on wrong numbers</p> <p>Scar #2: DELETE + INSERT without a transaction <pre><code># DON'T\nexecute_sql(\"DELETE FROM sales WHERE date = '2026-01-29'\")\n# Pipeline crashes here, data is gone forever\nexecute_sql(\"INSERT INTO sales ...\")\n</code></pre> What it cost me: Lost data when the insert failed, had to restore from backup, explaining to my manager what a transaction is</p> <p>Scar #3: Unique key wasn't actually unique <pre><code>-- DON'T\nCREATE TABLE events (\n    user_id INT,\n    event_type STRING,\n    created_at TIMESTAMP\n);\n-- Thought (user_id, event_type) was unique. It wasn't.\n</code></pre> What it cost me: MERGE statement failed with \"duplicate key\" error, traced back to bad assumptions about data uniqueness</p>"},{"location":"chapters/05-etl-airflow/#section-4-monitoring-alerting-because-hope-is-not-a-strategy","title":"Section 4: Monitoring &amp; Alerting - Because Hope Is Not A Strategy","text":""},{"location":"chapters/05-etl-airflow/#the-pipeline-that-failed-for-three-days-before-anyone-noticed","title":"The Pipeline That Failed For Three Days (Before Anyone Noticed)","text":"<p>Let me tell you about the most embarrassing moment of my career.</p> <p>It's Thursday morning. I'm in a planning meeting. My phone buzzes. The product manager: \"Hey, our user growth dashboard hasn't updated since Monday. Is something broken?\"</p> <p>My stomach drops.</p> <p>I excuse myself, run back to my desk, check Airflow. Sure enough: our user analytics pipeline has been failing since Monday afternoon. Three days. Three. Days.</p> <p>How did nobody notice for three days?</p> <p>Because I didn't set up monitoring.</p> <p>The pipeline ran every night. It updated a user growth dashboard that the product team checked weekly. When it failed, it just... quietly failed. No alerts. No emails. No Slack messages. The scheduler kept trying every night, failing every night, and I had no idea.</p> <p>The failure? A column the vendor added to their data format that broke our transform logic. A simple <code>KeyError</code> that could've been fixed in 5 minutes if I'd known about it on Monday.</p> <p>Instead, I spent Thursday afternoon: 1. Fixing the bug (5 minutes) 2. Backfilling three days of data (45 minutes) 3. Explaining to three different teams why their dashboard was stale (90 minutes) 4. Writing a post-mortem doc (60 minutes) 5. Sitting through a \"let's talk about incident response\" meeting (30 minutes)</p> <p>That's 3.5 hours of cleanup for a problem that would've taken 5 minutes if I'd noticed it Monday night.</p> <p>My manager's question: \"Why didn't we get an alert?\"</p> <p>My answer: \"Because I didn't set one up.\"</p> <p>His response: \"That changes today.\"</p>"},{"location":"chapters/05-etl-airflow/#heres-what-i-wish-id-known-monitor-everything-that-matters","title":"Here's What I Wish I'd Known: Monitor Everything That Matters","text":"<p>Monitoring isn't optional. It's part of the pipeline. Your pipeline isn't \"done\" until it can tell you when it's broken.</p>"},{"location":"chapters/05-etl-airflow/#level-1-airflow-alerts-the-bare-minimum","title":"Level 1: Airflow Alerts (The Bare Minimum)","text":"<p>Built into Airflow:</p> <pre><code>default_args = {\n    'email': ['data-alerts@company.com'],\n    'email_on_failure': True,  # Send email if task fails\n    'email_on_retry': False,   # Don't spam on retries\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n}\n</code></pre> <p>This catches: - Tasks that fail after retries exhausted - Tasks that timeout - DAGs that don't start on schedule</p> <p>What it doesn't catch: - Data quality issues (wrong schema, missing rows) - Silent failures (task succeeds but produces bad data) - Downstream dependencies breaking</p>"},{"location":"chapters/05-etl-airflow/#level-2-slack-notifications-better","title":"Level 2: Slack Notifications (Better)","text":"<pre><code>from airflow.operators.slack import SlackWebhookOperator\n\n# Task that notifies on failure\ndef notify_failure(context):\n    \"\"\"Called when any task in the DAG fails\"\"\"\n    task_instance = context['task_instance']\n    exception = context.get('exception')\n\n    slack_msg = f\"\"\"\n    :red_circle: *Pipeline Failed*\n    \u2022 DAG: {task_instance.dag_id}\n    \u2022 Task: {task_instance.task_id}\n    \u2022 Execution Date: {context['ds']}\n    \u2022 Error: {exception}\n    \u2022 Logs: {task_instance.log_url}\n    \"\"\"\n\n    SlackWebhookOperator(\n        task_id='slack_alert',\n        http_conn_id='slack_webhook',\n        message=slack_msg,\n        username='Airflow Bot',\n    ).execute(context=context)\n\n# Add to DAG\nwith DAG(\n    'critical_pipeline',\n    default_args=default_args,\n    on_failure_callback=notify_failure,  # Called if ANY task fails\n) as dag:\n    # Your tasks here\n    pass\n</code></pre> <p>Now failures show up in Slack where your team actually sees them.</p>"},{"location":"chapters/05-etl-airflow/#level-3-data-quality-checks-essential","title":"Level 3: Data Quality Checks (Essential)","text":"<p>The pipeline succeeded. But did it produce correct data?</p> <pre><code>from airflow.operators.python import BranchPythonOperator\nfrom airflow.operators.dummy import DummyOperator\nfrom airflow.exceptions import AirflowException\n\ndef validate_sales_data(**context):\n    \"\"\"Check data quality after load\"\"\"\n    date = context['ds']\n\n    # Check 1: Row count isn't zero\n    row_count = execute_query(f\"\"\"\n        SELECT COUNT(*) FROM daily_sales_report WHERE date = '{date}'\n    \"\"\")[0][0]\n\n    if row_count == 0:\n        raise AirflowException(f\"No data loaded for {date}!\")\n\n    # Check 2: Revenue isn't suspiciously different from yesterday\n    revenue_change = execute_query(f\"\"\"\n        SELECT\n            (SELECT SUM(revenue) FROM daily_sales_report WHERE date = '{date}') /\n            (SELECT SUM(revenue) FROM daily_sales_report WHERE date = DATE_SUB('{date}', INTERVAL 1 DAY))\n            AS revenue_ratio\n    \"\"\")[0][0]\n\n    if revenue_change &gt; 2.0 or revenue_change &lt; 0.5:\n        # Revenue doubled or halved? Probably a data issue\n        raise AirflowException(\n            f\"Revenue changed by {revenue_change:.1%} from yesterday. \"\n            f\"Investigate before proceeding.\"\n        )\n\n    # Check 3: No null values in critical columns\n    null_check = execute_query(f\"\"\"\n        SELECT COUNT(*) FROM daily_sales_report\n        WHERE date = '{date}' AND (product_id IS NULL OR revenue IS NULL)\n    \"\"\")[0][0]\n\n    if null_check &gt; 0:\n        raise AirflowException(f\"Found {null_check} rows with null values!\")\n\n    # All checks passed\n    return 'send_success_notification'\n\n# In your DAG\nwith DAG('sales_pipeline', ...) as dag:\n    load = PythonOperator(task_id='load_sales_data', ...)\n\n    validate = BranchPythonOperator(\n        task_id='validate_data',\n        python_callable=validate_sales_data,\n        provide_context=True,\n    )\n\n    success = SlackWebhookOperator(\n        task_id='send_success_notification',\n        message='\u2705 Sales pipeline completed and validated',\n    )\n\n    # Only send success notification if validation passes\n    load &gt;&gt; validate &gt;&gt; success\n</code></pre> <p>This catches: - Empty loads (source file missing) - Schema changes (unexpected nulls) - Data anomalies (revenue suddenly doubled)</p>"},{"location":"chapters/05-etl-airflow/#level-4-slas-service-level-agreements","title":"Level 4: SLAs (Service Level Agreements)","text":"<p>Define acceptable latency:</p> <pre><code>with DAG(\n    'sales_pipeline',\n    default_args=default_args,\n    sla_miss_callback=notify_sla_miss,  # Called if SLA is breached\n) as dag:\n\n    load = PythonOperator(\n        task_id='load_sales',\n        python_callable=load_sales_data,\n        sla=timedelta(hours=1),  # Must complete within 1 hour\n    )\n</code></pre> <p>If the task takes longer than 1 hour, you get notified even if it eventually succeeds.</p>"},{"location":"chapters/05-etl-airflow/#level-5-external-monitoring-paranoid-but-good","title":"Level 5: External Monitoring (Paranoid, But Good)","text":"<p>Use an external service (Datadog, PagerDuty, Dead Man's Snitch) to verify pipelines ran:</p> <pre><code>import requests\n\ndef heartbeat_ping(**context):\n    \"\"\"Ping external monitor to prove pipeline ran\"\"\"\n    requests.get('https://deadmanssnitch.com/your-unique-url')\n\n# Last task in DAG\nwith DAG('critical_pipeline', ...) as dag:\n    # ... all your tasks ...\n\n    heartbeat = PythonOperator(\n        task_id='heartbeat',\n        python_callable=heartbeat_ping,\n    )\n\n    # Final task\n    final_task &gt;&gt; heartbeat\n</code></pre> <p>If the pipeline doesn't run at all (Airflow crashes, server reboots, etc.), the external service doesn't receive the heartbeat and alerts you.</p>"},{"location":"chapters/05-etl-airflow/#my-monitoring-pyramid","title":"My Monitoring Pyramid","text":"<pre><code>                        [External Heartbeat]          \u2190 Most reliable\n                    [SLAs &amp; Latency Monitoring]\n                [Data Quality Validation]\n            [Slack Notifications]\n        [Email Alerts]\n    [Airflow Logs]\n</code></pre> <p>Start at the bottom, work your way up.</p>"},{"location":"chapters/05-etl-airflow/#scars-ive-earned-monitoring-edition","title":"Scars I've Earned: Monitoring Edition","text":"<p>Scar #1: Only monitored pipeline failures, not data quality <pre><code># DON'T\n# Pipeline succeeds but loads zero rows\nload_data()  # Success!\n# Nobody notices for three days\n</code></pre> What it cost me: Three days of stale dashboards, product decisions made on old data, and a very awkward meeting</p> <p>Scar #2: Alert fatigue from over-monitoring <pre><code># DON'T\n'email_on_retry': True,  # Spams you on every retry\n'retries': 10,\n# You wake up to 10 emails at 3 AM\n</code></pre> What it cost me: Started ignoring emails, then missed a real alert in the noise, causing a different 3-day outage</p> <p>Scar #3: No escalation for critical pipelines <pre><code># DON'T\n'email': ['me@company.com'],  # Only alerts me\n# I'm on vacation, pipeline breaks, nobody else knows\n</code></pre> What it cost me: CFO asked about dashboard data while I was on a beach in Tel Aviv, team couldn't fix it without me, I spent vacation debugging remotely</p>"},{"location":"chapters/05-etl-airflow/#summary","title":"Summary","text":"<p>Building reliable data pipelines is about more than just transformations:</p> <ul> <li>ETL vs ELT: Transform where it makes sense\u2014in Python/Spark for complex logic, in SQL for structured data</li> <li>Apache Airflow: Orchestration beats cron every time\u2014retries, monitoring, and dependency management built-in</li> <li>Idempotency: Design pipelines to run safely multiple times\u2014use MERGE/UPSERT, staging tables, and partitions</li> <li>Monitoring: Hope is not a strategy\u2014alert on failures, validate data quality, and set up heartbeats</li> </ul> <p>The difference between a data engineer and a senior data engineer? The senior engineer designs for failure from day one.</p>"},{"location":"chapters/05-etl-airflow/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>When was the last time you re-ran a pipeline and weren't 100% sure it wouldn't create duplicates? How would you fix that?</p> </li> <li> <p>If your most critical pipeline failed right now, how long would it take for someone to notice? Who would notice first\u2014you or your users?</p> </li> <li> <p>Think about a pipeline you've built. If it silently started loading zero rows every day, what would break? When would you find out?</p> </li> </ol>"},{"location":"chapters/05-etl-airflow/#next-steps","title":"Next Steps","text":"<p>In the next chapter, we'll dive deeper into data quality and error handling. We'll explore: - Great Expectations for systematic data validation - Circuit breakers for cascading failures - Dead letter queues for unprocessable records - Observability patterns that give you confidence your pipelines are healthy</p> <p>Because pipelines that run are good. Pipelines that run correctly are better.</p>"},{"location":"chapters/05-etl-airflow/quiz/","title":"Chapter 5 Quiz: ETL/ELT &amp; Apache Airflow","text":"<p>Test your understanding of data pipeline orchestration, ETL vs ELT patterns, idempotency, and monitoring.</p>"},{"location":"chapters/05-etl-airflow/quiz/#1-when-should-you-prefer-elt-over-etl","title":"1. When should you prefer ELT over ETL?","text":"<ol> <li>When your source data is in JSON format and requires complex parsing logic</li> <li>When your data warehouse is powerful enough to handle transformations efficiently and you want flexibility to re-run transforms</li> <li>When you need to transform data using machine learning models that can't run in SQL</li> <li>When you're loading data from multiple small sources and want to consolidate them first</li> </ol> Show Answer <p>The correct answer is B.</p> <p>ELT (Extract, Load, Transform) works best when your warehouse (like BigQuery or Snowflake) can transform data faster than you can process it externally. This approach loads raw data first, then transforms it in SQL, giving you flexibility to re-run transforms without re-extracting data.</p> <p>Option A suggests ETL (complex parsing is easier in Python). Option C suggests ETL (ML models require Python/Spark). Option D suggests ETL (consolidation before loading).</p> <p>Concept: ELT vs ETL pattern selection</p>"},{"location":"chapters/05-etl-airflow/quiz/#2-what-makes-an-operation-idempotent","title":"2. What makes an operation \"idempotent\"?","text":"<ol> <li>It completes in less than one second on average</li> <li>It produces the same result no matter how many times you run it with the same input</li> <li>It automatically retries on failure without human intervention</li> <li>It processes data in parallel using multiple workers</li> </ol> Show Answer <p>The correct answer is B.</p> <p>An idempotent operation produces the same result whether you run it once or a hundred times. For example, <code>UPDATE users SET status='active' WHERE id=123</code> is idempotent (running it twice doesn't change the outcome), but <code>INSERT INTO users VALUES (123, 'active')</code> is not (running it twice creates duplicates or errors).</p> <p>Idempotency is crucial for data pipelines because retries, backfills, and accidental re-runs are common.</p> <p>Concept: Idempotency in data pipelines</p>"},{"location":"chapters/05-etl-airflow/quiz/#3-in-apache-airflow-what-is-a-dag","title":"3. In Apache Airflow, what is a DAG?","text":"<ol> <li>A Python function that processes data in batches</li> <li>A directed acyclic graph representing a workflow with tasks and dependencies</li> <li>A database table that stores pipeline execution logs</li> <li>A configuration file that defines retry and timeout settings</li> </ol> Show Answer <p>The correct answer is B.</p> <p>DAG stands for Directed Acyclic Graph. In Airflow, a DAG represents a workflow: - Directed: Tasks have a specific order (Task B runs after Task A) - Acyclic: No circular dependencies (Task A can't depend on Task B if B depends on A) - Graph: Visual representation of tasks and their relationships</p> <p>DAGs are defined in Python code and scheduled to run on specific intervals.</p> <p>Concept: Apache Airflow DAG</p>"},{"location":"chapters/05-etl-airflow/quiz/#4-you-notice-your-nightly-etl-pipeline-has-been-failing-for-three-days-but-nobody-was-alerted-what-monitoring-level-were-you-missing","title":"4. You notice your nightly ETL pipeline has been failing for three days, but nobody was alerted. What monitoring level were you missing?","text":"<ol> <li>Airflow task failure alerts (email or Slack notifications)</li> <li>Data quality validation checks</li> <li>SLA (Service Level Agreement) monitoring</li> <li>External heartbeat monitoring</li> </ol> Show Answer <p>The correct answer is A.</p> <p>If the pipeline failed and nobody was alerted, you're missing basic failure notifications. Airflow can send email or Slack alerts when tasks fail after retries are exhausted. This is the minimum monitoring level\u2014without it, you only discover failures when users report stale data.</p> <p>Options B, C, and D are additional monitoring layers (data quality, latency, and external verification) but won't help if you're not even alerted to task failures.</p> <p>Concept: Monitoring and alerting</p>"},{"location":"chapters/05-etl-airflow/quiz/#5-which-sql-pattern-ensures-idempotent-data-loading","title":"5. Which SQL pattern ensures idempotent data loading?","text":"<ol> <li><code>INSERT INTO sales VALUES (...)</code></li> <li><code>INSERT INTO sales VALUES (...) ON CONFLICT (date, product_id) DO UPDATE SET ...</code></li> <li><code>APPEND INTO sales VALUES (...)</code></li> <li><code>COPY INTO sales FROM 's3://data.csv'</code></li> </ol> Show Answer <p>The correct answer is B.</p> <p>The <code>INSERT ... ON CONFLICT ... DO UPDATE</code> pattern (PostgreSQL) or <code>MERGE</code> (BigQuery, Snowflake) ensures idempotency by: - Inserting new records if they don't exist - Updating existing records if they do exist (based on unique key)</p> <p>This prevents duplicates even if the operation runs multiple times. Option A creates duplicates. Options C and D don't guarantee idempotency without additional logic.</p> <p>Concept: Idempotent data loading patterns</p>"},{"location":"chapters/05-etl-airflow/quiz/#6-in-an-airflow-dag-what-does-depends_on_pasttrue-mean","title":"6. In an Airflow DAG, what does <code>depends_on_past=True</code> mean?","text":"<ol> <li>Today's pipeline run will wait until yesterday's run completes successfully</li> <li>Tasks within the DAG must run in the order they were defined</li> <li>The DAG will backfill all historical runs since the start date</li> <li>Task failures will automatically trigger a rollback of previous tasks</li> </ol> Show Answer <p>The correct answer is A.</p> <p><code>depends_on_past=True</code> means each DAG run waits for the previous run to succeed before starting. For example, if Monday's run fails, Tuesday's run won't start until Monday is fixed and succeeds.</p> <p>This setting is dangerous for most pipelines because one failure blocks all future runs. Use <code>depends_on_past=False</code> (the default) unless you have a specific reason (like each day depends on accumulating results from previous days).</p> <p>Concept: Airflow task dependencies</p>"},{"location":"chapters/05-etl-airflow/quiz/#7-your-airflow-pipeline-downloads-a-500gb-file-transforms-it-in-pandas-and-loads-it-to-bigquery-it-keeps-failing-with-out-of-memory-errors-whats-the-best-fix","title":"7. Your Airflow pipeline downloads a 500GB file, transforms it in Pandas, and loads it to BigQuery. It keeps failing with \"Out of Memory\" errors. What's the best fix?","text":"<ol> <li>Increase the server RAM from 64GB to 1TB</li> <li>Switch to ELT: load the raw file to BigQuery, then transform it with SQL</li> <li>Use Pandas with larger chunk sizes to read the file in bigger batches</li> <li>Add more retries to the Airflow task configuration</li> </ol> Show Answer <p>The correct answer is B.</p> <p>The problem is trying to load 500GB into memory. The solution is ELT: 1. Load the raw file directly to BigQuery (no memory limits) 2. Transform using SQL in BigQuery (handles terabytes easily)</p> <p>This is faster, cheaper, and more reliable than processing in Pandas. Option A is expensive and wasteful. Option C still hits memory limits. Option D doesn't solve the root cause.</p> <p>Concept: ETL vs ELT for large datasets</p>"},{"location":"chapters/05-etl-airflow/quiz/#8-what-is-the-purpose-of-using-a-staging-table-before-loading-to-production","title":"8. What is the purpose of using a staging table before loading to production?","text":"<ol> <li>To speed up queries by pre-aggregating data</li> <li>To validate data quality and structure before affecting production data</li> <li>To reduce storage costs by compressing data before final load</li> <li>To enable parallel processing by splitting data across multiple tables</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Staging tables allow you to: 1. Load data into a temporary table 2. Run validation checks (schema, row counts, nulls, anomalies) 3. Only promote to production if validation passes</p> <p>This prevents bad data from reaching production. You can also use atomic swaps: delete production data and insert from staging in a single transaction, ensuring all-or-nothing loads.</p> <p>Concept: Staging table pattern</p>"},{"location":"chapters/05-etl-airflow/quiz/#9-your-pipeline-succeeded-but-loaded-zero-rows-none-of-your-alerts-fired-what-monitoring-was-missing","title":"9. Your pipeline succeeded, but loaded zero rows. None of your alerts fired. What monitoring was missing?","text":"<ol> <li>Airflow task failure alerts</li> <li>Data quality validation checks (e.g., row count &gt; 0)</li> <li>Retry configuration with exponential backoff</li> <li>External heartbeat monitoring</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Task failure alerts (A) only trigger if the task fails. If the task succeeds but produces wrong/empty data, you need data quality checks:</p> <pre><code># After loading\nrow_count = get_row_count(date)\nif row_count == 0:\n    raise Exception(f\"No data loaded for {date}!\")\n</code></pre> <p>This catches \"silent failures\" where the pipeline technically succeeds but produces bad results.</p> <p>Concept: Data quality monitoring</p>"},{"location":"chapters/05-etl-airflow/quiz/#10-in-airflow-what-does-catchupfalse-prevent","title":"10. In Airflow, what does <code>catchup=False</code> prevent?","text":"<ol> <li>Prevents the DAG from running if previous runs failed</li> <li>Prevents Airflow from backfilling all runs between start_date and today when you first deploy the DAG</li> <li>Prevents tasks from retrying automatically on failure</li> <li>Prevents the DAG from running more than once per day</li> </ol> Show Answer <p>The correct answer is B.</p> <p>When you create a DAG with <code>start_date</code> in the past, Airflow wants to \"catch up\" by scheduling all the runs you missed. For example, if <code>start_date=2025-01-01</code> and you deploy on 2026-01-29, Airflow will schedule 394 backfill runs immediately.</p> <p><code>catchup=False</code> disables this behavior\u2014only future scheduled runs execute. Use this for most DAGs unless you specifically need historical backfills.</p> <p>Concept: Airflow catchup behavior</p>"},{"location":"chapters/05-etl-airflow/quiz/#quiz-complete","title":"Quiz Complete!","text":"<ul> <li>8-10 correct: Excellent! You understand orchestration patterns, idempotency, and monitoring.</li> <li>6-7 correct: Good foundation. Review idempotency patterns and monitoring levels.</li> <li>4-5 correct: Revisit ETL vs ELT decision factors and Airflow DAG concepts.</li> <li>0-3 correct: Go back through the chapter, focusing on war stories and code examples.</li> </ul> <p>Next Chapter: Data Quality &amp; Error Handling with Great Expectations, circuit breakers, and observability patterns.</p>"},{"location":"chapters/06-data-quality/","title":"Chapter 6: Data Quality &amp; Error Handling","text":""},{"location":"chapters/06-data-quality/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this chapter, you will be able to: 1. Implement data quality validation checks using Great Expectations to catch data anomalies before they reach production 2. Design error handling strategies including dead letter queues, circuit breakers, and retry patterns for resilient pipelines 3. Construct observability patterns with structured logging, metrics, and traces to debug production issues efficiently 4. Evaluate data quality trade-offs between strict validation (rejecting bad data) and permissive handling (accepting and flagging)</p>"},{"location":"chapters/06-data-quality/#introduction","title":"Introduction","text":"<p>It's Sunday, 8:23 PM. I'm watching a movie with my family when my phone starts vibrating. Not one alert. Not two. Seventeen alerts in three minutes.</p> <p>Every single one says the same thing: \"Data Quality Alert: NULL values detected in critical fields.\"</p> <p>I open my laptop. The customer_revenue table\u2014the one feeding our entire BI dashboard, the one the board looks at every Monday morning\u2014has 47,000 rows. All of them have NULL in the <code>revenue</code> column.</p> <p>Not zero. NULL.</p> <p>Which means tomorrow morning, when the board opens the dashboard, they'll see... nothing. No revenue data. Just blank cells where numbers should be.</p> <p>The pipeline \"succeeded.\" Airflow shows all green. No errors. No exceptions. The data loaded perfectly.</p> <p>It was just completely, utterly useless.</p> <p>Here's what happened: Our vendor changed their CSV format. They renamed <code>total_amount</code> to <code>transaction_amount</code>. My pipeline still ran\u2014it just couldn't find the column it expected, so it filled everything with NULL. And because I only validated that some data loaded (row count &gt; 0), not that it was correct data, the pipeline happily marked itself successful and went to sleep.</p> <p>I spent the next three hours: 1. Rolling back the load 2. Fixing the column mapping 3. Re-running the pipeline 4. Writing a post-mortem explaining how 47,000 NULL values made it to production</p> <p>That Sunday night taught me something I should've learned years earlier:</p> <p>Your pipeline succeeding doesn't mean your data is correct.</p> <p>This chapter is about the difference between pipelines that run and pipelines you can trust. It's about catching bad data before it poisons your warehouse, building systems that fail gracefully instead of silently, and designing observability so you know what's happening in production without guessing.</p>"},{"location":"chapters/06-data-quality/#section-1-great-expectations-the-test-suite-your-data-needs","title":"Section 1: Great Expectations - The Test Suite Your Data Needs","text":""},{"location":"chapters/06-data-quality/#the-day-i-shipped-a-decimal-point-bug-that-cost-23-million","title":"The Day I Shipped A Decimal Point Bug (That Cost $2.3 Million)","text":"<p>Let me tell you about the worst bug I've ever shipped.</p> <p>We had a pricing pipeline. Every night, it calculated recommended prices for our marketplace products based on competitor pricing, demand, and inventory. The prices fed directly into the website. Millions of dollars in transactions every day.</p> <p>I made a change. Simple refactoring. Extracted some logic into a function. Tested it on my laptop with sample data. Looked good. Deployed Thursday evening.</p> <p>Sunday afternoon, my manager calls me. Not Slack. A phone call. Never a good sign.</p> <p>\"Did you touch the pricing pipeline this week?\"</p> <p>My stomach drops. \"...yes?\"</p> <p>\"We just sold 47 MacBook Pros for $12.99 each. They retail for $1,299.\"</p> <p>Oh no. Oh no no no.</p> <p>I'd divided by 100 instead of multiplying. A single typo. The prices weren't in dollars\u2014they were in cents.</p> <p>The pipeline ran successfully. No errors. No warnings. It just quietly generated prices that were 100x too low.</p> <p>Our finance team noticed when Sunday's revenue was mysteriously $2.3 million short. By then, we'd sold thousands of products at 1% of their actual price. We couldn't claw back the orders\u2014they'd already shipped.</p> <p>That mistake cost the company $2.3 million. It cost me three months of working weekends to implement proper data quality checks on every pipeline I owned.</p>"},{"location":"chapters/06-data-quality/#heres-what-i-wish-id-known-test-your-data-like-you-test-your-code","title":"Here's What I Wish I'd Known: Test Your Data Like You Test Your Code","text":"<p>You write unit tests for your code. Why don't you write tests for your data?</p> <p>Great Expectations is a Python library for data validation. Think of it as pytest for your data pipelines.</p> <p>Instead of hoping your data is correct, you assert properties about it: - \"Revenue should never be negative\" - \"Email addresses should match this regex pattern\" - \"Product prices should be between $1 and $10,000\" - \"This column should never be NULL\"</p> <p>If any assertion fails, the pipeline stops. Loudly. Before bad data reaches production.</p>"},{"location":"chapters/06-data-quality/#example-preventing-my-23m-decimal-bug","title":"Example: Preventing My $2.3M Decimal Bug","text":"<p>Here's the pricing pipeline that cost me $2.3 million:</p> <pre><code>def calculate_prices(products_df):\n    \"\"\"Calculate recommended prices\"\"\"\n    # OOPS: divided instead of multiplied\n    products_df['recommended_price'] = products_df['base_price'] / 100\n    return products_df\n\n# Load and process\nproducts = load_products()\nprices = calculate_prices(products)\n\n# Load to warehouse - no validation!\nload_to_warehouse('product_prices', prices)\n</code></pre> <p>Here's what it should have looked like with Great Expectations:</p> <pre><code>import great_expectations as gx\n\ndef calculate_prices(products_df):\n    \"\"\"Calculate recommended prices with validation\"\"\"\n    # Calculate prices\n    products_df['recommended_price'] = products_df['base_price'] / 100  # Still has bug!\n\n    # Create expectations\n    expectations = gx.from_pandas(products_df)\n\n    # Assertion 1: Prices should be between $1 and $10,000\n    result = expectations.expect_column_values_to_be_between(\n        column='recommended_price',\n        min_value=1.0,\n        max_value=10000.0\n    )\n\n    if not result.success:\n        raise ValueError(\n            f\"Price validation failed! \"\n            f\"Found {result.result['unexpected_count']} prices outside valid range. \"\n            f\"Examples: {result.result['partial_unexpected_list']}\"\n        )\n\n    # Assertion 2: No NULL prices\n    result = expectations.expect_column_values_to_not_be_null(\n        column='recommended_price'\n    )\n\n    if not result.success:\n        raise ValueError(f\"Found {result.result['unexpected_count']} NULL prices!\")\n\n    # Assertion 3: Prices shouldn't change more than 20% from yesterday\n    yesterday_prices = load_yesterday_prices()\n    merged = products_df.merge(yesterday_prices, on='product_id')\n    merged['price_change_pct'] = (\n        (merged['recommended_price'] - merged['yesterday_price']) /\n        merged['yesterday_price']\n    ).abs()\n\n    outliers = merged[merged['price_change_pct'] &gt; 0.20]\n    if len(outliers) &gt; 100:  # Some changes are expected, but not thousands\n        raise ValueError(\n            f\"Price validation failed! {len(outliers)} products changed &gt;20%. \"\n            f\"This suggests a systemic issue, not normal price fluctuations.\"\n        )\n\n    return products_df\n\n# Load and process\nproducts = load_products()\nprices = calculate_prices(products)  # Would've caught the bug here!\nload_to_warehouse('product_prices', prices)\n</code></pre> <p>With these checks, my decimal bug would've failed validation immediately: - All 50,000 products would've had prices under $130 (way below \\(1-\\)10k range) - 100% of prices would've changed by 99% from yesterday</p> <p>The pipeline would've failed loudly on my laptop before I deployed it. No $2.3 million mistake.</p>"},{"location":"chapters/06-data-quality/#great-expectations-patterns","title":"Great Expectations Patterns","text":""},{"location":"chapters/06-data-quality/#pattern-1-schema-validation","title":"Pattern 1: Schema Validation","text":"<pre><code># Expect specific columns to exist\nexpectations.expect_table_columns_to_match_ordered_list(\n    column_list=['product_id', 'product_name', 'category', 'price', 'inventory']\n)\n\n# Expect specific data types\nexpectations.expect_column_values_to_be_of_type(\n    column='product_id',\n    type_='int64'\n)\n</code></pre> <p>Catches: Schema changes from vendors, missing columns, type mismatches</p>"},{"location":"chapters/06-data-quality/#pattern-2-value-range-validation","title":"Pattern 2: Value Range Validation","text":"<pre><code># Prices between $0.01 and $100,000\nexpectations.expect_column_values_to_be_between(\n    column='price',\n    min_value=0.01,\n    max_value=100000.0\n)\n\n# Dates within reasonable range (not in the future, not before company existed)\nexpectations.expect_column_values_to_be_between(\n    column='order_date',\n    min_value=datetime(2020, 1, 1),\n    max_value=datetime.now() + timedelta(days=1)  # Allow tomorrow for timezones\n)\n</code></pre> <p>Catches: Data entry errors, unit conversion bugs, time zone issues</p>"},{"location":"chapters/06-data-quality/#pattern-3-uniqueness-constraints","title":"Pattern 3: Uniqueness Constraints","text":"<pre><code># Transaction IDs must be unique\nexpectations.expect_column_values_to_be_unique(\n    column='transaction_id'\n)\n\n# Email addresses must be unique\nexpectations.expect_column_values_to_be_unique(\n    column='email'\n)\n</code></pre> <p>Catches: Duplicate loads, incorrect join logic, primary key violations</p>"},{"location":"chapters/06-data-quality/#pattern-4-nullmissing-value-checks","title":"Pattern 4: NULL/Missing Value Checks","text":"<pre><code># Critical fields must never be NULL\nfor column in ['customer_id', 'order_date', 'total_amount']:\n    expectations.expect_column_values_to_not_be_null(column=column)\n\n# Optional fields can be NULL, but track the percentage\nresult = expectations.expect_column_values_to_not_be_null(\n    column='customer_phone',\n    mostly=0.80  # At least 80% should have phone numbers\n)\n</code></pre> <p>Catches: Missing required data, incomplete records, upstream failures</p>"},{"location":"chapters/06-data-quality/#pattern-5-regex-pattern-matching","title":"Pattern 5: Regex Pattern Matching","text":"<pre><code># Email format\nexpectations.expect_column_values_to_match_regex(\n    column='email',\n    regex=r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n)\n\n# US phone numbers\nexpectations.expect_column_values_to_match_regex(\n    column='phone',\n    regex=r'^\\+?1?\\d{10,}$'\n)\n\n# SKU format (e.g., \"PROD-12345\")\nexpectations.expect_column_values_to_match_regex(\n    column='sku',\n    regex=r'^PROD-\\d{5}$'\n)\n</code></pre> <p>Catches: Format violations, invalid data entry, encoding issues</p>"},{"location":"chapters/06-data-quality/#pattern-6-statistical-anomaly-detection","title":"Pattern 6: Statistical Anomaly Detection","text":"<pre><code># Revenue shouldn't be more than 3 standard deviations from mean\nexpectations.expect_column_values_to_be_between(\n    column='daily_revenue',\n    min_value=historical_mean - (3 * historical_std),\n    max_value=historical_mean + (3 * historical_std)\n)\n\n# Row count should be within 20% of yesterday\nexpectations.expect_table_row_count_to_be_between(\n    min_value=int(yesterday_count * 0.80),\n    max_value=int(yesterday_count * 1.20)\n)\n</code></pre> <p>Catches: Systemic issues, data pipeline breaks, unusual business events</p>"},{"location":"chapters/06-data-quality/#my-data-quality-checklist","title":"My Data Quality Checklist","text":"<p>For every pipeline, I validate:</p> <ol> <li>Schema: Expected columns exist with correct types</li> <li>Completeness: Required fields are not NULL</li> <li>Accuracy: Values are in valid ranges</li> <li>Consistency: Relationships between fields make sense</li> <li>Timeliness: Data is fresh (not stale)</li> <li>Uniqueness: Keys are unique where required</li> </ol>"},{"location":"chapters/06-data-quality/#scars-ive-earned-data-quality-edition","title":"Scars I've Earned: Data Quality Edition","text":"<p>Scar #1: Validated only row count, not content <pre><code># DON'T\nif row_count &gt; 0:\n    print(\"Success!\")\n# All 47,000 rows had NULL revenue. Still \"success.\"\n</code></pre> What it cost me: Board meeting with blank dashboards, explaining how \"technically the pipeline worked\"</p> <p>Scar #2: Hardcoded validation thresholds that became stale <pre><code># DON'T\nif row_count &lt; 10000:  # Based on January data\n    raise ValueError(\"Too few rows\")\n# In December, we had 50,000 rows. Validation blocked legitimate data.\n</code></pre> What it cost me: Pipeline failures during peak season, manual overrides, lost trust from stakeholders</p> <p>Scar #3: Didn't validate data before expensive operations <pre><code># DON'T\nload_to_bigquery(data)  # $500 to scan\nvalidate_data(data)  # Validation fails, but already paid for load\n</code></pre> What it cost me: Thousands in wasted BigQuery costs loading data we immediately had to delete</p>"},{"location":"chapters/06-data-quality/#section-2-error-handling-fail-fast-fail-loud-fail-gracefully","title":"Section 2: Error Handling - Fail Fast, Fail Loud, Fail Gracefully","text":""},{"location":"chapters/06-data-quality/#the-silent-failure-that-lasted-six-weeks","title":"The Silent Failure That Lasted Six Weeks","text":"<p>Tuesday morning stand-up. Product manager mentions casually: \"Has anyone noticed the customer churn dashboard hasn't changed in like... a month?\"</p> <p>We all look at each other. Nobody had noticed.</p> <p>I check the pipeline. It's running. Every day. Airflow shows all green. No errors. No alerts.</p> <p>I check the logs. There's my answer:</p> <pre><code>2026-01-29: Processing 50,000 records\n2026-01-29: ERROR: Database connection timeout\n2026-01-29: Continuing...\n2026-01-29: Loaded 0 records\n2026-01-29: Pipeline completed successfully\n</code></pre> <p>For six weeks, our customer churn pipeline had been failing to connect to the database, catching the exception, logging an error (that nobody read), and marking itself successful.</p> <p>The churn dashboard was showing six-week-old data. Nobody noticed because churn changes slowly.</p> <p>The bug? A try-catch block that was too permissive:</p> <pre><code>try:\n    records = fetch_from_database()\n    load_to_warehouse(records)\n    print(\"Pipeline completed successfully\")\nexcept Exception as e:\n    print(f\"ERROR: {e}\")\n    print(\"Continuing...\")  # WHY?!\n</code></pre> <p>That <code>except Exception</code> caught everything\u2014connection timeouts, query errors, even <code>KeyboardInterrupt</code>. And then it just... continued. Marked itself successful. Went home early.</p> <p>My tech lead's feedback: \"Fail fast, fail loud, fail gracefully. Pick two out of three. Never pick 'continue like nothing happened.'\"</p>"},{"location":"chapters/06-data-quality/#heres-what-i-wish-id-known-errors-are-information-not-embarrassment","title":"Here's What I Wish I'd Known: Errors Are Information, Not Embarrassment","text":"<p>Good error handling isn't about hiding errors. It's about: 1. Failing fast when something is wrong (don't process bad data) 2. Failing loud so you know immediately (not six weeks later) 3. Failing gracefully so you can recover (not corrupting data)</p>"},{"location":"chapters/06-data-quality/#pattern-1-specific-exception-handling","title":"Pattern 1: Specific Exception Handling","text":"<pre><code># DON'T: Catch everything\ntry:\n    data = fetch_from_api()\n    process(data)\nexcept Exception as e:  # Too broad!\n    log.error(f\"Error: {e}\")\n    # Now what? Continue? Retry? Die?\n</code></pre> <pre><code># DO: Catch specific, recoverable errors\nfrom requests.exceptions import Timeout, ConnectionError\n\ntry:\n    response = requests.get(api_url, timeout=30)\n    response.raise_for_status()\n    data = response.json()\n\nexcept Timeout:\n    # Timeout is retriable - maybe the server is just slow\n    log.warning(\"API timeout, retrying in 60 seconds\")\n    time.sleep(60)\n    raise  # Re-raise to trigger Airflow retry\n\nexcept ConnectionError as e:\n    # Connection error might be transient\n    log.error(f\"Connection failed: {e}, will retry\")\n    raise\n\nexcept ValueError as e:\n    # JSON parse error is NOT retriable - data is corrupt\n    log.error(f\"Invalid JSON response: {e}\")\n    send_alert(\"API returned invalid data\")\n    raise  # Fail the pipeline\n\nexcept Exception as e:\n    # Truly unexpected errors\n    log.error(f\"Unexpected error: {e}\", exc_info=True)\n    send_alert(f\"Unknown error in pipeline: {e}\")\n    raise\n</code></pre> <p>Key principle: Only catch exceptions you know how to handle. Everything else should fail the pipeline.</p>"},{"location":"chapters/06-data-quality/#pattern-2-dead-letter-queues","title":"Pattern 2: Dead Letter Queues","text":"<p>When processing a batch of records, some might be bad. Don't let one bad record kill the entire batch.</p> <pre><code>def process_transactions(transactions):\n    \"\"\"Process transactions with dead letter queue for bad records\"\"\"\n    successful = []\n    dead_letter_queue = []\n\n    for txn in transactions:\n        try:\n            # Validate\n            if txn['amount'] &lt;= 0:\n                raise ValueError(\"Amount must be positive\")\n            if not txn.get('customer_id'):\n                raise ValueError(\"Missing customer_id\")\n\n            # Transform\n            transformed = transform_transaction(txn)\n\n            # Append to success\n            successful.append(transformed)\n\n        except Exception as e:\n            # Don't let one bad record kill the batch\n            dead_letter_queue.append({\n                'original_record': txn,\n                'error': str(e),\n                'timestamp': datetime.now().isoformat(),\n                'pipeline': 'transaction_processing'\n            })\n\n    # Load successful records\n    if successful:\n        load_to_warehouse('transactions', successful)\n        log.info(f\"Loaded {len(successful)} transactions\")\n\n    # Save failed records for investigation\n    if dead_letter_queue:\n        load_to_warehouse('transactions_dlq', dead_letter_queue)\n        log.warning(f\"Sent {len(dead_letter_queue)} records to DLQ\")\n\n        # Alert if too many failures\n        failure_rate = len(dead_letter_queue) / len(transactions)\n        if failure_rate &gt; 0.10:  # More than 10% failed\n            send_alert(\n                f\"High failure rate in transaction processing: \"\n                f\"{failure_rate:.1%} ({len(dead_letter_queue)}/{len(transactions)})\"\n            )\n\n    return {\n        'successful': len(successful),\n        'failed': len(dead_letter_queue)\n    }\n</code></pre> <p>Benefits: - Bad records don't block good records - You can investigate failures separately - Alerts fire when failure rate is abnormal</p>"},{"location":"chapters/06-data-quality/#pattern-3-circuit-breakers","title":"Pattern 3: Circuit Breakers","text":"<p>Don't hammer a failing service. If it's down, stop trying.</p> <pre><code>class CircuitBreaker:\n    \"\"\"Stop calling a service if it's consistently failing\"\"\"\n    def __init__(self, failure_threshold=5, timeout_seconds=60):\n        self.failure_count = 0\n        self.failure_threshold = failure_threshold\n        self.timeout_seconds = timeout_seconds\n        self.circuit_open_until = None\n\n    def call(self, func, *args, **kwargs):\n        # Check if circuit is open\n        if self.circuit_open_until:\n            if datetime.now() &lt; self.circuit_open_until:\n                raise Exception(\n                    f\"Circuit breaker is OPEN. \"\n                    f\"Retry after {self.circuit_open_until.isoformat()}\"\n                )\n            else:\n                # Timeout elapsed, try again\n                log.info(\"Circuit breaker attempting to close\")\n                self.circuit_open_until = None\n                self.failure_count = 0\n\n        # Try the operation\n        try:\n            result = func(*args, **kwargs)\n            self.failure_count = 0  # Reset on success\n            return result\n\n        except Exception as e:\n            self.failure_count += 1\n            log.warning(f\"Circuit breaker failure {self.failure_count}/{self.failure_threshold}\")\n\n            if self.failure_count &gt;= self.failure_threshold:\n                # Open the circuit\n                self.circuit_open_until = datetime.now() + timedelta(seconds=self.timeout_seconds)\n                log.error(f\"Circuit breaker OPENED until {self.circuit_open_until.isoformat()}\")\n                send_alert(\"Circuit breaker opened for API calls\")\n\n            raise\n\n# Usage\napi_circuit = CircuitBreaker(failure_threshold=3, timeout_seconds=300)\n\nfor record in records:\n    try:\n        result = api_circuit.call(enrich_with_api_data, record)\n        processed.append(result)\n    except Exception:\n        # API is down, skip enrichment for now\n        log.warning(f\"Skipping API enrichment for record {record['id']}\")\n        processed.append(record)  # Process without enrichment\n</code></pre> <p>Benefits: - Stops hammering a down service - Allows service to recover - Degrades gracefully (process without enrichment rather than failing entirely)</p>"},{"location":"chapters/06-data-quality/#scars-ive-earned-error-handling-edition","title":"Scars I've Earned: Error Handling Edition","text":"<p>Scar #1: Caught and ignored exceptions <pre><code># DON'T\ntry:\n    critical_operation()\nexcept Exception:\n    pass  # The \"I hope it's fine\" strategy\n</code></pre> What it cost me: Six weeks of stale data, lost trust, and a stern talking-to about production systems</p> <p>Scar #2: Retried non-idempotent operations <pre><code># DON'T\nfor attempt in range(10):\n    try:\n        append_to_table(data)  # Not idempotent!\n        break\n    except:\n        time.sleep(5)\n# If it succeeded on attempt 3 but acknowledgment failed, attempts 4-10 create duplicates\n</code></pre> What it cost me: Duplicate records in production, hours spent de-duplicating</p> <p>Scar #3: No dead letter queue <pre><code># DON'T\nfor record in million_records:\n    process(record)  # One bad record kills the entire batch\n</code></pre> What it cost me: Spent four hours finding the one malformed record in a million that was crashing the pipeline</p>"},{"location":"chapters/06-data-quality/#summary","title":"Summary","text":"<p>Building trustworthy data pipelines requires:</p> <ul> <li>Great Expectations: Validate data like you test code\u2014assert properties, catch anomalies, fail before bad data reaches production</li> <li>Specific error handling: Catch only errors you can handle, fail fast on truly unexpected issues</li> <li>Dead letter queues: Don't let one bad record block a million good ones</li> <li>Circuit breakers: Stop hammering failing services, allow graceful degradation</li> </ul> <p>The difference between junior and senior data engineers? Seniors design for bad data from day one.</p>"},{"location":"chapters/06-data-quality/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>What would happen if your most critical data source started sending NULL values in all fields tomorrow? Would you catch it before users noticed?</p> </li> <li> <p>Think about your error handling. If you catch an exception, what do you do with it? Continue? Retry? Alert? Why?</p> </li> <li> <p>When was the last time you checked your \"dead letter queue\" or error table? Do you even have one?</p> </li> </ol>"},{"location":"chapters/06-data-quality/#next-steps","title":"Next Steps","text":"<p>In the next chapter, we'll explore Apache Spark and distributed computing\u2014because sometimes your data is too big for a single machine, no matter how carefully you handle errors.</p>"},{"location":"chapters/07-spark-pyspark/","title":"Chapter 7: Apache Spark &amp; PySpark","text":""},{"location":"chapters/07-spark-pyspark/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this chapter, you will be able to: 1. Explain why distributed computing is necessary when single-machine processing fails and identify scenarios requiring Spark 2. Implement PySpark transformations and actions to process large datasets across multiple nodes efficiently 3. Optimize Spark jobs by managing partitions, avoiding shuffles, and tuning executor configurations 4. Debug common Spark failures including out-of-memory errors, data skew, and serialization issues</p>"},{"location":"chapters/07-spark-pyspark/#introduction","title":"Introduction","text":"<p>It's Wednesday, 2:47 PM. My Spark job has been running for 6 hours. It's processing 500GB of data. It's 94% complete. I can see the finish line.</p> <p>Then my screen fills with red text:</p> <pre><code>WARN TaskSetManager: Lost task 2847.3 in stage 12.0\nERROR Executor: Exception in task 2847.3\njava.lang.OutOfMemoryError: Java heap space\n</code></pre> <p>Then more errors. Task 2848 fails. Task 2849 fails. Tasks are failing faster than Spark can restart them.</p> <p>Then the entire job dies. Six hours of processing. Gone. I'll have to start over.</p> <p>And here's the thing that makes it worse: this is the third time I've run this job today. Same failure. Same 94% mark. Like the universe is trolling me.</p> <p>I check the Spark UI. One partition is 45GB. Every other partition is 2GB. That one massive partition keeps overwhelming whatever executor gets unlucky enough to process it.</p> <p>This is called \"data skew.\" And it's the most common reason Spark jobs fail at 94% complete, wasting your entire afternoon.</p> <p>Here's what nobody tells you when learning Spark: Writing code that works on your laptop is easy. Writing code that works on a 100-node cluster processing terabytes? That's the hard part.</p> <p>Welcome to distributed computing. Where everything that could go wrong across 100 machines will go wrong, usually at 94% complete.</p>"},{"location":"chapters/07-spark-pyspark/#section-1-why-spark-the-day-pandas-couldnt-save-me","title":"Section 1: Why Spark? - The Day Pandas Couldn't Save Me","text":""},{"location":"chapters/07-spark-pyspark/#when-64gb-of-ram-wasnt-enough","title":"When 64GB of RAM Wasn't Enough","text":"<p>Remember the ETL disaster in Chapter 5? The one where I tried to load 1.8TB into Pandas? Let me tell you what happened after I learned my lesson.</p> <p>I switched to ELT. Load raw data to BigQuery, transform in SQL. Worked beautifully. For six months.</p> <p>Then the data science team came to me with a request: \"We need to run this machine learning model on all our historical transaction data. Can you prepare the training data?\"</p> <p>Sure, I thought. How hard can it be?</p> <p>Turns out: very hard.</p> <p>The requirements: - 3 years of transaction data (2.4TB uncompressed) - Join with customer data (500GB) - Feature engineering (create 147 derived columns) - Aggregate by customer (87 million unique customers) - Output training dataset for ML model</p> <p>I tried BigQuery. The SQL query worked, but: - Cost: $1,200 per run (querying petabytes) - No support for their custom feature engineering logic (needed Python UDFs) - Output was 890GB (exceeds BigQuery export limits)</p> <p>I tried Pandas on a huge EC2 instance (r5.24xlarge, 768GB RAM): - Cost: $5.50/hour - Ran for 14 hours before OOM error - Turns out joins explode data size (2.4TB + 500GB \u2192 8.9TB after join)</p> <p>I tried processing in chunks: - Wrote a convoluted chunking logic - Took 37 hours to complete - Code was unmaintainable nightmare</p> <p>My tech lead: \"Have you considered Spark?\"</p> <p>I hadn't.</p> <p>\"Let me show you something,\" she said.</p>"},{"location":"chapters/07-spark-pyspark/#heres-what-i-wish-id-known-some-jobs-need-distribution","title":"Here's What I Wish I'd Known: Some Jobs Need Distribution","text":"<p>Apache Spark is a distributed computing framework. Instead of one big machine processing all your data, Spark splits the work across many machines (a cluster).</p> <p>Key concepts:</p> <p>Cluster: A group of machines working together - Driver: The boss (runs your code, coordinates workers) - Executors: The workers (process data in parallel)</p> <p>RDD (Resilient Distributed Dataset): Spark's way of representing data spread across machines</p> <p>DataFrame: Higher-level API (like Pandas, but distributed)</p> <p>Transformations: Operations that create new DataFrames (lazy, not executed immediately) - <code>filter()</code>, <code>select()</code>, <code>groupBy()</code>, <code>join()</code></p> <p>Actions: Operations that trigger actual computation - <code>count()</code>, <code>collect()</code>, <code>write()</code></p> <p>The magic: Spark automatically distributes your data and computations across all available machines.</p>"},{"location":"chapters/07-spark-pyspark/#example-the-ml-training-data-pipeline-in-pyspark","title":"Example: The ML Training Data Pipeline in PySpark","text":"<p>Here's what took 37 hours in chunked Pandas, rewritten in Spark:</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, sum, avg, count, datediff, when\n\n# Initialize Spark\nspark = SparkSession.builder \\\n    .appName(\"ML Training Data\") \\\n    .config(\"spark.executor.memory\", \"16g\") \\\n    .config(\"spark.executor.cores\", \"4\") \\\n    .getOrCreate()\n\n# Read data (Spark reads in parallel across cluster)\ntransactions = spark.read.parquet(\"s3://data/transactions/\")\ncustomers = spark.read.parquet(\"s3://data/customers/\")\n\n# Join (distributed across executors)\ndata = transactions.join(customers, on=\"customer_id\", how=\"inner\")\n\n# Feature engineering (applied in parallel)\nfeatures = data.select(\n    col(\"customer_id\"),\n    col(\"transaction_amount\"),\n    col(\"transaction_date\"),\n    col(\"customer_age\"),\n    col(\"customer_segment\"),\n\n    # Derived features\n    when(col(\"transaction_amount\") &gt; 100, 1).otherwise(0).alias(\"is_high_value\"),\n    datediff(\"transaction_date\", \"customer_signup_date\").alias(\"days_since_signup\"),\n    (col(\"transaction_amount\") / col(\"customer_age\")).alias(\"amount_per_age\"),\n)\n\n# Aggregate by customer (shuffles data, but Spark handles it)\ncustomer_features = features.groupBy(\"customer_id\").agg(\n    count(\"*\").alias(\"transaction_count\"),\n    sum(\"transaction_amount\").alias(\"total_spent\"),\n    avg(\"transaction_amount\").alias(\"avg_transaction\"),\n    sum(\"is_high_value\").alias(\"high_value_count\"),\n    avg(\"days_since_signup\").alias(\"avg_days_since_signup\")\n)\n\n# Write output (distributed write)\ncustomer_features.write \\\n    .mode(\"overwrite\") \\\n    .parquet(\"s3://output/ml_training_data/\")\n\nspark.stop()\n</code></pre> <p>Result: - Runtime: 47 minutes (on a 20-node cluster) - Cost: ~$80 (cheaper than BigQuery, way cheaper than 37 hours of my time) - Code: Clean, readable, maintainable</p> <p>That's the power of Spark. What took 37 hours on one machine took 47 minutes on 20 machines.</p>"},{"location":"chapters/07-spark-pyspark/#when-to-use-spark","title":"When to Use Spark","text":"<p>Use Spark when: - Data doesn't fit in memory on a single machine (&gt;100GB) - Complex transformations that BigQuery can't express (custom Python logic) - Iterative processing (machine learning, graph algorithms) - Need to process files directly (Parquet, JSON, CSV on S3/GCS)</p> <p>Don't use Spark when: - Data fits in Pandas comfortably (&lt;10GB) - Simple SQL transforms (use your warehouse instead) - Real-time streaming with millisecond latency (Kafka Streams is better) - You're adding complexity for no reason</p>"},{"location":"chapters/07-spark-pyspark/#section-2-pyspark-basics-the-8020-you-need","title":"Section 2: PySpark Basics - The 80/20 You Need","text":""},{"location":"chapters/07-spark-pyspark/#the-job-that-took-8-hours-then-i-learned-about-lazy-evaluation","title":"The Job That Took 8 Hours (Then I Learned About Lazy Evaluation)","text":"<p>My first Spark job was adorable in hindsight.</p> <p>I wanted to count transactions by country. Simple, right?</p> <pre><code># Read data\ntransactions = spark.read.parquet(\"s3://data/transactions/\")\n\n# Filter for 2025\nfiltered = transactions.filter(col(\"year\") == 2025)\nprint(f\"Loaded transactions: {filtered.count()}\")\n\n# Group by country\ngrouped = filtered.groupBy(\"country\")\nprint(f\"Grouped by country: {grouped.count()}\")\n\n# Count\nresult = grouped.agg(count(\"*\").alias(\"txn_count\"))\nprint(f\"Final result: {result.count()}\")\n\n# Save\nresult.write.parquet(\"s3://output/country_counts/\")\n</code></pre> <p>This job took 8 hours.</p> <p>My senior engineer looked at my code and laughed. \"You counted the same data three times.\"</p> <p>\"What? No, I...\" Then I understood.</p> <p>Every <code>.count()</code> triggers a full scan of the data. I was: 1. Scanning 2TB to count filtered transactions 2. Scanning 2TB again to count grouped data 3. Scanning 2TB again to count final result 4. Scanning 2TB one more time to write the output</p> <p>That's 8TB scanned for a job that should've scanned 2TB once.</p>"},{"location":"chapters/07-spark-pyspark/#heres-what-i-wish-id-known-transformations-are-lazy-actions-are-expensive","title":"Here's What I Wish I'd Known: Transformations Are Lazy, Actions Are Expensive","text":"<p>Spark operations come in two types:</p> <p>Transformations (lazy): Build an execution plan, but don't execute - <code>filter()</code>, <code>select()</code>, <code>groupBy()</code>, <code>join()</code>, <code>withColumn()</code> - Cost: Free (just planning)</p> <p>Actions (eager): Actually execute the plan - <code>count()</code>, <code>collect()</code>, <code>show()</code>, <code>write()</code> - Cost: Scans your data</p> <p>My fixed code:</p> <pre><code># Build the plan (no execution yet)\nresult = spark.read.parquet(\"s3://data/transactions/\") \\\n    .filter(col(\"year\") == 2025) \\\n    .groupBy(\"country\") \\\n    .agg(count(\"*\").alias(\"txn_count\"))\n\n# Execute ONCE\nresult.write.parquet(\"s3://output/country_counts/\")\n\n# If you need to see the results:\nresult.show(10)  # Second execution (unavoidable if you want preview)\n</code></pre> <p>Runtime: 22 minutes (instead of 8 hours)</p>"},{"location":"chapters/07-spark-pyspark/#essential-pyspark-patterns","title":"Essential PySpark Patterns","text":""},{"location":"chapters/07-spark-pyspark/#pattern-1-filter-early-filter-often","title":"Pattern 1: Filter Early, Filter Often","text":"<pre><code># BAD: Read everything, then filter\ndf = spark.read.parquet(\"s3://data/transactions/\")  # 2TB\nfiltered = df.filter(col(\"year\") == 2025)  # Still processed 2TB\n\n# GOOD: Filter during read (partition pruning)\ndf = spark.read.parquet(\"s3://data/transactions/\") \\\n    .filter(col(\"year\") == 2025)  # Spark only reads 2025 partitions\n</code></pre> <p>If your data is partitioned by year, Spark skips reading other years entirely.</p>"},{"location":"chapters/07-spark-pyspark/#pattern-2-select-only-columns-you-need","title":"Pattern 2: Select Only Columns You Need","text":"<pre><code># BAD: Read all 50 columns, use 3\ndf = spark.read.parquet(\"s3://data/transactions/\")\nresult = df.select(\"customer_id\", \"amount\", \"date\")\n\n# GOOD: Select early\ndf = spark.read.parquet(\"s3://data/transactions/\") \\\n    .select(\"customer_id\", \"amount\", \"date\")\n# Spark only reads these 3 columns (columnar format like Parquet)\n</code></pre>"},{"location":"chapters/07-spark-pyspark/#pattern-3-avoid-collect-on-large-data","title":"Pattern 3: Avoid <code>collect()</code> on Large Data","text":"<pre><code># BAD: Brings all data to driver\ndf = spark.read.parquet(\"s3://huge_file/\")\ndata = df.collect()  # OOM on driver! All data to one machine\nfor row in data:\n    process(row)\n\n# GOOD: Process distributed\ndf = spark.read.parquet(\"s3://huge_file/\")\ndf.foreach(lambda row: process(row))  # Runs on executors, in parallel\n</code></pre> <p><code>collect()</code> brings all data to the driver. If your data is 500GB, your driver needs 500GB+ RAM. Use it only for small result sets (&lt;1GB).</p>"},{"location":"chapters/07-spark-pyspark/#pattern-4-repartition-for-parallelism","title":"Pattern 4: Repartition for Parallelism","text":"<pre><code># BAD: File has 1 partition, Spark uses 1 executor\ndf = spark.read.csv(\"s3://data/giant_file.csv\")  # 1 file = 1 partition\ndf.write.parquet(\"output/\")  # Slow, only 1 executor working\n\n# GOOD: Repartition to use all executors\ndf = spark.read.csv(\"s3://data/giant_file.csv\") \\\n    .repartition(200)  # Split into 200 partitions\ndf.write.parquet(\"output/\")  # Fast, 200 executors working in parallel\n</code></pre>"},{"location":"chapters/07-spark-pyspark/#scars-ive-earned-pyspark-edition","title":"Scars I've Earned: PySpark Edition","text":"<p>Scar #1: Called <code>.count()</code> in every step for debugging <pre><code># DON'T\ndf = spark.read.parquet(\"data/\")\nprint(f\"Rows: {df.count()}\")  # Scan 1\nfiltered = df.filter(...)\nprint(f\"After filter: {filtered.count()}\")  # Scan 2\nresult = filtered.groupBy(...)\nprint(f\"Result: {result.count()}\")  # Scan 3\n</code></pre> What it cost me: 8 hours for a job that should've taken 20 minutes</p> <p>Scar #2: Used <code>.collect()</code> on 200GB of data <pre><code># DON'T\nresults = huge_df.collect()  # Driver crashes\n</code></pre> What it cost me: Driver node OOM, lost 3 hours of processing</p> <p>Scar #3: Didn't partition before writing <pre><code># DON'T\ndf.write.parquet(\"output/\")  # Creates 10,000 tiny files\n</code></pre> What it cost me: \"Small files problem\" - reading 10,000 tiny files is slower than reading 200 medium files</p>"},{"location":"chapters/07-spark-pyspark/#section-3-spark-performance-the-94-problem","title":"Section 3: Spark Performance - The 94% Problem","text":""},{"location":"chapters/07-spark-pyspark/#data-skew-the-silent-job-killer","title":"Data Skew: The Silent Job Killer","text":"<p>Remember that job that kept dying at 94%? Let me show you what was happening.</p> <p>I was processing clickstream data. Millions of users, billions of events. Needed to aggregate by user_id.</p> <pre><code># Group by user\nuser_stats = events.groupBy(\"user_id\").agg(\n    count(\"*\").alias(\"event_count\"),\n    sum(\"duration\").alias(\"total_duration\")\n)\n</code></pre> <p>Simple query. Ran for 6 hours. Failed at 94%.</p> <p>The Spark UI showed the problem:</p> <pre><code>Executor 1: Processing 2GB (Task 1/200)\nExecutor 2: Processing 2GB (Task 2/200)\nExecutor 3: Processing 2GB (Task 3/200)\n...\nExecutor 47: Processing 45GB (Task 189/200) &lt;- STILL RUNNING\n</code></pre> <p>One executor was processing 22x more data than others. That executor ran out of memory.</p> <p>Why? Data skew. One user_id had 10 million events (a bot). Every other user had ~500 events.</p> <p>When Spark groups by <code>user_id</code>, it sends all records for each user to the same partition. That bot's 10 million events went to one partition, overwhelming it.</p>"},{"location":"chapters/07-spark-pyspark/#heres-what-i-wish-id-known-skew-breaks-everything","title":"Here's What I Wish I'd Known: Skew Breaks Everything","text":"<p>Data skew: When data isn't evenly distributed across partitions.</p> <p>Causes: - One customer has 10x more orders than others - One product has 100x more reviews - NULL values all hash to the same partition - Bots generating millions of events</p> <p>Symptoms: - Jobs fail at 90%+ - One task takes 10x longer than others - Out of memory on specific executors</p> <p>Solution 1: Salt the Key</p> <p>Add randomness to break up hot keys:</p> <pre><code>from pyspark.sql.functions import rand, concat, lit\n\n# Add random salt to spread data\nsalted = events.withColumn(\"salted_user\",\n    concat(col(\"user_id\"), lit(\"_\"), (rand() * 10).cast(\"int\"))\n)\n\n# Group by salted key\nuser_stats = salted.groupBy(\"salted_user\").agg(...)\n\n# Remove salt and re-aggregate\nfinal = user_stats.withColumn(\"user_id\",\n    split(col(\"salted_user\"), \"_\")[0]\n).groupBy(\"user_id\").agg(\n    sum(\"event_count\").alias(\"event_count\"),\n    sum(\"total_duration\").alias(\"total_duration\")\n)\n</code></pre> <p>Now the bot's 10 million events are split across 10 partitions instead of overwhelming one.</p> <p>Solution 2: Broadcast Joins for Small Tables</p> <pre><code># BAD: Regular join shuffles both sides\nbig_df.join(small_df, on=\"key\")  # Shuffles 500GB\n\n# GOOD: Broadcast small table to all executors\nfrom pyspark.sql.functions import broadcast\n\nbig_df.join(broadcast(small_df), on=\"key\")  # No shuffle! Small table copied to all nodes\n</code></pre> <p>Use broadcast joins when one side is &lt;100MB.</p> <p>Solution 3: Increase Partition Count</p> <pre><code># More partitions = smaller partitions = less likely to OOM\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"1000\")  # Default is 200\n</code></pre>"},{"location":"chapters/07-spark-pyspark/#my-spark-performance-checklist","title":"My Spark Performance Checklist","text":"<p>Before running any Spark job:</p> <ol> <li>Filter early: Reduce data size ASAP</li> <li>Select only needed columns: Don't read what you won't use</li> <li>Check for skew: Look at partition sizes in Spark UI</li> <li>Avoid unnecessary shuffles: Use broadcast joins when possible</li> <li>Partition appropriately: Not too few (underutilized cluster), not too many (overhead)</li> <li>Cache strategically: <code>.cache()</code> DataFrames you'll reuse</li> </ol>"},{"location":"chapters/07-spark-pyspark/#summary","title":"Summary","text":"<p>Apache Spark distributes computation across many machines to handle data that doesn't fit on one:</p> <ul> <li>Lazy evaluation: Transformations build a plan; actions execute it</li> <li>Data skew: Uneven data distribution kills jobs at 94% - use salting</li> <li>Broadcast joins: For small tables, avoid shuffling by copying to all nodes</li> <li>Partitioning: Balance parallelism (more partitions) vs overhead (too many partitions)</li> </ul> <p>The difference between working Spark code and fast Spark code? Understanding what happens when Spark distributes your data.</p>"},{"location":"chapters/07-spark-pyspark/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>Your Spark job processes 1TB but takes 8 hours. How would you diagnose whether it's data skew, too few partitions, or unnecessary shuffles?</p> </li> <li> <p>When should you use <code>.collect()</code> vs <code>.write()</code>? What's the difference in where data ends up?</p> </li> <li> <p>Think about your largest dataset. Is it big enough to need Spark? Or could Pandas + chunking work?</p> </li> </ol>"},{"location":"chapters/07-spark-pyspark/#next-steps","title":"Next Steps","text":"<p>In the next chapter, we'll explore Kafka and streaming data\u2014because batch processing is great, but sometimes you need results in seconds, not hours.</p>"},{"location":"chapters/08-kafka-streaming/","title":"Chapter 8: Kafka &amp; Streaming Data","text":""},{"location":"chapters/08-kafka-streaming/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this chapter, you will be able to: 1. Explain the difference between batch and stream processing and identify use cases requiring real-time data pipelines 2. Design Kafka topics with appropriate partitioning strategies to ensure message ordering and parallelism 3. Implement consumer groups to scale stream processing across multiple workers with fault tolerance 4. Handle backpressure, message replay, and exactly-once processing semantics in production streaming systems</p>"},{"location":"chapters/08-kafka-streaming/#introduction","title":"Introduction","text":"<p>It's Monday, 10:13 AM. The CEO is presenting our new real-time analytics dashboard to investors. Live metrics. Updates every second. Very impressive.</p> <p>I'm in the back of the conference room, laptop open, monitoring the system. Everything's green. Messages flowing. Consumers processing. Dashboards updating.</p> <p>Then I see it. Consumer lag: 47 messages. Then 124 messages. Then 389 messages. The number keeps climbing.</p> <p>My heart starts racing. The dashboard is showing live data from 3 minutes ago. Then 5 minutes ago. Then 10 minutes old.</p> <p>The CEO is still talking: \"As you can see, we're processing thousands of events per second in real-time...\"</p> <p>The dashboard now shows data from 15 minutes ago. Not real-time. Very much delayed-time.</p> <p>I'm frantically checking logs. The consumer is running. It's processing messages. But it can't keep up. For every message it processes, two more arrive.</p> <p>This is called \"backpressure.\" And I'm experiencing it in the worst possible moment\u2014during an investor demo.</p> <p>Here's what nobody tells you about streaming: It's not enough to process data fast. You have to process it faster than it arrives. Because if you fall behind, you never catch up. The lag just grows. Forever.</p> <p>Welcome to streaming data. Where \"real-time\" is a promise, and falling behind is a disaster.</p>"},{"location":"chapters/08-kafka-streaming/#section-1-why-streaming-the-report-that-couldnt-wait-until-tomorrow","title":"Section 1: Why Streaming? - The Report That Couldn't Wait Until Tomorrow","text":""},{"location":"chapters/08-kafka-streaming/#the-day-batch-processing-wasnt-fast-enough","title":"The Day Batch Processing Wasn't Fast Enough","text":"<p>For two years, our fraud detection system ran nightly. Every night at 2 AM, we'd: 1. Extract the day's transactions 2. Run them through ML models 3. Flag suspicious transactions 4. Email the fraud team a report</p> <p>Worked fine. Until it didn't.</p> <p>One Wednesday, the fraud team lead called me: \"We just lost $47,000 to a fraudulent account. It made 23 transactions yesterday. We didn't find out until this morning's report.\"</p> <p>23 transactions. 14 hours of undetected fraud. By the time we flagged it, the money was gone.</p> <p>\"Can't we detect fraud... faster?\" she asked.</p> <p>\"How fast?\"</p> <p>\"Within seconds. Before the money moves.\"</p> <p>That's when I learned about streaming.</p>"},{"location":"chapters/08-kafka-streaming/#heres-what-i-wish-id-known-some-problems-need-continuous-processing","title":"Here's What I Wish I'd Known: Some Problems Need Continuous Processing","text":"<p>Batch processing: Process data in large chunks on a schedule - ETL runs nightly - Reports generated hourly - Data warehouse refreshes daily</p> <p>Stream processing: Process data as it arrives - Fraud detection in real-time - Live dashboards updating every second - Alerting on anomalies immediately</p> <p>Apache Kafka is a distributed message queue designed for streaming: - Producers publish messages to topics - Consumers read messages and process them - Messages are stored durably (can replay history)</p> <p>Think of it as a pipe between systems that never sleeps.</p>"},{"location":"chapters/08-kafka-streaming/#example-fraud-detection-goes-real-time","title":"Example: Fraud Detection Goes Real-Time","text":"<p>Before (Batch): <pre><code># Runs once per day at 2 AM\ndef daily_fraud_check():\n    # Get yesterday's transactions\n    transactions = get_transactions(yesterday)\n\n    # Check each one\n    for txn in transactions:\n        if is_fraudulent(txn):\n            flag_transaction(txn)\n\n    # Email report\n    send_fraud_report()\n</code></pre></p> <p>Detection latency: Up to 24 hours</p> <p>After (Streaming): <pre><code>from kafka import KafkaConsumer\nimport json\n\n# Consumer reads continuously\nconsumer = KafkaConsumer(\n    'transactions',  # Topic name\n    bootstrap_servers=['kafka1:9092', 'kafka2:9092'],\n    group_id='fraud-detection',\n    value_deserializer=lambda m: json.loads(m.decode('utf-8'))\n)\n\n# Process messages as they arrive\nfor message in consumer:\n    txn = message.value\n\n    # Check immediately\n    if is_fraudulent(txn):\n        # Block the transaction NOW\n        block_transaction(txn)\n        alert_fraud_team(txn)\n\n        # Log for investigation\n        save_to_database(txn)\n</code></pre></p> <p>Detection latency: &lt; 1 second</p> <p>Now fraudulent transactions are blocked before the money moves.</p>"},{"location":"chapters/08-kafka-streaming/#when-to-use-streaming","title":"When to Use Streaming","text":"<p>Use streaming when: - You need real-time or near-real-time results (seconds, not hours) - Late data is useless data (fraud, anomaly detection, live dashboards) - System-to-system communication with decoupling (microservices)</p> <p>Don't use streaming when: - Batch is fast enough (daily reports, monthly aggregations) - You're adding complexity for no reason - Historical reprocessing is more important than speed</p>"},{"location":"chapters/08-kafka-streaming/#section-2-kafka-basics-the-producerconsumer-model","title":"Section 2: Kafka Basics - The Producer/Consumer Model","text":""},{"location":"chapters/08-kafka-streaming/#the-day-i-sent-10-million-messages-to-the-wrong-topic","title":"The Day I Sent 10 Million Messages to the Wrong Topic","text":"<p>My first Kafka integration was supposed to be simple. Listen to <code>user-signups</code> topic, send welcome emails.</p> <p>I wrote the code. Tested it locally with test data. Deployed to production.</p> <p>Tuesday morning, our email provider sends an alert: \"You've hit your daily sending limit (100,000 emails). Account suspended.\"</p> <p>What? We only had 2,000 signups yesterday.</p> <p>I check my consumer. It's running. Processing messages. Sending emails.</p> <p>Then I see the problem. My consumer is reading from <code>user-events</code> instead of <code>user-signups</code>.</p> <p><code>user-events</code> has 10 million messages per day. Every click, every page view, every interaction.</p> <p>My code was trying to send a welcome email for every single event. 10 million welcome emails.</p> <p>Our email provider blocked us. Users who actually signed up didn't get welcome emails. And I had to explain to customer success why we looked like spammers.</p>"},{"location":"chapters/08-kafka-streaming/#heres-what-i-wish-id-known-topics-partitions-and-consumer-groups","title":"Here's What I Wish I'd Known: Topics, Partitions, and Consumer Groups","text":"<p>Kafka's core concepts:</p> <p>Topic: A category of messages (like a database table) - <code>user-signups</code> - <code>transactions</code> - <code>clickstream-events</code></p> <p>Partition: A topic is split into partitions for parallelism - Each partition is an ordered, append-only log - Messages in a partition are ordered (FIFO) - Messages across partitions are NOT ordered</p> <p>Producer: Writes messages to topics <pre><code>from kafka import KafkaProducer\nimport json\n\nproducer = KafkaProducer(\n    bootstrap_servers=['kafka1:9092'],\n    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n)\n\n# Send a message\nproducer.send('user-signups', {\n    'user_id': 12345,\n    'email': 'user@example.com',\n    'timestamp': '2026-01-29T10:15:30Z'\n})\n\nproducer.flush()  # Make sure it's sent\n</code></pre></p> <p>Consumer: Reads messages from topics <pre><code>from kafka import KafkaConsumer\nimport json\n\nconsumer = KafkaConsumer(\n    'user-signups',  # Subscribe to specific topic\n    bootstrap_servers=['kafka1:9092'],\n    group_id='welcome-email-sender',\n    auto_offset_reset='earliest',  # Start from beginning\n    value_deserializer=lambda m: json.loads(m.decode('utf-8'))\n)\n\nfor message in consumer:\n    user = message.value\n    send_welcome_email(user['email'])\n</code></pre></p> <p>Consumer Group: Multiple consumers working together - Each message is processed by only one consumer in the group - Kafka distributes partitions across consumers - Enables parallel processing + fault tolerance</p>"},{"location":"chapters/08-kafka-streaming/#example-scaling-with-consumer-groups","title":"Example: Scaling with Consumer Groups","text":"<p>One consumer (slow): <pre><code>Topic: user-signups (3 partitions)\n  Partition 0: [msg1, msg2, msg3] \u2500\u2510\n  Partition 1: [msg4, msg5, msg6] \u2500\u253c\u2500\u2500&gt; Consumer 1 (processes all)\n  Partition 2: [msg7, msg8, msg9] \u2500\u2518\n</code></pre></p> <p>Consumer 1 processes all 3 partitions. Bottleneck.</p> <p>Three consumers (fast): <pre><code>Topic: user-signups (3 partitions)\n  Partition 0: [msg1, msg2, msg3] \u2500\u2500&gt; Consumer 1\n  Partition 1: [msg4, msg5, msg6] \u2500\u2500&gt; Consumer 2\n  Partition 2: [msg7, msg8, msg9] \u2500\u2500&gt; Consumer 3\n</code></pre></p> <p>Each consumer processes one partition. 3x parallelism.</p> <p>Consumer code (same for all three): <pre><code># Consumer 1, 2, and 3 run the same code\nconsumer = KafkaConsumer(\n    'user-signups',\n    group_id='welcome-email-sender',  # Same group ID\n    # Kafka automatically assigns partitions\n)\n\nfor message in consumer:\n    send_welcome_email(message.value['email'])\n</code></pre></p> <p>Kafka handles partition assignment automatically. Start 3 instances, get 3x throughput.</p>"},{"location":"chapters/08-kafka-streaming/#partitioning-strategies","title":"Partitioning Strategies","text":"<p>When producing, you choose how messages are partitioned:</p> <p>Strategy 1: Key-based (preserves order per key) <pre><code># All messages for same user_id go to same partition\nproducer.send(\n    'user-events',\n    key=str(user_id).encode('utf-8'),  # Partition by user_id\n    value={'event': 'click', 'user_id': user_id}\n)\n</code></pre></p> <p>Benefits: Events for each user are ordered Use case: User activity streams, account transactions</p> <p>Strategy 2: Round-robin (load balancing) <pre><code># Messages distributed evenly across partitions\nproducer.send(\n    'logs',\n    value={'level': 'INFO', 'message': 'Server started'}\n    # No key = round-robin\n)\n</code></pre></p> <p>Benefits: Even distribution, maximum parallelism Use case: Logs, metrics, events where order doesn't matter</p> <p>Strategy 3: Custom partitioner <pre><code>def geo_partitioner(key, all_partitions, available):\n    \"\"\"Route US traffic to partition 0, EU to partition 1\"\"\"\n    if key.startswith(b'US'):\n        return 0\n    elif key.startswith(b'EU'):\n        return 1\n    else:\n        return 2\n\nproducer = KafkaProducer(partitioner=geo_partitioner)\n</code></pre></p> <p>Benefits: Control over data locality Use case: Geo-distributed processing, compliance requirements</p>"},{"location":"chapters/08-kafka-streaming/#scars-ive-earned-kafka-basics-edition","title":"Scars I've Earned: Kafka Basics Edition","text":"<p>Scar #1: Subscribed to wrong topic <pre><code># DON'T\nconsumer = KafkaConsumer('user-events')  # Has 10M msgs/day\n# Meant to subscribe to 'user-signups' (2K msgs/day)\n</code></pre> What it cost me: Email provider blocked us, missed real signups, angry customer success team</p> <p>Scar #2: Forgot to commit offsets <pre><code># DON'T\nconsumer = KafkaConsumer('orders', enable_auto_commit=False)\nfor msg in consumer:\n    process(msg)\n    # Never called consumer.commit() - re-processes same messages forever\n</code></pre> What it cost me: Processed same 100K messages in an infinite loop, duplicate emails, confused customers</p> <p>Scar #3: Used one partition for everything <pre><code># DON'T\n# Topic has 1 partition, can only use 1 consumer\n# No parallelism possible\n</code></pre> What it cost me: Consumer lag grew to millions, couldn't scale, had to recreate topic with more partitions</p>"},{"location":"chapters/08-kafka-streaming/#section-3-backpressure-reliability-when-streams-overflow","title":"Section 3: Backpressure &amp; Reliability - When Streams Overflow","text":""},{"location":"chapters/08-kafka-streaming/#the-consumer-that-fell-behind-and-never-caught-up","title":"The Consumer That Fell Behind (And Never Caught Up)","text":"<p>Remember that investor demo disaster? Let me tell you what went wrong.</p> <p>Our clickstream events: - Incoming rate: 2,000 messages/second - Consumer processing time: 600ms per message - Consumer capacity: 1,000/600 = 1.67 messages/second</p> <p>Math doesn't lie: 2,000 msg/sec incoming, 1.67 msg/sec outgoing.</p> <p>The consumer fell behind immediately. Lag grew by 1,998 messages per second. After 15 minutes: 1.8 million messages behind.</p> <p>By the time I realized, the \"real-time\" dashboard was showing data from 6 hours ago.</p>"},{"location":"chapters/08-kafka-streaming/#heres-what-i-wish-id-known-you-must-handle-backpressure","title":"Here's What I Wish I'd Known: You Must Handle Backpressure","text":"<p>Backpressure: When data arrives faster than you can process it.</p> <p>Causes: - Slow downstream systems (database writes, API calls) - Insufficient consumer instances - Inefficient processing logic - Traffic spikes</p> <p>Solution 1: Scale Consumers Horizontally</p> <pre><code># Before: 1 consumer, 1.67 msg/sec\n# Topic has 10 partitions, but only 1 consumer\n\n# After: 10 consumers (one per partition)\n# 10 consumers * 1.67 msg/sec = 16.7 msg/sec capacity\n</code></pre> <p>Deploy more consumer instances (up to number of partitions).</p> <p>Solution 2: Batch Processing</p> <pre><code># Before: Process one at a time\nfor message in consumer:\n    write_to_database(message.value)  # 600ms per message\n\n# After: Batch writes\nbatch = []\nfor message in consumer:\n    batch.append(message.value)\n\n    if len(batch) &gt;= 100:\n        write_to_database_batch(batch)  # 2 seconds for 100 = 20ms per message\n        batch = []\n        consumer.commit()  # Commit after successful batch\n</code></pre> <p>Throughput improvement: 1.67 \u2192 50 messages/second</p> <p>Solution 3: Optimize Processing</p> <pre><code># Before: Synchronous API call per message\nfor message in consumer:\n    enrich_with_api(message.value)  # 300ms API call\n\n# After: Async batch API calls\nimport asyncio\n\nasync def process_batch(messages):\n    tasks = [enrich_with_api_async(msg.value) for msg in messages]\n    await asyncio.gather(*tasks)  # Parallel API calls\n\nbatch = []\nfor message in consumer:\n    batch.append(message)\n    if len(batch) &gt;= 50:\n        asyncio.run(process_batch(batch))\n        batch = []\n</code></pre> <p>Throughput improvement: 3.3 \u2192 50+ messages/second</p>"},{"location":"chapters/08-kafka-streaming/#monitoring-consumer-lag","title":"Monitoring Consumer Lag","text":"<pre><code>from kafka import KafkaConsumer\n\nconsumer = KafkaConsumer(...)\n\n# Check lag periodically\ndef check_lag():\n    partitions = consumer.assignment()\n    lag_info = {}\n\n    for partition in partitions:\n        # Latest offset in topic\n        end_offset = consumer.end_offsets([partition])[partition]\n\n        # Current consumer position\n        current_offset = consumer.position(partition)\n\n        # Lag = how far behind\n        lag = end_offset - current_offset\n        lag_info[partition] = lag\n\n    total_lag = sum(lag_info.values())\n\n    if total_lag &gt; 10000:  # Alert if &gt;10K messages behind\n        alert(f\"Consumer lag: {total_lag} messages behind\")\n\n    return lag_info\n</code></pre> <p>Set up alerts when lag exceeds thresholds.</p>"},{"location":"chapters/08-kafka-streaming/#exactly-once-processing","title":"Exactly-Once Processing","text":"<p>At-most-once: Message might be lost (fast, risky) <pre><code>consumer = KafkaConsumer(enable_auto_commit=True)\nfor msg in consumer:\n    try:\n        process(msg)\n    except:\n        pass  # Message is already committed, lost if processing fails\n</code></pre></p> <p>At-least-once: Message might be processed twice (safe, possible duplicates) <pre><code>consumer = KafkaConsumer(enable_auto_commit=False)\nfor msg in consumer:\n    process(msg)\n    consumer.commit()  # Commit after processing\n    # If processing succeeds but commit fails, reprocess on restart\n</code></pre></p> <p>Exactly-once: Message processed exactly once (ideal, complex) <pre><code># Requires idempotent processing + transactional producers\n# Use Kafka transactions + idempotency keys in database\nconsumer = KafkaConsumer(\n    isolation_level='read_committed',\n    enable_auto_commit=False\n)\n\nfor msg in consumer:\n    # Check if already processed (idempotency key)\n    if not already_processed(msg.key):\n        process(msg)\n        mark_as_processed(msg.key)\n        consumer.commit()\n</code></pre></p> <p>Most systems use at-least-once + idempotent processing (same result if run twice).</p>"},{"location":"chapters/08-kafka-streaming/#scars-ive-earned-streaming-reliability-edition","title":"Scars I've Earned: Streaming Reliability Edition","text":"<p>Scar #1: Ignored consumer lag <pre><code># DON'T\n# Never checked lag, fell 6 hours behind during investor demo\n</code></pre> What it cost me: Embarrassing demo, \"real-time\" dashboard showing old data, lost investor confidence</p> <p>Scar #2: No backpressure handling <pre><code># DON'T\n# Incoming: 2K msg/sec, Processing: 1.67 msg/sec\n# Never catches up, lag grows forever\n</code></pre> What it cost me: Permanent backlog, had to reset offsets and lose data</p> <p>Scar #3: Committed before processing <pre><code># DON'T\nconsumer.commit()  # Commit first\nprocess(msg)  # Process second - if this fails, message is lost\n</code></pre> What it cost me: Lost messages when processing failed, data gaps in analytics</p>"},{"location":"chapters/08-kafka-streaming/#summary","title":"Summary","text":"<p>Streaming with Kafka enables real-time data processing:</p> <ul> <li>Topics &amp; Partitions: Organize messages, enable parallelism</li> <li>Consumer Groups: Scale processing across multiple workers</li> <li>Backpressure: Handle incoming rate &gt; processing rate via scaling, batching, optimization</li> <li>Reliability: Choose semantics (at-least-once, exactly-once) based on requirements</li> </ul> <p>The difference between working streaming and production streaming? Handling what happens when you fall behind.</p>"},{"location":"chapters/08-kafka-streaming/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>Your consumer is processing 100 messages/second, but receiving 500/second. What are three ways you could solve this?</p> </li> <li> <p>When would you choose batch processing over streaming? When is streaming worth the complexity?</p> </li> <li> <p>If messages must be processed in order, how should you partition your topic? What trade-offs does this create?</p> </li> </ol>"},{"location":"chapters/08-kafka-streaming/#next-steps","title":"Next Steps","text":"<p>Congratulations! You've completed the first 8 weeks of the Data Engineering Bootcamp. You now understand: - Python &amp; SQL optimization - Git &amp; Docker workflows - Database design &amp; warehousing - ETL/ELT orchestration with Airflow - Data quality &amp; error handling - Distributed computing with Spark - Real-time streaming with Kafka</p> <p>Coming in Weeks 9-12: - Cloud platforms (GCP, AWS) - Infrastructure as Code with Terraform - CI/CD for data pipelines - Capstone project: Build an end-to-end data platform</p> <p>Keep building. Keep learning. Keep avoiding 3 AM disasters.</p>"},{"location":"chapters/09-cloud-gcp/","title":"Chapter 9: Cloud Data Engineering with GCP","text":""},{"location":"chapters/09-cloud-gcp/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this chapter, you will be able to: 1. Design cloud-native data architectures using GCP services (BigQuery, Dataflow, Composer, Pub/Sub) that scale automatically 2. Optimize cloud data costs by choosing appropriate storage classes, partitioning strategies, and query patterns 3. Implement serverless data pipelines with Cloud Functions and Dataflow that don't require infrastructure management 4. Evaluate trade-offs between managed services and self-hosted solutions in cloud environments</p>"},{"location":"chapters/09-cloud-gcp/#introduction","title":"Introduction","text":"<p>It's Thursday, 4:47 PM. I'm about to leave for the day when Slack lights up. The CFO: \"Why is our GCP bill $47,000 this month? It was $4,000 last month.\"</p> <p>My stomach drops. I open the GCP console. Navigate to Billing. Filter by service.</p> <p>BigQuery: $43,000.</p> <p>What? How? We barely use BigQuery. We run like... a few queries a day.</p> <p>I dig into the query logs. There it is. One query. Run 4,700 times. Each scan: 2.8TB.</p> <p>Total scanned: 13 petabytes.</p> <p>At $5 per TB, that's... yeah. $43,000.</p> <p>The query? A Looker dashboard someone created. Auto-refreshes every 5 minutes. No caching. No partitioning. Full table scans. Every. Five. Minutes.</p> <p>For a month.</p> <p>That dashboard cost us more than my annual salary.</p> <p>Here's what nobody tells you about the cloud: It's not expensive because of what you intentionally use. It's expensive because of what you accidentally leave running. A forgotten query. An unoptimized table. A debugging script you deployed on Friday and forgot about.</p> <p>Welcome to cloud data engineering. Where everything scales beautifully, costs are magical, and one misconfigured dashboard can fund someone's car payment.</p>"},{"location":"chapters/09-cloud-gcp/#section-1-bigquery-the-warehouse-that-billed-me-43k","title":"Section 1: BigQuery - The Warehouse That Billed Me $43K","text":""},{"location":"chapters/09-cloud-gcp/#the-dashboard-that-refreshed-its-way-into-our-budget","title":"The Dashboard That Refreshed Its Way Into Our Budget","text":"<p>Let me tell you the full story of that $43K disaster.</p> <p>We'd just migrated from PostgreSQL to BigQuery. The data science team was excited\u2014finally, they could query terabytes without waiting hours. I was excited\u2014no more \"database is slow\" tickets.</p> <p>Someone from the BI team created a Looker dashboard: \"Daily Active Users by Region.\" Simple query:</p> <pre><code>SELECT\n    DATE(event_timestamp) AS date,\n    country,\n    COUNT(DISTINCT user_id) AS active_users\nFROM `project.dataset.events`\nWHERE event_type = 'page_view'\nGROUP BY date, country\nORDER BY date DESC\n</code></pre> <p>Looked fine. Ran in 12 seconds on our 8.9TB events table. Dashboard looked great.</p> <p>Then they set it to auto-refresh. Every 5 minutes. Because \"we want real-time insights.\"</p> <p>For 30 days straight, that query ran 8,640 times (24 hours \u00d7 12 queries/hour \u00d7 30 days). Each query scanned 2.8TB. No cache (Looker bypassed BigQuery's cache). No partitioning (scanned entire history).</p> <p>Total: 24,192 TB scanned. At $5/TB: $120,960.</p> <p>Wait, I said $43K earlier. What happened to the other $77K?</p> <p>BigQuery has a free tier: 1TB/month. We used 13PB. So we paid for 12.999 PB.</p> <p>Actually, after the first week when I got the first bill ($11K), I panicked and set a budget alert. The CFO saw it and asked me to \"look into it.\" That's when I found the dashboard and killed it.</p> <p>But the damage was done. $43K in three weeks.</p>"},{"location":"chapters/09-cloud-gcp/#heres-what-i-wish-id-known-bigquery-costs-are-per-query-not-per-storage","title":"Here's What I Wish I'd Known: BigQuery Costs Are Per-Query, Not Per-Storage","text":"<p>Traditional databases: You pay for servers/storage. Query all you want.</p> <p>BigQuery: You pay per query based on how much data you scan.</p> <p>Pricing (simplified): - Storage: $0.02/GB/month (first 10GB free) - Queries: $5 per TB scanned - Streaming inserts: $0.01 per 200MB</p> <p>That \"simple\" query scanning 2.8TB? $14 per execution.</p> <p>Run it 8,640 times? $120,960.</p>"},{"location":"chapters/09-cloud-gcp/#how-to-not-bankrupt-your-company","title":"How to Not Bankrupt Your Company","text":""},{"location":"chapters/09-cloud-gcp/#strategy-1-partition-your-tables","title":"Strategy 1: Partition Your Tables","text":"<pre><code>-- BAD: No partitioning\nCREATE TABLE events (\n    event_timestamp TIMESTAMP,\n    user_id STRING,\n    event_type STRING,\n    country STRING\n);\n\n-- Query scans ALL 8.9TB every time\nSELECT COUNT(DISTINCT user_id)\nFROM events\nWHERE DATE(event_timestamp) = '2026-01-29';\n-- Cost: $44.50 (scanned 8.9TB)\n</code></pre> <pre><code>-- GOOD: Partition by date\nCREATE TABLE events (\n    event_timestamp TIMESTAMP,\n    user_id STRING,\n    event_type STRING,\n    country STRING\n)\nPARTITION BY DATE(event_timestamp);\n\n-- Query scans only today's partition (2.9GB)\nSELECT COUNT(DISTINCT user_id)\nFROM events\nWHERE DATE(event_timestamp) = '2026-01-29';\n-- Cost: $0.01 (scanned 2.9GB)\n</code></pre> <p>Savings: $44.50 \u2192 $0.01 per query. 4,450x cheaper.</p>"},{"location":"chapters/09-cloud-gcp/#strategy-2-cluster-your-tables","title":"Strategy 2: Cluster Your Tables","text":"<pre><code>-- Partition by date, cluster by country\nCREATE TABLE events (\n    event_timestamp TIMESTAMP,\n    user_id STRING,\n    event_type STRING,\n    country STRING\n)\nPARTITION BY DATE(event_timestamp)\nCLUSTER BY country;\n\n-- Query scans only US data from today (400MB)\nSELECT COUNT(DISTINCT user_id)\nFROM events\nWHERE DATE(event_timestamp) = '2026-01-29'\n    AND country = 'US';\n-- Cost: $0.002 (scanned 400MB)\n</code></pre> <p>Partitioning = skip entire days. Clustering = skip irrelevant data within a day.</p>"},{"location":"chapters/09-cloud-gcp/#strategy-3-use-materialized-views","title":"Strategy 3: Use Materialized Views","text":"<pre><code>-- Original expensive query\nSELECT\n    DATE(event_timestamp) AS date,\n    country,\n    COUNT(DISTINCT user_id) AS active_users\nFROM events\nGROUP BY date, country;\n-- Scans 8.9TB, costs $44.50\n\n-- Create materialized view (pre-aggregated)\nCREATE MATERIALIZED VIEW daily_active_users AS\nSELECT\n    DATE(event_timestamp) AS date,\n    country,\n    COUNT(DISTINCT user_id) AS active_users\nFROM events\nGROUP BY date, country;\n\n-- Query the view instead\nSELECT * FROM daily_active_users\nWHERE date = '2026-01-29';\n-- Scans 12KB, costs $0.00006\n</code></pre> <p>BigQuery maintains the materialized view automatically. Queries are instant and nearly free.</p>"},{"location":"chapters/09-cloud-gcp/#strategy-4-enable-query-caching","title":"Strategy 4: Enable Query Caching","text":"<pre><code>-- BigQuery caches results for 24 hours\n-- Same query within 24h? Free! (0 bytes scanned)\n\n-- First run: Scans 2.8TB, costs $14\nSELECT COUNT(*) FROM events WHERE date = '2026-01-29';\n\n-- Second run (within 24h): Scans 0 bytes, costs $0\nSELECT COUNT(*) FROM events WHERE date = '2026-01-29';\n</code></pre> <p>But: Cache is bypassed if: - Query has <code>CURRENT_TIMESTAMP()</code> or random functions - Table changed since last query - Using BI tools that add unique identifiers to queries</p> <p>That Looker dashboard bypassed cache every time.</p>"},{"location":"chapters/09-cloud-gcp/#strategy-5-set-cost-controls","title":"Strategy 5: Set Cost Controls","text":"<pre><code>-- Maximum bytes billed per query\n-- Prevents runaway queries\nbq query --maximum_bytes_billed=1000000000000 \"SELECT ...\"  -- Max 1TB\n\n-- Budget alerts in GCP console\n-- Email when spend exceeds threshold\n</code></pre> <p>I now set: - Budget alerts at $1K, $5K, $10K - Maximum bytes billed: 100GB per query - Required approval for queries &gt; 1TB</p>"},{"location":"chapters/09-cloud-gcp/#my-43k-lessons","title":"My $43K Lessons","text":"<ol> <li>Partition everything - It's free, saves 100x-10,000x in costs</li> <li>Watch auto-refresh dashboards - Cache aggressively or die expensively</li> <li>Set budget alerts - Get notified at $1K, not $43K</li> <li>Preview query costs - BigQuery shows \"This will scan 2.8TB\" before running</li> <li>Use materialized views - Pre-aggregate hot queries</li> </ol>"},{"location":"chapters/09-cloud-gcp/#scars-ive-earned-bigquery-edition","title":"Scars I've Earned: BigQuery Edition","text":"<p>Scar #1: No partitioning on 8.9TB table <pre><code>-- DON'T\nCREATE TABLE events (...);  -- No PARTITION BY\n-- Every query scans everything\n</code></pre> What it cost me: $43K in three weeks, very awkward CFO meeting</p> <p>Scar #2: Used <code>SELECT *</code> on wide tables <pre><code>-- DON'T\nSELECT * FROM events;  -- 147 columns, scans 8.9TB\n-- Only needed 3 columns, could've scanned 180GB\n</code></pre> What it cost me: $44 queries that should've cost $0.90</p> <p>Scar #3: No cost controls <pre><code>-- DON'T\n# Just run queries, hope for the best\n# No budget alerts, no byte limits\n</code></pre> What it cost me: Didn't notice $11K week until bill arrived</p>"},{"location":"chapters/09-cloud-gcp/#section-2-cloud-composer-managed-airflow-when-serverless-isnt-free","title":"Section 2: Cloud Composer (Managed Airflow) - When Serverless Isn't Free","text":""},{"location":"chapters/09-cloud-gcp/#the-airflow-instance-that-cost-more-than-my-salary","title":"The Airflow Instance That Cost More Than My Salary","text":"<p>After the BigQuery disaster, I was cautious about cloud costs. So when we needed to run Airflow, I did the math.</p> <p>Option 1: Self-hosted on Compute Engine - n1-standard-4 (4 vCPUs, 15GB RAM): $140/month - Setup time: ~8 hours - Maintenance: Me, manually</p> <p>Option 2: Cloud Composer (managed Airflow) - Small environment: $300/month - Setup time: ~15 minutes - Maintenance: Google</p> <p>I chose Cloud Composer. \"It's only $160/month more for zero maintenance,\" I told my manager. \"Worth it.\"</p> <p>Three months later, our Cloud Composer bill: $2,700/month.</p> <p>What? How? The docs said $300/month!</p> <p>Turns out: - Base cost: $300/month - Additional workers (autoscaled to 8): $1,120/month - Cloud SQL for metadata: $180/month - GCS for logs: $90/month - Networking egress: $410/month - Redis for Celery: $180/month - Monitoring &amp; logging: $420/month</p> <p>Total: $2,700/month. 9x the advertised price.</p> <p>For comparison, I could've run 19 self-hosted Airflow instances for that price.</p>"},{"location":"chapters/09-cloud-gcp/#heres-what-i-wish-id-known-managed-means-more-expensive","title":"Here's What I Wish I'd Known: \"Managed\" Means \"More Expensive\"","text":"<p>Managed services are convenient. They're also expensive. You pay for: - The service itself - Underlying infrastructure - Network traffic - Storage - Logs - Monitoring - Every little add-on</p> <p>Cloud Composer pricing breakdown:</p> <pre><code>Base environment: $300/month\n+ n1-standard-4 worker nodes \u00d7 3: $420/month\n+ Autoscaling to 8 workers during peak: $1,120/month\n+ Cloud SQL (db-n1-standard-1): $180/month\n+ GCS logs (100GB/month): $2/month + egress $90/month\n+ Redis (M1): $180/month\n+ Stackdriver logging: $420/month\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nTotal: $2,712/month\n</code></pre> <p>For a service I thought was \"$300/month.\"</p>"},{"location":"chapters/09-cloud-gcp/#when-to-use-managed-services","title":"When to Use Managed Services","text":"<p>Use Cloud Composer when: - You don't have ops expertise - Maintenance time costs more than $2K/month - Autoscaling is critical - You need high availability (99.9% SLA)</p> <p>Use self-hosted Airflow when: - You have ops skills - Budget is tight - Workloads are predictable (don't need autoscaling) - You're okay with manual upgrades</p> <p>I eventually migrated back to self-hosted. Saved $2,400/month. Worth the 2 hours/month of maintenance.</p>"},{"location":"chapters/09-cloud-gcp/#section-3-dataflow-pubsub-streaming-at-cloud-scale","title":"Section 3: Dataflow &amp; Pub/Sub - Streaming at Cloud Scale","text":""},{"location":"chapters/09-cloud-gcp/#the-pubsub-topic-that-processed-nothing-but-cost-400month","title":"The Pub/Sub Topic That Processed Nothing (But Cost $400/Month)","text":"<p>We built a real-time analytics pipeline: 1. Web events \u2192 Pub/Sub topic 2. Dataflow job \u2192 processes events 3. BigQuery \u2192 stores results</p> <p>Deployed on a Friday (I know, I know). Worked perfectly. Processed 10M events over the weekend.</p> <p>Monday morning, I check costs. Pub/Sub: $400 for the weekend.</p> <p>What? Pub/Sub is cheap! It's like $0.40 per million messages!</p> <p>I check the metrics. Messages published: 10 million. Messages delivered: 47 million.</p> <p>How did we deliver 4.7x more messages than we published?</p> <p>Dead letter queue infinite retry loop.</p> <p>Our Dataflow job failed to process 3% of messages (malformed JSON). Pub/Sub retried those failed messages. Dataflow failed them again. Pub/Sub retried again. Forever.</p> <p>For 72 hours, Pub/Sub retried 300K messages over and over. Total deliveries: 37 million retries.</p> <p>At $0.40 per million, that's $14.80 for retries. Where'd the other $385 come from?</p> <p>Egress costs. Our Dataflow job ran in <code>us-central1</code>. Our Pub/Sub topic was in <code>us-east1</code>. Cross-region egress: $0.01/GB.</p> <p>10M messages \u00d7 5KB each = 50GB. Cross-region delivery: 47M \u00d7 5KB = 235GB. At $0.01/GB: $2.35.</p> <p>But Pub/Sub also stores messages. 72 hours of retention: $0.05/GB/day. 235GB \u00d7 3 days: $35.25.</p> <p>Plus snapshot storage, plus monitoring, plus logs...</p> <p>Total: $397.42.</p> <p>For messages that never successfully processed.</p>"},{"location":"chapters/09-cloud-gcp/#heres-what-i-wish-id-known-cloud-costs-are-hidden-everywhere","title":"Here's What I Wish I'd Known: Cloud Costs Are Hidden Everywhere","text":"<p>Pub/Sub pricing (per million messages): - Publish: $0.40 - Deliver: $0.40 - Storage: $0.27/GB/month - Snapshots: $0.27/GB/month - Egress (cross-region): $10/GB</p> <p>Hidden multipliers: - Failed message retries (can be 10x-100x) - Cross-region traffic - Message retention - Snapshots and backups</p>"},{"location":"chapters/09-cloud-gcp/#solution-dead-letter-queues-for-real-this-time","title":"Solution: Dead Letter Queues (For Real This Time)","text":"<pre><code>from google.cloud import pubsub_v1\n\nsubscriber = pubsub_v1.SubscriberClient()\n\n# Create subscription with dead letter queue\nsubscription_path = subscriber.subscription_path(project_id, subscription_name)\ndead_letter_topic = subscriber.topic_path(project_id, 'failed-events-dlq')\n\nsubscription = subscriber.create_subscription(\n    request={\n        \"name\": subscription_path,\n        \"topic\": topic_path,\n        \"dead_letter_policy\": {\n            \"dead_letter_topic\": dead_letter_topic,\n            \"max_delivery_attempts\": 5  # Retry 5 times, then DLQ\n        }\n    }\n)\n</code></pre> <p>Now failed messages go to DLQ after 5 attempts. No infinite retries. No $400 bills.</p>"},{"location":"chapters/09-cloud-gcp/#dataflow-cost-optimization","title":"Dataflow Cost Optimization","text":"<pre><code># BAD: Auto-scaling with no limits\npipeline_options = {\n    'runner': 'DataflowRunner',\n    'autoscaling_algorithm': 'THROUGHPUT_BASED',\n    # Dataflow will scale to 100s of workers if needed\n}\n\n# GOOD: Constrained auto-scaling\npipeline_options = {\n    'runner': 'DataflowRunner',\n    'autoscaling_algorithm': 'THROUGHPUT_BASED',\n    'max_num_workers': 10,  # Cap at 10 workers\n    'num_workers': 2,  # Start with 2\n}\n</code></pre> <p>Without constraints, Dataflow scaled to 87 workers during a traffic spike. Cost: $340 for 2 hours.</p> <p>With constraints, max 10 workers. Cost: $40 for 2 hours.</p>"},{"location":"chapters/09-cloud-gcp/#summary","title":"Summary","text":"<p>Cloud data engineering requires cost awareness:</p> <ul> <li>BigQuery: Partition and cluster tables, use materialized views, watch auto-refresh dashboards</li> <li>Cloud Composer: Managed services cost 5x-10x more than self-hosted, choose wisely</li> <li>Pub/Sub: Set max delivery attempts, avoid cross-region traffic, monitor retry loops</li> <li>Dataflow: Cap auto-scaling, use batch when real-time isn't needed</li> </ul> <p>The cloud makes scaling easy. It also makes overspending easy. Vigilance is the price of serverless.</p>"},{"location":"chapters/09-cloud-gcp/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>Your BigQuery table is 5TB. A daily query scans all of it. How much does that cost per month (30 queries)? How would you reduce it to under $10/month?</p> </li> <li> <p>When is a managed service worth the 5x-10x cost premium over self-hosted?</p> </li> <li> <p>How would you design a budget alert system for your cloud data platform?</p> </li> </ol>"},{"location":"chapters/09-cloud-gcp/#next-steps","title":"Next Steps","text":"<p>Next chapter: Infrastructure as Code &amp; CI/CD\u2014because clicking in the console is fine for learning, but terrifying for production.</p>"},{"location":"chapters/10-iac-cicd/","title":"Chapter 10: Infrastructure as Code &amp; CI/CD","text":""},{"location":"chapters/10-iac-cicd/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this chapter, you will be able to: 1. Implement Infrastructure as Code using Terraform to provision cloud resources in a reproducible, version-controlled manner 2. Design CI/CD pipelines for data infrastructure that automatically test, validate, and deploy changes 3. Create automated testing strategies for data pipelines including schema validation, data quality checks, and integration tests 4. Apply GitOps principles to manage data infrastructure and pipeline deployments through pull requests and code review</p>"},{"location":"chapters/10-iac-cicd/#introduction","title":"Introduction","text":"<p>It's Monday, 9:15 AM. New team member's first day. I'm giving him access to our data platform.</p> <p>\"Okay, so to set up the development environment, first go to the GCP console...\"</p> <p>I open my browser. Navigate to Cloud Composer. Click through 14 menus to find the right settings. Screenshot them. Send to him.</p> <p>\"Now create a Cloud SQL instance. Make sure you use PostgreSQL 13, not 14. Machine type should be db-n1-standard-2. Storage is 100GB. Backups daily at 3 AM. High availability enabled...\"</p> <p>I'm literally reading from a 47-point checklist I maintain in a Google Doc. Every time GCP changes their UI, I update the screenshots. I've updated it 23 times this year.</p> <p>\"How long does this take?\" he asks.</p> <p>\"Oh, about 4-5 hours if you don't make mistakes. 2 days if you do.\"</p> <p>He looks horrified.</p> <p>My manager walks by. \"Why are you doing that manually? Don't we have Terraform?\"</p> <p>\"We have what?\"</p> <p>\"Terraform. Infrastructure as Code. You define infrastructure in config files, run one command, everything gets created. Takes 10 minutes.\"</p> <p>\"That... exists?\"</p> <p>I'd spent the last year clicking through GCP console creating resources one by one. Manually. Like a caveman with a mouse.</p> <p>That afternoon, I learned about Infrastructure as Code. By Friday, I'd automated our entire infrastructure setup. From 2 days of clicking to 10 minutes of running <code>terraform apply</code>.</p> <p>Here's what nobody tells you: If you're clicking in the cloud console, you're doing it wrong.</p>"},{"location":"chapters/10-iac-cicd/#section-1-terraform-the-day-i-stopped-clicking","title":"Section 1: Terraform - The Day I Stopped Clicking","text":""},{"location":"chapters/10-iac-cicd/#the-production-resource-i-accidentally-deleted-because-i-couldnt-remember-what-i-clicked","title":"The Production Resource I Accidentally Deleted (Because I Couldn't Remember What I Clicked)","text":"<p>Let me tell you about the worst Tuesday of my career.</p> <p>We had a production BigQuery dataset. Critical data. 47 tables. Complex IAM permissions. The BI team relied on it for all their dashboards.</p> <p>Something broke. I couldn't remember how I'd set up the permissions. I checked my notes. \"Give analytics team access.\" That's it. That's all I wrote six months ago.</p> <p>What permissions? Editor? Viewer? Custom role? Which service accounts? I had no idea.</p> <p>So I did what any desperate engineer does: I tried to recreate the permissions based on what seemed right.</p> <p>I clicked. I granted. I revoked. I tested.</p> <p>\"Did that fix it?\"</p> <p>\"No.\"</p> <p>More clicking. More testing.</p> <p>Then: \"Uh, now I can't see the tables at all.\"</p> <p>Oh no.</p> <p>I'd somehow deleted the permissions that gave me access. To a production dataset. That I was the admin of.</p> <p>I spent the next 6 hours: 1. Begging GCP support to restore my access (3 hours) 2. Trying to remember what permissions existed before (impossible) 3. Recreating everything from scratch (2 hours) 4. Testing with angry BI team members (1 hour)</p> <p>The worst part? I still wasn't sure I'd recreated it correctly. I just kept clicking until people stopped complaining.</p> <p>If I'd had Infrastructure as Code, I could've just run <code>terraform apply</code> and restored everything in 2 minutes.</p>"},{"location":"chapters/10-iac-cicd/#heres-what-i-wish-id-known-code-clicking","title":"Here's What I Wish I'd Known: Code &gt; Clicking","text":"<p>Infrastructure as Code (IaC): Define infrastructure in configuration files instead of clicking in consoles.</p> <p>Benefits: - Reproducible: Run the same code, get the same infrastructure - Version controlled: See history of all changes - Documented: The code is the documentation - Testable: Validate before applying - Collaborative: Code review infrastructure changes</p> <p>Terraform is the most popular IaC tool. It works with AWS, GCP, Azure, and 100+ other providers.</p>"},{"location":"chapters/10-iac-cicd/#example-creating-infrastructure-with-terraform","title":"Example: Creating Infrastructure with Terraform","text":"<p>The old way (clicking): 1. Open GCP Console 2. Navigate to BigQuery \u2192 Create Dataset 3. Enter name: <code>analytics_prod</code> 4. Region: <code>us-central1</code> 5. Default table expiration: 30 days 6. Click \"Create\" 7. Navigate to IAM 8. Click \"Add Member\" 9. Enter email: <code>analytics-team@company.com</code> 10. Role: <code>BigQuery Data Viewer</code> 11. Click \"Save\" 12. Repeat for 6 more service accounts... 13. 45 minutes later, you're done 14. Tomorrow, try to remember what you did</p> <p>The new way (Terraform):</p> <pre><code># main.tf - Define everything in code\n\n# BigQuery dataset\nresource \"google_bigquery_dataset\" \"analytics_prod\" {\n  dataset_id = \"analytics_prod\"\n  location   = \"us-central1\"\n\n  default_table_expiration_ms = 2592000000  # 30 days\n\n  access {\n    role          = \"READER\"\n    user_by_email = \"analytics-team@company.com\"\n  }\n\n  access {\n    role          = \"READER\"\n    user_by_email = \"dashboard-service@company.iam.gserviceaccount.com\"\n  }\n\n  access {\n    role          = \"WRITER\"\n    user_by_email = \"etl-pipeline@company.iam.gserviceaccount.com\"\n  }\n\n  labels = {\n    environment = \"production\"\n    team        = \"data-engineering\"\n  }\n}\n\n# BigQuery table\nresource \"google_bigquery_table\" \"user_events\" {\n  dataset_id = google_bigquery_dataset.analytics_prod.dataset_id\n  table_id   = \"user_events\"\n\n  time_partitioning {\n    type  = \"DAY\"\n    field = \"event_timestamp\"\n  }\n\n  clustering = [\"country\", \"event_type\"]\n\n  schema = file(\"schemas/user_events.json\")\n}\n</code></pre> <p>Run it: <pre><code>terraform init      # Initialize\nterraform plan      # Preview changes\nterraform apply     # Create resources\n</code></pre></p> <p>Result: Everything created in 2 minutes. And now it's in git. Forever documented. Can recreate anytime.</p>"},{"location":"chapters/10-iac-cicd/#terraform-best-practices","title":"Terraform Best Practices","text":""},{"location":"chapters/10-iac-cicd/#pattern-1-organize-by-environment","title":"Pattern 1: Organize by Environment","text":"<pre><code>terraform/\n\u251c\u2500\u2500 environments/\n\u2502   \u251c\u2500\u2500 dev/\n\u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u2514\u2500\u2500 terraform.tfvars  # dev-specific values\n\u2502   \u251c\u2500\u2500 staging/\n\u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u2514\u2500\u2500 terraform.tfvars\n\u2502   \u2514\u2500\u2500 prod/\n\u2502       \u251c\u2500\u2500 main.tf\n\u2502       \u2514\u2500\u2500 terraform.tfvars\n\u2514\u2500\u2500 modules/\n    \u251c\u2500\u2500 bigquery/\n    \u2502   \u251c\u2500\u2500 main.tf\n    \u2502   \u251c\u2500\u2500 variables.tf\n    \u2502   \u2514\u2500\u2500 outputs.tf\n    \u2514\u2500\u2500 composer/\n        \u251c\u2500\u2500 main.tf\n        \u251c\u2500\u2500 variables.tf\n        \u2514\u2500\u2500 outputs.tf\n</code></pre> <p>Same code, different values per environment.</p>"},{"location":"chapters/10-iac-cicd/#pattern-2-use-modules-for-reusability","title":"Pattern 2: Use Modules for Reusability","text":"<pre><code># modules/bigquery_dataset/main.tf\nvariable \"dataset_id\" {}\nvariable \"location\" {}\nvariable \"readers\" { type = list(string) }\n\nresource \"google_bigquery_dataset\" \"dataset\" {\n  dataset_id = var.dataset_id\n  location   = var.location\n\n  dynamic \"access\" {\n    for_each = var.readers\n    content {\n      role          = \"READER\"\n      user_by_email = access.value\n    }\n  }\n}\n\n# Use the module\nmodule \"analytics_dataset\" {\n  source = \"./modules/bigquery_dataset\"\n\n  dataset_id = \"analytics\"\n  location   = \"us-central1\"\n  readers    = [\n    \"analytics-team@company.com\",\n    \"bi-dashboard@company.iam.gserviceaccount.com\"\n  ]\n}\n</code></pre>"},{"location":"chapters/10-iac-cicd/#pattern-3-store-state-remotely","title":"Pattern 3: Store State Remotely","text":"<pre><code># backend.tf - Store state in GCS, not locally\nterraform {\n  backend \"gcs\" {\n    bucket = \"company-terraform-state\"\n    prefix = \"data-platform/prod\"\n  }\n}\n</code></pre> <p>Why: If state is local, only you can run Terraform. Remote state enables team collaboration.</p>"},{"location":"chapters/10-iac-cicd/#scars-ive-earned-infrastructure-as-code-edition","title":"Scars I've Earned: Infrastructure as Code Edition","text":"<p>Scar #1: Manually modified resources Terraform created <pre><code># DON'T\n# Create with Terraform\nterraform apply\n\n# Manually modify in console\n# GCP Console: Change dataset location\n\n# Try to update with Terraform\nterraform apply\n# Terraform wants to recreate! (destroys data)\n</code></pre> What it cost me: Accidentally deleted production dataset, 4-hour restore from backup</p> <p>Scar #2: Didn't use remote state, kept it local <pre><code># DON'T\n# My laptop: terraform.tfstate (local file)\n# Teammate's laptop: different terraform.tfstate\n# Now we have two sources of truth, infrastructure drifts apart\n</code></pre> What it cost me: Two team members created conflicting resources, took a day to untangle</p> <p>Scar #3: Didn't review terraform plan before apply <pre><code># DON'T\nterraform apply -auto-approve  # YOLO mode\n# Whoops, just deleted production database\n</code></pre> What it cost me: Destroyed staging environment that had test data people needed</p>"},{"location":"chapters/10-iac-cicd/#section-2-cicd-for-data-pipelines-the-deploy-that-broke-everything","title":"Section 2: CI/CD for Data Pipelines - The Deploy That Broke Everything","text":""},{"location":"chapters/10-iac-cicd/#the-manual-deployment-that-worked-on-my-laptop","title":"The Manual Deployment That Worked On My Laptop","text":"<p>Friday, 3 PM. I'd just finished a new Airflow DAG. Tested locally. Worked perfectly. Ready to deploy.</p> <p>My deployment process: 1. SSH into Airflow server 2. <code>git pull origin main</code> 3. Restart Airflow scheduler 4. Hope for the best</p> <p>I ran it. Logged out. Went home.</p> <p>Sunday morning, 6 AM. Phone rings. Production Airflow is down. All DAGs stopped.</p> <p>I SSH in. Check logs:</p> <pre><code>ImportError: No module named 'great_expectations'\n</code></pre> <p>What? Great Expectations is definitely installed. I used it in my new DAG.</p> <p>On my laptop.</p> <p>I forgot to add it to <code>requirements.txt</code>. My local environment had it from a different project. Production didn't.</p> <p>Zero DAGs running. Because I deployed untested code directly to production. By SSHing and running git pull.</p>"},{"location":"chapters/10-iac-cicd/#heres-what-i-wish-id-known-deployments-should-be-automated-not-manual","title":"Here's What I Wish I'd Known: Deployments Should Be Automated, Not Manual","text":"<p>CI/CD (Continuous Integration / Continuous Deployment): - CI: Automatically test every code change - CD: Automatically deploy code that passes tests</p> <p>For data pipelines: - Test DAG syntax before deploying - Validate data schemas - Run data quality checks - Deploy only if tests pass</p>"},{"location":"chapters/10-iac-cicd/#example-github-actions-cicd-for-airflow","title":"Example: GitHub Actions CI/CD for Airflow","text":"<pre><code># .github/workflows/deploy-airflow.yml\nname: Deploy Airflow DAGs\n\non:\n  push:\n    branches: [main]\n    paths:\n      - 'dags/**'\n      - 'requirements.txt'\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n\n      - name: Install dependencies\n        run: |\n          pip install -r requirements.txt\n          pip install pytest\n\n      - name: Test DAG syntax\n        run: |\n          python -m pytest tests/test_dag_validation.py\n\n      - name: Test DAG integrity\n        run: |\n          python -c \"from airflow.models import DagBag; \\\n                     dag_bag = DagBag('dags/'); \\\n                     assert len(dag_bag.import_errors) == 0\"\n\n  deploy:\n    needs: test\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main'\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Deploy to Cloud Composer\n        run: |\n          gcloud composer environments storage dags import \\\n            --environment=prod-composer \\\n            --location=us-central1 \\\n            --source=dags/\n\n      - name: Notify Slack\n        run: |\n          curl -X POST ${{ secrets.SLACK_WEBHOOK }} \\\n            -d '{\"text\":\"\u2705 Airflow DAGs deployed to production\"}'\n</code></pre> <p>Now: 1. Push to main 2. Tests run automatically 3. If tests pass, deploy automatically 4. If tests fail, deployment blocked 5. Team gets notified</p> <p>No more SSHing. No more \"oops forgot requirements.txt.\"</p>"},{"location":"chapters/10-iac-cicd/#testing-data-pipelines","title":"Testing Data Pipelines","text":"<pre><code># tests/test_etl_pipeline.py\nimport pytest\nfrom dags.etl_pipeline import transform_user_data\n\ndef test_transform_handles_missing_columns():\n    \"\"\"Pipeline should handle missing optional columns\"\"\"\n    input_data = [\n        {'user_id': 1, 'email': 'user@example.com'}\n        # Missing 'phone' column\n    ]\n\n    result = transform_user_data(input_data)\n\n    assert result[0]['phone'] is None  # Should default to None\n    assert result[0]['email'] == 'user@example.com'\n\ndef test_transform_rejects_invalid_emails():\n    \"\"\"Pipeline should reject invalid emails\"\"\"\n    input_data = [\n        {'user_id': 1, 'email': 'not-an-email'}\n    ]\n\n    with pytest.raises(ValueError, match=\"Invalid email\"):\n        transform_user_data(input_data)\n\ndef test_dag_has_no_cycles():\n    \"\"\"DAG should not have circular dependencies\"\"\"\n    from airflow.models import DagBag\n\n    dag_bag = DagBag('dags/')\n    assert len(dag_bag.import_errors) == 0\n\n    for dag_id, dag in dag_bag.dags.items():\n        # Check for cycles\n        assert len(list(dag.task_dict.values())) &gt; 0\n</code></pre> <p>Run tests before every deploy: <pre><code>pytest tests/\n# All tests pass? Deploy.\n# Any test fails? Block deployment.\n</code></pre></p>"},{"location":"chapters/10-iac-cicd/#scars-ive-earned-cicd-edition","title":"Scars I've Earned: CI/CD Edition","text":"<p>Scar #1: Deployed without testing <pre><code># DON'T\ngit push origin main\nssh prod-server\ngit pull\n# Imported module that doesn't exist in prod\n# Everything breaks\n</code></pre> What it cost me: Sunday morning emergency, all DAGs down for 3 hours</p> <p>Scar #2: No rollback strategy <pre><code># DON'T\n# Deploy new version\n# It breaks\n# No way to quickly revert\n# Spend 2 hours debugging in production\n</code></pre> What it cost me: Production downtime while I fixed bugs that passed \"local testing\"</p> <p>Scar #3: Deployed to production first <pre><code># DON'T\n# Skip staging environment\n# Deploy untested code directly to production\n# \"It worked on my laptop\"\n</code></pre> What it cost me: Broke production data quality checks, bad data reached dashboards</p>"},{"location":"chapters/10-iac-cicd/#summary","title":"Summary","text":"<p>Modern data engineering requires automation:</p> <ul> <li>Infrastructure as Code (Terraform): Define infrastructure in code, version control everything, no more clicking</li> <li>CI/CD pipelines: Automatically test and deploy, block bad code from reaching production</li> <li>Automated testing: Validate DAG syntax, data schemas, transformation logic before deployment</li> <li>GitOps: All changes via pull requests, code review infrastructure updates</li> </ul> <p>The difference between junior and senior engineers? Seniors automate everything they have to do twice.</p>"},{"location":"chapters/10-iac-cicd/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>How much time do you spend manually creating/configuring infrastructure? Could that be automated?</p> </li> <li> <p>Your teammate pushes broken code to production. How would CI/CD have prevented it?</p> </li> <li> <p>What's your rollback strategy if a deployment breaks? Can you revert in 1 minute or 1 hour?</p> </li> </ol>"},{"location":"chapters/10-iac-cicd/#next-steps","title":"Next Steps","text":"<p>Next chapter: Capstone Project\u2014time to build an end-to-end data platform using everything you've learned.</p>"},{"location":"chapters/11-capstone-part1/","title":"Chapter 11: Capstone Project - Design &amp; Architecture","text":""},{"location":"chapters/11-capstone-part1/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this chapter, you will be able to: 1. Design an end-to-end data platform architecture incorporating all concepts from the bootcamp 2. Evaluate technology trade-offs and justify architectural decisions in technical design documents 3. Plan a data engineering project using Agile methodology with appropriate scope for a 2-week implementation 4. Collaborate in teams using GitHub workflows, code reviews, and technical communication</p>"},{"location":"chapters/11-capstone-part1/#introduction","title":"Introduction","text":"<p>Welcome to the final stretch. You've spent 10 weeks learning Python, SQL, Git, Docker, databases, warehousing, ETL, Airflow, data quality, Spark, Kafka, cloud platforms, and infrastructure as code.</p> <p>Now it's time to put it all together.</p> <p>For the next two weeks, you'll build a real data platform from scratch. Not a tutorial. Not a guided exercise. A real system that ingests data, transforms it, stores it, monitors it, and serves it to end users.</p> <p>This is your chance to make every mistake we've talked about\u2014and fix them before you're doing it for a paycheck.</p>"},{"location":"chapters/11-capstone-part1/#the-capstone-project-real-time-e-commerce-analytics-platform","title":"The Capstone Project: Real-Time E-Commerce Analytics Platform","text":""},{"location":"chapters/11-capstone-part1/#project-overview","title":"Project Overview","text":"<p>Build a data platform that processes e-commerce events in real-time and powers business intelligence dashboards.</p> <p>Data Sources: - User clickstream events (Kafka topic, ~1000 events/second) - Product catalog (PostgreSQL database, updated hourly) - Order transactions (REST API, ~50 orders/minute)</p> <p>Requirements: 1. Streaming ingestion: Consume Kafka clickstream events in real-time 2. Batch integration: Load product catalog and orders on schedule 3. Data warehouse: Store clean, modeled data in BigQuery 4. Data quality: Validate all incoming data, track data quality metrics 5. Orchestration: Use Airflow to manage all batch workflows 6. Infrastructure: All infrastructure defined in Terraform 7. CI/CD: Automated testing and deployment via GitHub Actions 8. Monitoring: Dashboards showing pipeline health and data quality</p> <p>Deliverables: - Working data platform (code in GitHub) - Architecture diagram - Technical design document - 15-minute presentation - Live demo</p>"},{"location":"chapters/11-capstone-part1/#success-criteria","title":"Success Criteria","text":"<p>Technical: - \u2705 Ingests all three data sources successfully - \u2705 Processes streaming data with &lt;5 second latency - \u2705 Maintains data quality &gt;99% - \u2705 Infrastructure fully automated (one command to deploy) - \u2705 All tests passing in CI/CD - \u2705 Monitoring shows green health</p> <p>Professional: - \u2705 Code review process followed - \u2705 Git commit messages are clear - \u2705 Documentation explains design decisions - \u2705 Presentation is polished</p>"},{"location":"chapters/11-capstone-part1/#week-1-design-setup","title":"Week 1: Design &amp; Setup","text":""},{"location":"chapters/11-capstone-part1/#day-1-2-architecture-design","title":"Day 1-2: Architecture Design","text":"<p>Task 1: Draw the architecture</p> <p>Create a diagram showing: - Data sources - Ingestion layer (Kafka consumers, API clients) - Processing layer (Dataflow, Spark) - Storage layer (BigQuery, GCS) - Orchestration (Airflow) - Monitoring (Stackdriver, Great Expectations)</p> <p>Tools: draw.io, Lucidchart, or pen and paper</p> <p>Task 2: Technology selection</p> <p>For each component, choose technologies and justify:</p> Component Options Your Choice Justification Streaming ingestion Dataflow, Spark Streaming, Python consumer ? Why? Batch orchestration Airflow, Cloud Composer, Prefect ? Why? Data warehouse BigQuery, Snowflake, Redshift ? Why? Infrastructure Terraform, Pulumi, manual ? Why? <p>Task 3: Write design document</p> <p>Use this template:</p> <pre><code># E-Commerce Analytics Platform Design\n\n## 1. Executive Summary\n[2-3 sentences: What are you building and why?]\n\n## 2. Requirements\n- Streaming: [Details]\n- Batch: [Details]\n- Data quality: [Details]\n- Scale: [Details]\n\n## 3. Architecture\n[Diagram + explanation]\n\n## 4. Technology Choices\n### Streaming\n- **Choice:** [Technology]\n- **Rationale:** [Why this vs alternatives]\n- **Trade-offs:** [What you're giving up]\n\n[Repeat for each component]\n\n## 5. Data Model\n[Schema diagrams for warehouse tables]\n\n## 6. Data Quality\n[What checks, where, how failures are handled]\n\n## 7. Monitoring\n[What metrics, what alerts]\n\n## 8. Risks &amp; Mitigation\n[What could go wrong, how you'll handle it]\n\n## 9. Timeline\n[What gets built when over 2 weeks]\n</code></pre> <p>Deliverable: Design document in <code>docs/design.md</code></p>"},{"location":"chapters/11-capstone-part1/#day-3-4-repository-setup-infrastructure","title":"Day 3-4: Repository Setup &amp; Infrastructure","text":"<p>Task 1: GitHub repository structure</p> <pre><code>ecommerce-data-platform/\n\u251c\u2500\u2500 .github/\n\u2502   \u2514\u2500\u2500 workflows/\n\u2502       \u251c\u2500\u2500 test.yml\n\u2502       \u2514\u2500\u2500 deploy.yml\n\u251c\u2500\u2500 terraform/\n\u2502   \u251c\u2500\u2500 environments/\n\u2502   \u2502   \u251c\u2500\u2500 dev/\n\u2502   \u2502   \u2514\u2500\u2500 prod/\n\u2502   \u2514\u2500\u2500 modules/\n\u251c\u2500\u2500 airflow/\n\u2502   \u251c\u2500\u2500 dags/\n\u2502   \u251c\u2500\u2500 plugins/\n\u2502   \u2514\u2500\u2500 tests/\n\u251c\u2500\u2500 streaming/\n\u2502   \u251c\u2500\u2500 consumers/\n\u2502   \u251c\u2500\u2500 processors/\n\u2502   \u2514\u2500\u2500 tests/\n\u251c\u2500\u2500 sql/\n\u2502   \u251c\u2500\u2500 schemas/\n\u2502   \u2514\u2500\u2500 transformations/\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 integration/\n\u2502   \u2514\u2500\u2500 unit/\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 design.md\n\u2502   \u251c\u2500\u2500 runbook.md\n\u2502   \u2514\u2500\u2500 architecture.png\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 Makefile\n\u2514\u2500\u2500 README.md\n</code></pre> <p>Task 2: Terraform infrastructure</p> <p>Create base infrastructure: - BigQuery datasets (raw, staging, prod) - GCS buckets (data lake, logs) - Cloud Composer environment (if using) - Service accounts with minimal permissions - Pub/Sub topics</p> <pre><code># terraform/environments/dev/main.tf\nmodule \"bigquery\" {\n  source = \"../../modules/bigquery\"\n\n  dataset_id = \"ecommerce_raw\"\n  location   = \"us-central1\"\n}\n\nmodule \"storage\" {\n  source = \"../../modules/storage\"\n\n  bucket_name = \"ecommerce-data-lake-dev\"\n  location    = \"us-central1\"\n}\n</code></pre> <p>Run: <code>terraform apply</code></p> <p>Task 3: CI/CD pipeline</p> <pre><code># .github/workflows/test.yml\nname: Test\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n\n      - name: Install dependencies\n        run: pip install -r requirements.txt\n\n      - name: Lint\n        run: |\n          pip install flake8\n          flake8 airflow/ streaming/ --max-line-length=100\n\n      - name: Unit tests\n        run: pytest tests/unit/\n\n      - name: DAG validation\n        run: pytest tests/test_dags.py\n</code></pre> <p>Deliverable: Infrastructure deployed, CI passing</p>"},{"location":"chapters/11-capstone-part1/#day-5-data-modeling","title":"Day 5: Data Modeling","text":"<p>Task 1: Design warehouse schema</p> <p>Create dimensional model:</p> <pre><code>-- Fact table: user events\nCREATE TABLE `ecommerce_prod.fact_events` (\n    event_id STRING NOT NULL,\n    event_timestamp TIMESTAMP NOT NULL,\n    user_id STRING,\n    session_id STRING,\n    event_type STRING,\n    page_url STRING,\n    product_id STRING,\n    -- Foreign keys\n    date_key INT64,\n    user_key INT64,\n    product_key INT64\n)\nPARTITION BY DATE(event_timestamp)\nCLUSTER BY user_id, event_type;\n\n-- Dimension: users\nCREATE TABLE `ecommerce_prod.dim_users` (\n    user_key INT64 NOT NULL,\n    user_id STRING NOT NULL,\n    email STRING,\n    signup_date DATE,\n    country STRING,\n    -- SCD Type 2\n    valid_from TIMESTAMP,\n    valid_to TIMESTAMP,\n    is_current BOOLEAN\n);\n\n-- Dimension: products\nCREATE TABLE `ecommerce_prod.dim_products` (\n    product_key INT64 NOT NULL,\n    product_id STRING NOT NULL,\n    product_name STRING,\n    category STRING,\n    price FLOAT64,\n    valid_from TIMESTAMP,\n    valid_to TIMESTAMP,\n    is_current BOOLEAN\n);\n</code></pre> <p>Task 2: Define data quality rules</p> <pre><code># tests/data_quality/test_events.py\nimport great_expectations as gx\n\ndef test_fact_events_quality():\n    \"\"\"Validate fact_events table\"\"\"\n    df = read_bigquery(\"SELECT * FROM ecommerce_prod.fact_events\")\n    expectations = gx.from_pandas(df)\n\n    # Required columns not null\n    expectations.expect_column_values_to_not_be_null('event_id')\n    expectations.expect_column_values_to_not_be_null('event_timestamp')\n\n    # Valid event types\n    expectations.expect_column_values_to_be_in_set(\n        'event_type',\n        ['page_view', 'add_to_cart', 'purchase', 'search']\n    )\n\n    # Timestamps are recent (not in future, not too old)\n    expectations.expect_column_values_to_be_between(\n        'event_timestamp',\n        min_value=datetime.now() - timedelta(days=7),\n        max_value=datetime.now() + timedelta(hours=1)\n    )\n</code></pre> <p>Deliverable: Schema DDL in <code>sql/schemas/</code>, data quality tests</p>"},{"location":"chapters/11-capstone-part1/#week-2-implementation-presentation","title":"Week 2: Implementation &amp; Presentation","text":""},{"location":"chapters/11-capstone-part1/#day-6-8-build-core-pipelines","title":"Day 6-8: Build Core Pipelines","text":"<p>Priority order: 1. Streaming ingestion (most critical) 2. Batch ETL for dimensions 3. Data quality monitoring 4. Dashboards</p> <p>Streaming pipeline (Day 6):</p> <pre><code># streaming/consumers/clickstream_consumer.py\nfrom kafka import KafkaConsumer\nfrom google.cloud import bigquery\nimport json\n\ndef process_events():\n    consumer = KafkaConsumer(\n        'clickstream',\n        bootstrap_servers=['kafka:9092'],\n        value_deserializer=lambda m: json.loads(m.decode('utf-8'))\n    )\n\n    client = bigquery.Client()\n    table_id = \"ecommerce_prod.fact_events\"\n    errors = []\n\n    for message in consumer:\n        try:\n            event = message.value\n            validate_event(event)\n            rows_to_insert = [transform_event(event)]\n\n            errors = client.insert_rows_json(table_id, rows_to_insert)\n            if errors:\n                log_to_dlq(event, errors)\n\n        except Exception as e:\n            log_to_dlq(event, str(e))\n\ndef validate_event(event):\n    \"\"\"Raise ValueError if invalid\"\"\"\n    required = ['event_id', 'event_timestamp', 'event_type']\n    for field in required:\n        if field not in event:\n            raise ValueError(f\"Missing required field: {field}\")\n</code></pre> <p>Batch ETL (Day 7):</p> <pre><code># airflow/dags/product_catalog_etl.py\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\n\ndef extract_products():\n    \"\"\"Extract from PostgreSQL source\"\"\"\n    conn = get_postgres_connection()\n    df = pd.read_sql(\"SELECT * FROM products\", conn)\n    df.to_parquet('/tmp/products.parquet')\n\ndef load_to_warehouse():\n    \"\"\"Load to BigQuery with SCD Type 2\"\"\"\n    df = pd.read_parquet('/tmp/products.parquet')\n\n    # Detect changes, update SCD\n    update_dimension_table(\n        df,\n        table_id='ecommerce_prod.dim_products',\n        key_column='product_id',\n        scd_type=2\n    )\n\nwith DAG('product_catalog_etl', schedule_interval='0 * * * *') as dag:\n    extract = PythonOperator(task_id='extract', python_callable=extract_products)\n    load = PythonOperator(task_id='load', python_callable=load_to_warehouse)\n\n    extract &gt;&gt; load\n</code></pre> <p>Deliverable: Pipelines working, data flowing to warehouse</p>"},{"location":"chapters/11-capstone-part1/#day-9-10-testing-monitoring","title":"Day 9-10: Testing &amp; Monitoring","text":"<p>Integration tests:</p> <pre><code># tests/integration/test_end_to_end.py\ndef test_clickstream_to_warehouse():\n    \"\"\"Test event flows from Kafka to BigQuery\"\"\"\n    # Publish test event to Kafka\n    test_event = {\n        'event_id': 'test-123',\n        'event_timestamp': datetime.now().isoformat(),\n        'event_type': 'page_view',\n        'user_id': 'test-user'\n    }\n    publish_to_kafka('clickstream', test_event)\n\n    # Wait for processing\n    time.sleep(10)\n\n    # Verify in BigQuery\n    query = \"\"\"\n        SELECT * FROM ecommerce_prod.fact_events\n        WHERE event_id = 'test-123'\n    \"\"\"\n    results = bigquery_client.query(query).result()\n    assert len(list(results)) == 1\n</code></pre> <p>Monitoring dashboard:</p> <p>Create Looker/Data Studio dashboard showing: - Events processed per minute - Pipeline lag (streaming and batch) - Data quality metrics (% passing validation) - Error rates - Cost per day</p> <p>Deliverable: All tests passing, monitoring live</p>"},{"location":"chapters/11-capstone-part1/#day-11-12-documentation-presentation","title":"Day 11-12: Documentation &amp; Presentation","text":"<p>Runbook (<code>docs/runbook.md</code>):</p> <pre><code># Production Runbook\n\n## Deployment\n```bash\n# Deploy infrastructure\ncd terraform/environments/prod\nterraform apply\n\n# Deploy Airflow DAGs\ngcloud composer environments storage dags import ...\n\n# Start streaming consumer\ndocker-compose up -d clickstream-consumer\n</code></pre>"},{"location":"chapters/11-capstone-part1/#monitoring","title":"Monitoring","text":"<ul> <li>Dashboard: [URL]</li> <li>Alerts: Slack #data-alerts</li> <li>Logs: Stackdriver [URL]</li> </ul>"},{"location":"chapters/11-capstone-part1/#common-issues","title":"Common Issues","text":""},{"location":"chapters/11-capstone-part1/#issue-consumer-lag-growing","title":"Issue: Consumer lag growing","text":"<p>Symptom: Kafka consumer lag &gt; 10,000 Resolution: 1. Check consumer logs 2. Scale consumers horizontally 3. Verify BigQuery isn't rate-limited</p>"},{"location":"chapters/11-capstone-part1/#issue-airflow-dag-failing","title":"Issue: Airflow DAG failing","text":"<p>Symptom: DAG red in Airflow UI Resolution: 1. Check task logs 2. Verify source data availability 3. Check Great Expectations validation results ```</p> <p>Presentation (15 minutes):</p> <ol> <li>Problem (2 min): What business need does this solve?</li> <li>Architecture (3 min): Show diagram, explain data flow</li> <li>Demo (5 min): Live system, show data flowing</li> <li>Challenges (3 min): What went wrong, how you fixed it</li> <li>Learnings (2 min): What you'd do differently next time</li> </ol> <p>Deliverable: Polished presentation, working demo</p>"},{"location":"chapters/11-capstone-part1/#team-collaboration","title":"Team Collaboration","text":""},{"location":"chapters/11-capstone-part1/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Create feature branch: <code>git checkout -b feature/streaming-consumer</code></li> <li>Implement &amp; test locally: Make it work</li> <li>Open PR: Title: \"Add Kafka clickstream consumer\"</li> <li>Code review: At least 1 approval required</li> <li>Address feedback: Make changes if requested</li> <li>Merge: Squash and merge to main</li> <li>CI/CD deploys automatically</li> </ol>"},{"location":"chapters/11-capstone-part1/#code-review-checklist","title":"Code Review Checklist","text":"<p>Reviewer checks: - \u2705 Code is readable and documented - \u2705 Tests are included - \u2705 No secrets in code - \u2705 Error handling present - \u2705 Follows project conventions</p> <p>Author provides: - Description of changes - Testing notes - Screenshots/demo (if applicable)</p>"},{"location":"chapters/11-capstone-part1/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>What's the riskiest part of your architecture? How are you mitigating that risk?</p> </li> <li> <p>If your streaming consumer falls behind during a traffic spike, what happens?</p> </li> <li> <p>How will you know if data quality degrades? What alerts will fire?</p> </li> </ol>"},{"location":"chapters/11-capstone-part1/#next-steps","title":"Next Steps","text":"<p>Next chapter: Implementation best practices, code review process, and presentation prep.</p>"},{"location":"chapters/12-capstone-part2/","title":"Chapter 12: Capstone Project - Implementation &amp; Presentation","text":""},{"location":"chapters/12-capstone-part2/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this chapter, you will be able to: 1. Write production-quality code with proper error handling, logging, and documentation 2. Conduct effective code reviews that improve code quality while maintaining team morale 3. Debug distributed systems using logs, metrics, and traces across multiple services 4. Present technical work to diverse audiences with clear explanations and live demonstrations</p>"},{"location":"chapters/12-capstone-part2/#introduction","title":"Introduction","text":"<p>You've designed your data platform. Infrastructure is provisioned. Now comes the hard part: making it actually work.</p> <p>Not \"work on your laptop.\" Work in production. At scale. With real data. With things failing. With teammates depending on you.</p> <p>This chapter is about the difference between code that runs and code you're proud to show at a job interview.</p>"},{"location":"chapters/12-capstone-part2/#implementation-best-practices","title":"Implementation Best Practices","text":""},{"location":"chapters/12-capstone-part2/#code-quality-standards","title":"Code Quality Standards","text":"<p>Every file should have:</p> <ol> <li>Clear purpose (docstring at top)</li> <li>Error handling (specific exceptions)</li> <li>Logging (info, warning, error levels)</li> <li>Tests (unit tests minimum)</li> <li>Type hints (Python 3.10+)</li> </ol> <p>Example: Production-quality consumer</p> <pre><code>\"\"\"\nKafka clickstream consumer that ingests user events to BigQuery.\n\nHandles:\n- Message validation and schema enforcement\n- Dead letter queue for failed messages\n- Graceful shutdown on termination signals\n- Backpressure when BigQuery is slow\n\"\"\"\n\nimport logging\nimport signal\nimport sys\nfrom typing import Dict, List, Optional\nfrom kafka import KafkaConsumer\nfrom google.cloud import bigquery\nfrom google.api_core import exceptions as gcp_exceptions\nimport json\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\nclass ClickstreamConsumer:\n    \"\"\"Consumes clickstream events from Kafka and loads to BigQuery.\"\"\"\n\n    def __init__(\n        self,\n        kafka_servers: List[str],\n        topic: str,\n        bigquery_table: str,\n        dlq_table: str\n    ):\n        \"\"\"\n        Initialize consumer.\n\n        Args:\n            kafka_servers: List of Kafka bootstrap servers\n            topic: Kafka topic to consume from\n            bigquery_table: Target BigQuery table (project.dataset.table)\n            dlq_table: Dead letter queue table for failed messages\n        \"\"\"\n        self.consumer = KafkaConsumer(\n            topic,\n            bootstrap_servers=kafka_servers,\n            group_id='clickstream-to-bigquery',\n            auto_offset_reset='latest',\n            enable_auto_commit=False,\n            value_deserializer=lambda m: json.loads(m.decode('utf-8'))\n        )\n        self.bq_client = bigquery.Client()\n        self.table_id = bigquery_table\n        self.dlq_table_id = dlq_table\n        self.running = True\n\n        # Graceful shutdown on SIGTERM\n        signal.signal(signal.SIGTERM, self._shutdown)\n        signal.signal(signal.SIGINT, self._shutdown)\n\n    def _shutdown(self, signum, frame):\n        \"\"\"Handle shutdown signal.\"\"\"\n        logger.info(f\"Received signal {signum}, shutting down gracefully...\")\n        self.running = False\n\n    def validate_event(self, event: Dict) -&gt; Optional[str]:\n        \"\"\"\n        Validate event schema.\n\n        Args:\n            event: Event dictionary from Kafka\n\n        Returns:\n            Error message if invalid, None if valid\n        \"\"\"\n        required_fields = ['event_id', 'event_timestamp', 'event_type', 'user_id']\n\n        for field in required_fields:\n            if field not in event:\n                return f\"Missing required field: {field}\"\n\n        # Validate event type\n        valid_types = ['page_view', 'add_to_cart', 'purchase', 'search']\n        if event['event_type'] not in valid_types:\n            return f\"Invalid event_type: {event['event_type']}\"\n\n        return None  # Valid\n\n    def transform_event(self, event: Dict) -&gt; Dict:\n        \"\"\"\n        Transform event to BigQuery schema.\n\n        Args:\n            event: Raw event from Kafka\n\n        Returns:\n            Transformed event ready for BigQuery\n        \"\"\"\n        return {\n            'event_id': event['event_id'],\n            'event_timestamp': event['event_timestamp'],\n            'user_id': event['user_id'],\n            'event_type': event['event_type'],\n            'page_url': event.get('page_url'),  # Optional\n            'product_id': event.get('product_id'),  # Optional\n            'ingested_at': datetime.utcnow().isoformat()\n        }\n\n    def send_to_dlq(self, event: Dict, error: str):\n        \"\"\"\n        Send failed event to dead letter queue.\n\n        Args:\n            event: Original event that failed\n            error: Error message\n        \"\"\"\n        dlq_row = {\n            'original_event': json.dumps(event),\n            'error': error,\n            'failed_at': datetime.utcnow().isoformat(),\n            'consumer': 'clickstream-to-bigquery'\n        }\n\n        try:\n            errors = self.bq_client.insert_rows_json(self.dlq_table_id, [dlq_row])\n            if errors:\n                logger.error(f\"Failed to insert to DLQ: {errors}\")\n        except gcp_exceptions.GoogleAPIError as e:\n            logger.error(f\"DLQ insert failed: {e}\")\n\n    def process_batch(self, messages: List) -&gt; int:\n        \"\"\"\n        Process a batch of messages.\n\n        Args:\n            messages: List of Kafka messages\n\n        Returns:\n            Number of successfully processed messages\n        \"\"\"\n        valid_events = []\n        failed_count = 0\n\n        for message in messages:\n            event = message.value\n\n            # Validate\n            validation_error = self.validate_event(event)\n            if validation_error:\n                logger.warning(f\"Invalid event: {validation_error}\")\n                self.send_to_dlq(event, validation_error)\n                failed_count += 1\n                continue\n\n            # Transform\n            try:\n                transformed = self.transform_event(event)\n                valid_events.append(transformed)\n            except Exception as e:\n                logger.error(f\"Transform failed: {e}\", exc_info=True)\n                self.send_to_dlq(event, str(e))\n                failed_count += 1\n\n        # Load to BigQuery\n        if valid_events:\n            try:\n                errors = self.bq_client.insert_rows_json(self.table_id, valid_events)\n                if errors:\n                    logger.error(f\"BigQuery insert errors: {errors}\")\n                    for i, error in enumerate(errors):\n                        self.send_to_dlq(valid_events[i], str(error))\n                        failed_count += 1\n                else:\n                    logger.info(f\"Loaded {len(valid_events)} events to BigQuery\")\n            except gcp_exceptions.GoogleAPIError as e:\n                logger.error(f\"BigQuery insert failed: {e}\", exc_info=True)\n                # All events failed, send to DLQ\n                for event in valid_events:\n                    self.send_to_dlq(event, str(e))\n                failed_count += len(valid_events)\n\n        success_count = len(messages) - failed_count\n        return success_count\n\n    def run(self, batch_size: int = 100, batch_timeout: float = 5.0):\n        \"\"\"\n        Main processing loop.\n\n        Args:\n            batch_size: Number of messages to batch before inserting\n            batch_timeout: Max seconds to wait for batch to fill\n        \"\"\"\n        logger.info(\"Starting clickstream consumer...\")\n        batch = []\n        last_commit = time.time()\n\n        try:\n            for message in self.consumer:\n                if not self.running:\n                    break\n\n                batch.append(message)\n\n                # Process batch when full or timeout\n                should_process = (\n                    len(batch) &gt;= batch_size or\n                    time.time() - last_commit &gt; batch_timeout\n                )\n\n                if should_process:\n                    success_count = self.process_batch(batch)\n                    logger.info(f\"Processed {success_count}/{len(batch)} messages\")\n\n                    # Commit offsets\n                    self.consumer.commit()\n                    last_commit = time.time()\n                    batch = []\n\n        except Exception as e:\n            logger.error(f\"Fatal error in consumer: {e}\", exc_info=True)\n            raise\n        finally:\n            logger.info(\"Shutting down consumer...\")\n            self.consumer.close()\n\nif __name__ == \"__main__\":\n    consumer = ClickstreamConsumer(\n        kafka_servers=['kafka1:9092', 'kafka2:9092'],\n        topic='clickstream',\n        bigquery_table='ecommerce_prod.fact_events',\n        dlq_table='ecommerce_prod.events_dlq'\n    )\n    consumer.run()\n</code></pre> <p>Why this is production-quality: - \u2705 Full docstrings with type hints - \u2705 Specific error handling (not bare <code>except:</code>) - \u2705 Structured logging with levels - \u2705 Graceful shutdown handling - \u2705 Dead letter queue for failures - \u2705 Batch processing for efficiency - \u2705 Validation before transformation - \u2705 Clear variable names and logic flow</p>"},{"location":"chapters/12-capstone-part2/#testing-strategies","title":"Testing Strategies","text":"<p>Unit tests (test individual functions):</p> <pre><code># tests/unit/test_consumer.py\nimport pytest\nfrom streaming.clickstream_consumer import ClickstreamConsumer\n\ndef test_validate_event_success():\n    \"\"\"Valid event passes validation\"\"\"\n    consumer = ClickstreamConsumer([], 'topic', 'table', 'dlq')\n\n    event = {\n        'event_id': '123',\n        'event_timestamp': '2026-01-29T10:00:00Z',\n        'event_type': 'page_view',\n        'user_id': 'user-456'\n    }\n\n    error = consumer.validate_event(event)\n    assert error is None\n\ndef test_validate_event_missing_field():\n    \"\"\"Event missing required field fails validation\"\"\"\n    consumer = ClickstreamConsumer([], 'topic', 'table', 'dlq')\n\n    event = {\n        'event_id': '123',\n        # Missing event_timestamp\n        'event_type': 'page_view',\n        'user_id': 'user-456'\n    }\n\n    error = consumer.validate_event(event)\n    assert 'event_timestamp' in error\n\ndef test_transform_event():\n    \"\"\"Event transformation works correctly\"\"\"\n    consumer = ClickstreamConsumer([], 'topic', 'table', 'dlq')\n\n    event = {\n        'event_id': '123',\n        'event_timestamp': '2026-01-29T10:00:00Z',\n        'event_type': 'purchase',\n        'user_id': 'user-456',\n        'product_id': 'prod-789'\n    }\n\n    result = consumer.transform_event(event)\n\n    assert result['event_id'] == '123'\n    assert result['product_id'] == 'prod-789'\n    assert 'ingested_at' in result\n</code></pre> <p>Integration tests (test full flow):</p> <pre><code># tests/integration/test_consumer_to_bigquery.py\ndef test_consumer_end_to_end(kafka_producer, bigquery_client):\n    \"\"\"Test event flows from Kafka to BigQuery\"\"\"\n    # Publish test event\n    test_event = {\n        'event_id': f'test-{uuid.uuid4()}',\n        'event_timestamp': datetime.utcnow().isoformat(),\n        'event_type': 'page_view',\n        'user_id': 'test-user'\n    }\n    kafka_producer.send('clickstream', test_event)\n\n    # Start consumer (runs in background thread)\n    consumer = ClickstreamConsumer(\n        kafka_servers=['localhost:9092'],\n        topic='clickstream',\n        bigquery_table='test_dataset.events',\n        dlq_table='test_dataset.events_dlq'\n    )\n    consumer_thread = threading.Thread(target=consumer.run)\n    consumer_thread.start()\n\n    # Wait for processing\n    time.sleep(5)\n\n    # Verify in BigQuery\n    query = f\"\"\"\n        SELECT * FROM test_dataset.events\n        WHERE event_id = '{test_event['event_id']}'\n    \"\"\"\n    results = list(bigquery_client.query(query).result())\n\n    assert len(results) == 1\n    assert results[0].event_type == 'page_view'\n\n    # Cleanup\n    consumer.running = False\n    consumer_thread.join(timeout=10)\n</code></pre>"},{"location":"chapters/12-capstone-part2/#code-review-process","title":"Code Review Process","text":""},{"location":"chapters/12-capstone-part2/#writing-good-pull-requests","title":"Writing Good Pull Requests","text":"<p>Bad PR: <pre><code>Title: Fix\nDescription: (empty)\nFiles changed: 47 files, +2,847 lines\n</code></pre></p> <p>Good PR: <pre><code>Title: Add Kafka consumer with DLQ and validation\n\nDescription:\nImplements clickstream consumer that:\n- Reads from Kafka topic 'clickstream'\n- Validates event schema (required fields, valid types)\n- Transforms to BigQuery schema\n- Batches inserts for efficiency\n- Sends failed events to DLQ\n\nTesting:\n- Unit tests for validation/transformation logic\n- Integration test with test Kafka + BigQuery\n- Manually tested with 10K events, all processed successfully\n\nCloses #42\n</code></pre></p> <p>PR best practices: - Keep it small (&lt;500 lines if possible) - Clear title and description - Link to issue/ticket - Include testing notes - Add screenshots for UI changes</p>"},{"location":"chapters/12-capstone-part2/#conducting-code-reviews","title":"Conducting Code Reviews","text":"<p>What to look for:</p> <ol> <li>Correctness: Does it work? Are there bugs?</li> <li>Tests: Are there tests? Do they cover edge cases?</li> <li>Readability: Can you understand it? Is it documented?</li> <li>Performance: Are there obvious bottlenecks?</li> <li>Security: Any secrets hardcoded? SQL injection risks?</li> </ol> <p>How to give feedback:</p> <p>Bad review comment: <pre><code>This is wrong.\n</code></pre></p> <p>Good review comment: <pre><code>This could cause a memory leak if the consumer runs for days.\n\nThe `batch` list grows unbounded when BigQuery is slow.\nConsider adding a max batch size or timeout.\n\nSuggestion:\nif len(batch) &gt;= max_batch_size:\n    process_batch(batch)\n    batch = []\n</code></pre></p> <p>Review etiquette: - Assume good intent - Ask questions, don't demand changes - Praise good code - Suggest, don't command - Explain the \"why\"</p> <p>Example review:</p> <pre><code>Overall: Nice work! The error handling is solid and the tests are thorough.\n\nsrc/consumer.py:\nLine 45: Consider adding a type hint for `event`\nLine 67: What happens if BigQuery is down for &gt;1 hour? Will Kafka consumer lag forever?\n  Suggestion: Add a circuit breaker or max retry limit\nLine 89: \ud83c\udf89 Love the structured logging here\n\ntests/test_consumer.py:\nLine 23: Could we add a test for invalid event_type? (e.g., 'invalid_type')\n\nApproved pending discussion of circuit breaker for BigQuery failures.\n</code></pre>"},{"location":"chapters/12-capstone-part2/#debugging-distributed-systems","title":"Debugging Distributed Systems","text":""},{"location":"chapters/12-capstone-part2/#when-things-go-wrong","title":"When Things Go Wrong","text":"<p>Scenario: Events aren't appearing in BigQuery</p> <p>Debugging process:</p> <ol> <li> <p>Check the consumer logs <pre><code>docker logs clickstream-consumer --tail 100\n\n# Look for:\n# - Connection errors\n# - Validation failures\n# - BigQuery insert errors\n</code></pre></p> </li> <li> <p>Check Kafka consumer lag <pre><code>kafka-consumer-groups --bootstrap-server kafka:9092 \\\n    --describe --group clickstream-to-bigquery\n\n# Is lag growing? Consumer is falling behind\n# Is lag stable? Consumer is keeping up but maybe not processing\n</code></pre></p> </li> <li> <p>Check BigQuery streaming inserts <pre><code>SELECT\n    DATE(insertion_timestamp) AS date,\n    COUNT(*) AS records_inserted\nFROM `ecommerce_prod.fact_events`\nWHERE insertion_timestamp &gt;= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 HOUR)\nGROUP BY date\nORDER BY date DESC;\n</code></pre></p> </li> <li> <p>Check dead letter queue <pre><code>SELECT\n    error,\n    COUNT(*) AS count\nFROM `ecommerce_prod.events_dlq`\nWHERE failed_at &gt;= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 HOUR)\nGROUP BY error\nORDER BY count DESC;\n</code></pre></p> </li> <li> <p>Check Cloud Monitoring metrics</p> </li> <li>Consumer CPU/memory usage</li> <li>Network throughput</li> <li>BigQuery quota usage</li> </ol>"},{"location":"chapters/12-capstone-part2/#common-issues-solutions","title":"Common Issues &amp; Solutions","text":"Symptom Likely Cause Fix Consumer lag growing Can't keep up with event rate Scale consumers horizontally Events in DLQ Schema validation failing Check DLQ errors, fix upstream BigQuery insert errors Quota exceeded or schema mismatch Check quotas, validate schema Consumer crashing OOM or unhandled exception Check logs, add memory limits"},{"location":"chapters/12-capstone-part2/#technical-presentations","title":"Technical Presentations","text":""},{"location":"chapters/12-capstone-part2/#structure-15-minutes","title":"Structure (15 minutes)","text":"<p>Slide 1: Title (30 sec) - Project name - Your name - Date</p> <p>Slide 2: The Problem (2 min) - What business need does this solve? - Why does it matter? - One concrete example</p> <p>Slide 3-4: Architecture (3 min) - Show diagram - Explain data flow left to right - Highlight key components - Mention scale (events/sec, data volume)</p> <p>Slide 5-7: Live Demo (5 min) - Show working system - Publish test event \u2192 appears in dashboard - Show monitoring (lag, quality metrics) - Demonstrate graceful failure (DLQ)</p> <p>Slide 8-9: Challenges (3 min) - What went wrong during development? - How did you fix it? - What did you learn?</p> <p>Slide 10: What's Next (1 min) - Improvements you'd make - Production readiness gaps - Lessons for next project</p> <p>Slide 11: Questions (1 min) - Thank audience - Open for questions</p>"},{"location":"chapters/12-capstone-part2/#demo-tips","title":"Demo Tips","text":"<p>Before presenting: - \u2705 Test demo 3 times - \u2705 Have backup screenshots if live demo fails - \u2705 Clear browser history/tabs - \u2705 Zoom in on terminal text (font size 18+) - \u2705 Mute Slack/email notifications</p> <p>During demo: - Narrate what you're doing (\"I'm going to publish an event...\") - Go slow enough for audience to follow - If something breaks, have Plan B ready</p> <p>If demo fails: - Don't panic - Switch to screenshots - Explain what would have happened - Continue confidently</p>"},{"location":"chapters/12-capstone-part2/#capstone-rubric","title":"Capstone Rubric","text":""},{"location":"chapters/12-capstone-part2/#technical-60-points","title":"Technical (60 points)","text":"<p>Architecture (15 pts) - Clear diagram showing all components - Reasonable technology choices - Justification for decisions</p> <p>Implementation (25 pts) - Code works end-to-end - Streaming latency &lt;5 seconds - Data quality &gt;99% - Proper error handling - Tests passing</p> <p>Infrastructure (10 pts) - Fully automated with Terraform - One-command deployment - All secrets in environment variables</p> <p>Monitoring (10 pts) - Dashboard showing pipeline health - Alerts configured - Runbook documented</p>"},{"location":"chapters/12-capstone-part2/#professional-40-points","title":"Professional (40 points)","text":"<p>Collaboration (15 pts) - PR process followed - Code reviews conducted - Git commits are clear - Documentation is complete</p> <p>Presentation (15 pts) - Clear problem statement - Live demo works (or good backup) - Technical depth appropriate - Handles questions well</p> <p>Code Quality (10 pts) - Readable and documented - Follows conventions - No obvious bugs - Production-ready</p>"},{"location":"chapters/12-capstone-part2/#congratulations","title":"Congratulations!","text":"<p>You've completed the Data Engineering Bootcamp. You've learned: - Python &amp; SQL at scale - Git &amp; Docker workflows - Database design &amp; warehousing - ETL orchestration with Airflow - Data quality &amp; error handling - Distributed processing with Spark - Real-time streaming with Kafka - Cloud platforms (GCP) - Infrastructure as Code - CI/CD for data pipelines</p> <p>Most importantly, you've learned from 30+ war stories about production disasters. Now go build systems that don't create new ones.</p> <p>Welcome to data engineering. Try not to take down production on your first day.</p> <p>(But when you do\u2014and you will\u2014you'll know how to fix it.)</p>"},{"location":"plans/2026-01-28-storytelling-approach-design/","title":"Story-Driven Textbook Design: Proof of Concept","text":"<p>Date: 2026-01-28 Status: Implemented in Chapter 1 Purpose: Transform technical data engineering content into engaging, memorable lessons through humorous war stories</p>"},{"location":"plans/2026-01-28-storytelling-approach-design/#overview","title":"Overview","text":"<p>This document captures the storytelling approach used to rewrite Chapter 1 of the Data Engineering Bootcamp. The goal is to make heavy technical content more fun, immersive, and engaging by starting each major section with a behind-the-scenes production disaster story told in a conversational, self-deprecating tone.</p>"},{"location":"plans/2026-01-28-storytelling-approach-design/#core-principles","title":"Core Principles","text":""},{"location":"plans/2026-01-28-storytelling-approach-design/#1-story-first-approach","title":"1. Story-First Approach","text":"<p>Every major technical section opens with a war story that: - Sets up a relatable scenario - Describes a mistake or disaster - Shows the consequences (with humor) - Transitions naturally to \"Here's what I wish I'd known...\" - Flows seamlessly into the technical content</p>"},{"location":"plans/2026-01-28-storytelling-approach-design/#2-voice-and-tone","title":"2. Voice and Tone","text":"<ul> <li>Conversational mentor: Like a senior engineer over coffee</li> <li>Humorous &amp; self-deprecating: Laughing at our own mistakes</li> <li>Culturally aware: References Israeli work week (Sunday-Thursday)</li> <li>Specific details: Exact times, dollar amounts, error messages</li> <li>Honest consequences: Real costs, embarrassment, lessons learned</li> </ul>"},{"location":"plans/2026-01-28-storytelling-approach-design/#3-story-structure","title":"3. Story Structure","text":"<p>Each war story follows this 5-part pattern:</p> <pre><code>1. THE SETUP (1-2 sentences)\n   - \"So there I was, three months into my first data engineering job...\"\n   - Set the scene with relatable context\n\n2. THE MISTAKE (1-2 paragraphs)\n   - What I did wrong (with specific code/query)\n   - Why I thought it was a good idea at the time\n   - Self-deprecating humor\n\n3. THE DISASTER (1-2 paragraphs)\n   - When things went wrong (specific time/day)\n   - The \"oh no\" moment of realization\n   - Vivid details: memory usage, time elapsed, panicked responses\n\n4. THE LESSON (1 paragraph)\n   - How a senior engineer/mentor helped\n   - The fix (usually surprisingly simple)\n   - The memorable takeaway\n\n5. TRANSITION (1 sentence)\n   - \"Here's what I wish I'd known...\"\n   - \"Here's what [concept] actually does...\"\n   - Flows naturally into technical content\n</code></pre>"},{"location":"plans/2026-01-28-storytelling-approach-design/#chapter-1-implementation","title":"Chapter 1 Implementation","text":""},{"location":"plans/2026-01-28-storytelling-approach-design/#introduction","title":"Introduction","text":"<ul> <li>Enhanced the existing 3 AM scenario</li> <li>Added more sensory details (pajamas, squinting at screen)</li> <li>Emphasized the contrast between working vs. working at scale</li> </ul>"},{"location":"plans/2026-01-28-storytelling-approach-design/#section-1-python-generators","title":"Section 1: Python Generators","text":"<p>Story: \"The Day I Took Down Production With a Parenthesis\" - Thursday afternoon deployment (culturally appropriate) - Sunday morning disaster (working weekend in Israel) - $47,000 AWS bill (memorable, specific consequence) - One-character fix: <code>[]</code> to <code>()</code> - Leads into: generators as safety nets</p>"},{"location":"plans/2026-01-28-storytelling-approach-design/#section-2-list-comprehensions-vs-generator-expressions","title":"Section 2: List Comprehensions vs Generator Expressions","text":"<p>Story: \"The Pipeline That Looked Perfect (Until It Wasn't)\" - Sunday morning PR submission - Tech lead's skeptical comment - Laptop kernel panic testing with real data - \"Lost my unsaved code\" (relatable) - Parentheses vs brackets lesson</p>"},{"location":"plans/2026-01-28-storytelling-approach-design/#section-3-sql-window-functions","title":"Section 3: SQL Window Functions","text":"<p>Story: \"The Query That Took Three Days (And Should've Taken Three Minutes)\" - Thursday afternoon VP request (pressure situation) - Working late, desperate Slack post at 11 PM - 45-minute query timeout vs. 14-second solution - Made the board meeting \"barely\" - Introduction to window functions</p>"},{"location":"plans/2026-01-28-storytelling-approach-design/#section-4-ctes-common-table-expressions","title":"Section 4: CTEs (Common Table Expressions)","text":"<p>Story: \"The Query My Future Self Couldn't Understand\" - Three weeks later (shorter than typical 6 months) - \"Past me was a sadist\" (humor) - SQL Inception metaphor - Database admin rescue - \"Write SQL like you're leaving notes for yourself\"</p>"},{"location":"plans/2026-01-28-storytelling-approach-design/#common-pitfalls-section","title":"Common Pitfalls Section","text":"<ul> <li>Renamed to \"Scars I've Earned (So You Don't Have To)\"</li> <li>Each pitfall includes:</li> <li>The mistake (code example)</li> <li>The fix</li> <li>\"What it cost me\" (humorous consequence)</li> <li>Examples:</li> <li>\"$47,000 in AWS bills and awkward CTO conversation\"</li> <li>\"Nickname 'Overflow' because totals overflowed into other customers\"</li> <li>\"Two hours debugging, felt like an idiot\"</li> </ul>"},{"location":"plans/2026-01-28-storytelling-approach-design/#writing-guidelines","title":"Writing Guidelines","text":""},{"location":"plans/2026-01-28-storytelling-approach-design/#do","title":"DO:","text":"<ul> <li>Use specific numbers ($47,000, 52 million rows, 6:47 AM)</li> <li>Reference Israeli work culture (Thursday deployments, Sunday mornings)</li> <li>Include vivid sensory details (thrashing disk I/O, kernel panic, pajamas)</li> <li>Show the emotional journey (confident \u2192 confused \u2192 panicked \u2192 relieved)</li> <li>Make the fix surprisingly simple (one character, one clause, etc.)</li> <li>Use relatable scenarios (late-night debugging, code reviews, VP requests)</li> <li>Include memorable quotes from mentors</li> <li>Reference real tools and errors (AWS, SSH, memory usage, Slack)</li> </ul>"},{"location":"plans/2026-01-28-storytelling-approach-design/#dont","title":"DON'T:","text":"<ul> <li>Make yourself look too competent (defeats the self-deprecating humor)</li> <li>Skip the consequences (the \"what it cost me\" is essential)</li> <li>Use generic disasters (\"it was slow\" vs. \"45-minute timeout\")</li> <li>Forget the cultural context (Friday deployments don't work in Israel)</li> <li>Rush to the technical content (let the story breathe)</li> <li>Use overly technical language in the story (save that for after)</li> </ul>"},{"location":"plans/2026-01-28-storytelling-approach-design/#story-templates","title":"Story Templates","text":""},{"location":"plans/2026-01-28-storytelling-approach-design/#template-1-the-production-disaster","title":"Template 1: The Production Disaster","text":"<pre><code>So there I was, [timeframe] into [job/project], feeling [overconfident state].\nI'd just [what you built/deployed]. Ran beautifully on [small test].\nDeployed to production on [culturally appropriate bad timing].\n\n[Day/time], my phone [how disaster manifested].\n[What was broken]. Not [mild version]. Not [moderate version].\n[Severe version].\n\nI [panicked action] and [emotional reaction]. [Specific metric]: [shocking number].\n\nI frantically [debugging action]. There it was, line [number]:\n\n[The problematic code]\n\nThat's it. That's the whole problem. [Explain what went wrong in layman's terms].\n\n[Senior person] showed me the fix. [How simple it was]:\n\n[The fixed code]\n\n[Simple description of change]. That's it. [Result metric].\n\nThat [specific consequence] taught me more about [concept] than any tutorial ever could.\n</code></pre>"},{"location":"plans/2026-01-28-storytelling-approach-design/#template-2-the-code-review-humiliation","title":"Template 2: The Code Review Humiliation","text":"<pre><code>Let me tell you about the code review that still makes me cringe.\n\nI'd built this [complimentary description of your code]. I was *proud* of this thing.\nSubmitted the PR on [day], confident [what you expected].\n\nInstead, I got [disappointing feedback].\n\nI was [emotional reaction]. My code was [your justification]! Look at [what you were proud of].\n\nSo I did what [reviewer] asked. [What you tried]. Hit run.\n\n[Disaster]. Not [mild version]. [Severe version]. Had to [emergency action].\n[Additional consequence, often data loss or embarrassment].\n\nTurns out, [what was actually wrong].\n\n[Reviewer] showed me the fix:\n\n[The solution]\n\n[Why the solution was better].\n\nThe kicker? [Reviewer's memorable quote or your realization].\n</code></pre>"},{"location":"plans/2026-01-28-storytelling-approach-design/#template-3-the-urgent-request-gone-wrong","title":"Template 3: The Urgent Request Gone Wrong","text":"<pre><code>[Day of week] [time of day]. [Authority figure] walks over to my desk. Never a good sign.\n\n\"[The request that sounds simple].\"\n\n\"Sure,\" I said. \"[Why you thought it would be easy].\"\n\n[Time later], I was still [where/what you were doing]. My [work] was a [negative metaphor].\n[Describe multiple problems].\n\nWorse? It was [additional problem, often performance]. [Specific slow metric].\n\nI did what any desperate engineer does at [specific late time] on a [day]:\nI [what you did to ask for help].\n\n[Person who helped]\u2014bless [them]\u2014responded immediately: \"[Devastating question about obvious solution].\"\n\n[Concept you should have used]? I'd heard of them. Vaguely. Hadn't really bothered learning them because, you know, [your flawed reasoning].\n\n[Person] sent me [solution]. Same exact output. [Comparison metrics].\n\n[Your reaction]. That's it? That's the whole thing?\n\nTurns out, I'd spent [time wasted] trying to solve a problem that [tool/concept] had a built-in solution for.\n\n[How you barely made the deadline]. Never forgot [concept] again.\n</code></pre>"},{"location":"plans/2026-01-28-storytelling-approach-design/#technical-content-integration","title":"Technical Content Integration","text":"<p>After each war story:</p> <ol> <li>Transition sentence</li> <li>\"Here's what I wish I'd known...\"</li> <li>\"Here's why [concept] are your friend...\"</li> <li> <p>\"Here's what [concept] actually do...\"</p> </li> <li> <p>Key Idea (1-2 sentences)</p> </li> <li>Clear, concise explanation</li> <li> <p>Focus on the \"why\" not just the \"what\"</p> </li> <li> <p>Technical Examples</p> </li> <li>Keep all existing code examples</li> <li>Maintain \"bad way\" vs \"good way\" comparisons</li> <li> <p>Include all metrics and measurements</p> </li> <li> <p>Why This Matters</p> </li> <li>Real-world applications</li> <li>When to use the technique</li> <li> <p>Performance implications</p> </li> <li> <p>Try It</p> </li> <li>Keep all hands-on exercises</li> <li>Maintain the interactive learning elements</li> </ol>"},{"location":"plans/2026-01-28-storytelling-approach-design/#success-metrics","title":"Success Metrics","text":"<p>A successful story:</p> <ul> <li>Makes you laugh or cringe (emotional engagement)</li> <li>Sticks in your memory (specific details, consequences)</li> <li>Teaches the lesson before the technical content (intuitive understanding)</li> <li>Feels authentic (could have happened to anyone)</li> <li>Flows naturally (transition to technical content is seamless)</li> </ul>"},{"location":"plans/2026-01-28-storytelling-approach-design/#future-applications","title":"Future Applications","text":"<p>This approach can be applied to:</p> <ul> <li>Chapter 2 (Git &amp; Docker): Merge conflict disasters, container nightmares</li> <li>Chapter 3 (Database Modeling): Normalization gone wrong, schema disasters</li> <li>Chapter 4 (Data Warehousing): Partitioning failures, cost explosions</li> </ul>"},{"location":"plans/2026-01-28-storytelling-approach-design/#examples-for-future-chapters","title":"Examples for Future Chapters","text":""},{"location":"plans/2026-01-28-storytelling-approach-design/#chapter-2-git-docker","title":"Chapter 2: Git &amp; Docker","text":"<ul> <li>\"The Merge Conflict That Ate My Friday\"</li> <li>\"The Docker Image That Was 47GB (It Should've Been 200MB)\"</li> <li>\"The Day I Force-Pushed to Main\"</li> </ul>"},{"location":"plans/2026-01-28-storytelling-approach-design/#chapter-3-database-modeling","title":"Chapter 3: Database Modeling","text":"<ul> <li>\"The Perfectly Normalized Schema That Brought Production to Its Knees\"</li> <li>\"The Foreign Key I Forgot (And The Million Orphaned Records)\"</li> <li>\"The Migration That Took 19 Hours (And How I Explain It In The Post-Mortem)\"</li> </ul>"},{"location":"plans/2026-01-28-storytelling-approach-design/#chapter-4-data-warehousing","title":"Chapter 4: Data Warehousing","text":"<ul> <li>\"The Unpartitioned Table That Cost Us $12,000 In A Single Query\"</li> <li>\"The SCD Type 2 Implementation That Created 400 Million Rows\"</li> <li>\"The BI Dashboard That Scanned 8 Petabytes (When It Should've Scanned 8 Gigabytes)\"</li> </ul>"},{"location":"plans/2026-01-28-storytelling-approach-design/#conclusion","title":"Conclusion","text":"<p>The story-driven approach transforms technical content from \"this is how it works\" to \"let me tell you why you absolutely need to know this.\" By leading with memorable disasters and self-deprecating humor, we create emotional anchors that make the technical lessons stick.</p> <p>The key is authenticity: these stories feel real because they could be real. Every data engineer has had that 3 AM wake-up call, that embarrassing code review, that \"how did this get to production?\" moment. By sharing these experiences with humor and honesty, we create a learning environment where mistakes are normalized and lessons are memorable.</p>"},{"location":"sims/bigquery-partitioning/spec/","title":"MicroSim: BigQuery Partitioning Cost Calculator","text":""},{"location":"sims/bigquery-partitioning/spec/#overview","title":"Overview","text":""},{"location":"sims/bigquery-partitioning/spec/#concept-visualized","title":"Concept Visualized","text":"<ul> <li>Concept: BigQuery Partitioning (bigquery-partitioning), BigQuery Pricing Model (bigquery-pricing), BigQuery Optimization (bigquery-optimization)</li> <li>Learning Goal: Students understand how table partitioning in BigQuery dramatically reduces query costs by manipulating partition keys and date ranges, observing the reduction in data scanned and associated costs</li> <li>Difficulty: Intermediate</li> <li>Chapter: Week 3-4 (Data Storage &amp; Modeling)</li> </ul>"},{"location":"sims/bigquery-partitioning/spec/#the-aha-moment","title":"The \"Aha\" Moment","text":"<p>When students add a partition on the date column and run a query filtering by last 30 days, they see data scanned drop from 5TB (entire table, $25.00) to 150GB (one partition, $0.75)\u2014a 97% cost reduction\u2014demonstrating why partitioning is essential for large-scale analytics.</p>"},{"location":"sims/bigquery-partitioning/spec/#interface-design","title":"Interface Design","text":""},{"location":"sims/bigquery-partitioning/spec/#layout","title":"Layout","text":"<ul> <li>Left Panel (60%): Visual representation of table storage with partition visualization</li> <li>Right Panel (40%): Query controls, partition configuration, and cost analysis</li> </ul>"},{"location":"sims/bigquery-partitioning/spec/#controls-right-panel","title":"Controls (Right Panel)","text":"Control Type Range/Options Default Effect Partition Strategy dropdown \"None\", \"Daily\", \"Monthly\", \"Integer Range\" \"None\" Sets table partitioning method Partition Column dropdown \"created_date\", \"event_timestamp\", \"customer_id\" None Selects column to partition on Query Type dropdown \"Last 7 Days\", \"Last 30 Days\", \"Last Year\", \"Specific Date\", \"All Time\" \"Last 30 Days\" Determines query date range Table Size slider 100GB - 50TB 5TB Sets total table size Run Query button N/A N/A Executes query and shows cost calculation Show Clustering toggle On/Off Off Adds clustering visualization (advanced) <p>Cost Analysis Panel: Displays before/after comparison: <pre><code>Without Partitioning:\n\u251c\u2500 Data Scanned: 5,000 GB\n\u251c\u2500 Query Cost: $25.00\n\u2514\u2500 Partitions Touched: N/A\n\nWith Daily Partitioning (last 30 days):\n\u251c\u2500 Data Scanned: 150 GB\n\u251c\u2500 Query Cost: $0.75\n\u251c\u2500 Partitions Touched: 30 of 1,825\n\u251c\u2500 Cost Savings: 97%\n\u2514\u2500 Monthly Savings (100 queries): $2,425\n</code></pre></p>"},{"location":"sims/bigquery-partitioning/spec/#visualization-left-panel","title":"Visualization (Left Panel)","text":"<p>What is Displayed: - Unpartitioned Table View:   - Single large blue rectangle representing entire table   - Size label: \"5 TB\"   - When query runs, entire rectangle highlights red (all data scanned)</p> <ul> <li>Partitioned Table View:</li> <li>Grid of smaller rectangles, each representing one partition</li> <li>Daily partitions: 1825 squares (5 years of data)</li> <li>Color gradient: Recent partitions (darker blue) \u2192 Older partitions (lighter blue, faded)</li> <li>Each partition labeled with date and size (e.g., \"2024-01-15, 2.7GB\")</li> </ul> <p>Interactive Visualization: - When \"Run Query\" clicked:   1. Partitions matching query filter highlight in green (data to scan)   2. Other partitions fade to gray (skipped via partition pruning)   3. \"Data scanner\" animation sweeps across highlighted partitions (1s)   4. Cost meter fills proportionally to data scanned   5. Tooltip on hover shows partition metadata: date range, row count, size</p> <p>How it Updates: - Partition strategy change: Table morphs from single block to partitioned grid (1s animation) - Query execution: Color wave highlights relevant partitions (0.5s) - Size slider: Partition sizes scale proportionally - Clustering toggle: Adds second dimension visualization showing clustered blocks within partitions</p>"},{"location":"sims/bigquery-partitioning/spec/#educational-flow","title":"Educational Flow","text":""},{"location":"sims/bigquery-partitioning/spec/#step-1-default-state","title":"Step 1: Default State","text":"<p>Student sees unpartitioned 5TB analytics table: - Table: \"website_events\" with 5TB data spanning 5 years (2020-2024) - Query configured: \"Last 30 Days\" (typical analytics query) - Visualization: Single massive blue block - Cost panel shows: Data Scanned = 5,000 GB, Cost = $25.00 per query - Note displayed: \"This query scans the entire table even though you only need 30 days of data\"</p>"},{"location":"sims/bigquery-partitioning/spec/#step-2-first-interaction","title":"Step 2: First Interaction","text":"<p>Prompt: \"Change Partition Strategy to 'Daily' and Partition Column to 'event_timestamp', then click 'Run Query'\"</p> <p>Result - Animated transformation: 1. Table explodes into 1,825 small squares arranged in grid (1.5s animation) 2. Each square represents one day of data 3. Query execution highlights only last 30 partitions in green 4. Cost panel updates:    - Data Scanned: 5,000 GB \u2192 150 GB (97% reduction)    - Query Cost: $25.00 \u2192 $0.75 (97% savings)    - Partitions Touched: 30 of 1,825 5. Educational callout appears: \"BigQuery only scans partitions matching your WHERE clause filter!\"</p> <p>Students observe: Dramatic cost reduction by avoiding unnecessary data scanning</p>"},{"location":"sims/bigquery-partitioning/spec/#step-3-exploration","title":"Step 3: Exploration","text":"<p>Prompt: \"Try different query date ranges and observe how many partitions are scanned\"</p> <p>Pattern they should discover: - Query \"Last 7 Days\":   - Scans: 7 partitions, ~35 GB, $0.18   - More specific = cheaper</p> <ul> <li>Query \"Last Year\":</li> <li>Scans: 365 partitions, ~1,825 GB, $9.13</li> <li> <p>Still 64% savings vs full scan</p> </li> <li> <p>Query \"All Time\":</p> </li> <li>Scans: All 1,825 partitions, 5,000 GB, $25.00</li> <li> <p>No savings (but no penalty either)</p> </li> <li> <p>Key insight: Partitioning helps most when queries filter on partition key</p> </li> <li>Anti-pattern: Query without date filter = full table scan despite partitioning</li> </ul>"},{"location":"sims/bigquery-partitioning/spec/#step-4-wrong-partition-key","title":"Step 4: Wrong Partition Key","text":"<p>Prompt: \"Change Partition Column to 'customer_id' (integer range partitioning) and run the same 'Last 30 Days' query\"</p> <p>Result: - Table reorganizes into customer_id ranges: [0-100K], [100K-200K], etc. - Query still filters on event_timestamp (date) - All partitions highlight (cannot prune by date anymore) - Cost panel: Data Scanned = 5,000 GB, Cost = $25.00 - Warning appears: \"\u26a0 Partition key doesn't match your query filter! No cost savings.\"</p> <p>Students observe: Partition key must match query patterns</p>"},{"location":"sims/bigquery-partitioning/spec/#step-5-challenge","title":"Step 5: Challenge","text":"<p>Scenario: \"Your analytics team runs 100 queries per day, all filtering on event_timestamp. Table grows by 10GB daily. Should you partition by timestamp, and should you use daily or monthly partitions?\"</p> <p>Expected learning: - Should partition: Yes, 97% cost savings \u00d7 100 queries = $2,425/month saved - Daily vs Monthly:   - Daily: Better granularity, most queries need 1-30 days   - Monthly: Fewer partitions (60 instead of 1,825), but less precise   - Recommendation: Daily partitioning for this access pattern - Bonus insight: With 10GB daily growth, table will be 3.65TB/year\u2014partitioning also helps with partition expiration (auto-delete old data)</p> <p>Advanced question: \"What if queries filter on both timestamp AND customer_id?\" Answer: Use timestamp partitioning + customer_id clustering for best performance</p>"},{"location":"sims/bigquery-partitioning/spec/#technical-specification","title":"Technical Specification","text":"<ul> <li>Library: p5.js</li> <li>Canvas: Responsive, min 700px width \u00d7 500px height</li> <li>Frame Rate: 30fps</li> <li>Data: Formula-based calculations   <pre><code>// BigQuery pricing: $5 per TB scanned (as of 2024)\nconst PRICE_PER_TB = 5.00;\n\nfunction calculateCost(partitionStrategy, queryDateRange, tableSize) {\n  if (partitionStrategy === \"None\") {\n    return tableSize * PRICE_PER_TB;\n  }\n\n  const dataScanned = calculatePartitionsScanned(queryDateRange) * avgPartitionSize;\n  return dataScanned * PRICE_PER_TB;\n}\n\n// Daily partitions: ~2.7GB average (5TB / 1825 days)\n// Monthly partitions: ~83GB average (5TB / 60 months)\n</code></pre></li> </ul>"},{"location":"sims/bigquery-partitioning/spec/#assessment-integration","title":"Assessment Integration","text":"<p>After using this MicroSim, students should answer:</p> <ol> <li>Knowledge Check: BigQuery charges based on:</li> <li>a) Number of queries executed</li> <li>b) Amount of data scanned by queries \u2713</li> <li>c) Number of tables in your dataset</li> <li> <p>d) Storage space used</p> </li> <li> <p>Application: A partitioned table has daily partitions for 3 years (1,095 partitions, 10TB total). A query filters: <code>WHERE event_date &gt;= '2024-01-01'</code>. Approximately how much data is scanned?</p> </li> <li>a) 10 TB (entire table)</li> <li>b) ~27 GB (28 days of January 2024)</li> <li>c) ~1 GB (one partition)</li> <li>d) None, the query is free</li> </ol> <p>Answer: b) ~27 GB (queries scan all partitions matching the filter: 28 days in January)</p> <ol> <li>Cost Optimization: Your queries always filter on user_country. Should you partition the table by user_country?</li> <li>a) Yes, always partition on query filters</li> <li>b) No, BigQuery doesn't support country-based partitioning \u2713</li> <li>c) Yes, but only if there are fewer than 4,000 countries</li> <li>d) No, use clustering instead (partitioning limited to date/timestamp/integer; use clustering for strings)</li> </ol> <p>Better answer: d) No, partition on date/timestamp if available, use clustering on user_country</p>"},{"location":"sims/bigquery-partitioning/spec/#extension-ideas","title":"Extension Ideas","text":"<ul> <li>Add clustering visualization showing how clustering works within partitions</li> <li>Include partition expiration simulation (auto-delete old partitions)</li> <li>Show ingestion-time vs column-based partitioning differences</li> <li>Add \"Query Cost History\" graph showing monthly costs with/without partitioning</li> <li>Include partition size limits (4,000 partition max in BigQuery)</li> <li>Show require_partition_filter setting to prevent accidental full scans</li> <li>Add real BigQuery SQL examples showing partition metadata queries</li> <li>Include clustered vs non-clustered performance comparison</li> <li>Visualize columnar storage format and how it enables column pruning</li> <li>Add multi-statement transaction cost calculation</li> </ul>"},{"location":"sims/bigquery-partitioning-cost/spec/","title":"MicroSim: BigQuery Partitioning Cost Calculator","text":""},{"location":"sims/bigquery-partitioning-cost/spec/#overview","title":"Overview","text":""},{"location":"sims/bigquery-partitioning-cost/spec/#concept-visualized","title":"Concept Visualized","text":"<ul> <li>Concept: BigQuery table partitioning strategies and pricing model (data scanned = cost)</li> <li>Learning Goal: Students will understand how partitioning dramatically reduces query costs in BigQuery by comparing data scanned across different partition strategies for the same analytical queries</li> <li>Difficulty: Intermediate</li> <li>Chapter: Week 3-4 - Data Storage &amp; Modeling (BigQuery Introduction)</li> </ul>"},{"location":"sims/bigquery-partitioning-cost/spec/#the-aha-moment","title":"The \"Aha\" Moment","text":"<p>When the student runs the query \"Get sales for last 7 days\" on an unpartitioned 5TB table, they see BigQuery scans all 5TB costing $25. Then they apply daily partitioning and run the same query\u2014BigQuery scans only 35GB (7 days of data) costing $0.175. That's a 143x cost reduction for the exact same result. This demonstrates: in cloud data warehouses, schema design directly impacts your bill.</p>"},{"location":"sims/bigquery-partitioning-cost/spec/#interface-design","title":"Interface Design","text":""},{"location":"sims/bigquery-partitioning-cost/spec/#layout","title":"Layout","text":"<ul> <li>Left Panel (55% width): Table visualization showing data blocks colored by partition/date</li> <li>Right Panel (45% width): Controls, query builder, and cost metrics</li> <li>Bottom Status Bar (full width): Real-time cost calculator and data scanned indicator</li> </ul>"},{"location":"sims/bigquery-partitioning-cost/spec/#controls-right-panel","title":"Controls (Right Panel)","text":"Control Type Range/Options Default Effect on Visualization Table Size slider 100GB-10TB 5TB Scales table visualization and costs proportionally Partition Strategy dropdown [\"None (Unpartitioned)\", \"Daily\", \"Monthly\", \"Yearly\", \"Custom Range\"] \"None\" Changes how data blocks are organized/colored Partition Column dropdown [\"date\", \"timestamp\", \"ingestion_time\"] \"date\" Determines partition key (only if partitioned) Clustering checkbox - off Adds clustering within partitions (shows additional optimization) Query Preset dropdown [\"Last 7 Days\", \"Last Month\", \"Specific Date\", \"Year-to-Date\", \"Full Table Scan\"] \"Last 7 Days\" Loads common query patterns Date Range date picker 2020-2024 last 7 days Filters data range for query Run Query button - - Executes query, animates data scan, shows cost Compare Strategies button - - Runs same query across all partition strategies side-by-side <p>Additional UI Elements: - Query Editor: Editable SQL text showing current query with WHERE clause - Cost Breakdown Panel: Itemized costs (data scanned, on-demand vs flat-rate pricing) - Best Practice Tips: Context-sensitive recommendations based on current query - Historical Queries Log: Shows past queries with costs (builds over session)</p>"},{"location":"sims/bigquery-partitioning-cost/spec/#visualization-details-left-panel","title":"Visualization Details (Left Panel)","text":"<p>What is Displayed:</p> <p>Table Representation: - Visual Metaphor: Table shown as grid of \"data blocks\" (rectangles)   - Each block represents a chunk of data (e.g., 1GB)   - 5TB table = 5,000 blocks   - Arranged in timeline from left (older) to right (newer)</p> <p>Color Coding by Partition Strategy:</p> <p>Unpartitioned (None): - All blocks same gray color (no organization) - Data appears as monolithic mass - Label: \"5TB unpartitioned data\"</p> <p>Daily Partitioning: - Blocks colored by date (gradient from blue to red across time) - Vertical dividers separate days - Labels: \"2024-01-15\", \"2024-01-16\", \"2024-01-17\"... - Each day's data visually grouped</p> <p>Monthly Partitioning: - Blocks colored by month (12 distinct colors) - Thicker vertical dividers separate months - Labels: \"Jan 2024\", \"Feb 2024\"...</p> <p>Yearly Partitioning: - Blocks colored by year (4-5 distinct colors for multi-year data) - Very thick dividers separate years - Labels: \"2020\", \"2021\", \"2022\"...</p> <p>Query Execution Visualization: - When query runs, animation highlights which blocks are scanned - Unpartitioned: All blocks highlight yellow (full table scan) - Partitioned: Only relevant partition blocks highlight green (pruned scan) - Non-scanned blocks fade to 20% opacity (showing pruning effectiveness) - Progress bar shows percentage of table scanned</p> <p>Additional Visual Elements: - Partition Pruning Indicator: Scissors icon appears when partitions are eliminated - Data Scanned Bar: Horizontal bar below table showing proportion: \"35GB / 5TB (0.7%)\" - Cost Meter: Thermometer visualization filling from green (\\(0) to red (\\)50+)</p> <p>How it Updates:</p> <p>When \"Partition Strategy\" dropdown changes: 1. Table blocks animate rearranging (500ms) 2. Colors transition to new partition scheme (800ms) 3. Partition dividers appear/disappear 4. Labels update to show partition boundaries 5. Info panel explains chosen strategy: \"Daily partitioning creates 365 partitions per year. Queries filtering by date scan only relevant days.\"</p> <p>When \"Run Query\" button clicked: 1. Query text highlights WHERE clause (300ms) 2. System calculates matching partitions (shown in right panel):    - \"Date filter: 2024-01-15 to 2024-01-21\"    - \"Partitions matched: 7\"    - \"Partitions scanned: 7 / 1,825 (0.4%)\" 3. Table visualization animation (2 seconds):    - Matching partition blocks pulse yellow    - Non-matching blocks fade to 20% opacity    - Data flow animation: selected blocks \u2192 query processor icon    - Progress counter increments: \"Scanning 35GB...\" 4. Results panel appears:    - \"Data scanned: 35GB\"    - \"Cost: $0.175 (on-demand pricing)\"    - \"Execution time: 3.2s\"    - Comparison badge: \"143x cheaper than unpartitioned!\"</p> <p>When \"Compare Strategies\" button clicked: - Screen splits into 4 quadrants showing same query with:   1. No partitioning: Scans 5TB, costs $25   2. Daily partitioning: Scans 35GB, costs $0.175   3. Monthly partitioning: Scans 155GB (entire January), costs $0.775   4. Yearly partitioning: Scans 1.8TB (entire 2024), costs $9 - Bar chart appears comparing costs - Winner highlighted in green with crown icon</p> <p>Visual Feedback: - Hover over data blocks: Tooltip shows partition details (\"Partition: 2024-01-15, Size: 5GB, Rows: 2.1M\") - Hover over query: Highlights partition column in WHERE clause - Cost meter changes color: &lt;$1 green, \\(1-\\)10 yellow, &gt;$10 red - Optimization suggestions appear: \"\ud83d\udca1 Daily partitioning recommended for queries filtering by date\"</p>"},{"location":"sims/bigquery-partitioning-cost/spec/#educational-flow","title":"Educational Flow","text":""},{"location":"sims/bigquery-partitioning-cost/spec/#step-1-default-state","title":"Step 1: Default State","text":"<p>Student sees unpartitioned 5TB sales table with sample schema: <pre><code>CREATE TABLE sales (\n  order_id INT64,\n  customer_id INT64,\n  product_id INT64,\n  order_date DATE,\n  order_timestamp TIMESTAMP,\n  amount FLOAT64\n)\n</code></pre></p> <p>Query preset: \"Last 7 Days\" <pre><code>SELECT *\nFROM sales\nWHERE order_date &gt;= '2024-01-15' AND order_date &lt;= '2024-01-21'\n</code></pre></p> <p>Info panel explains: \"This table contains 3 years of sales data (5TB). Let's see how much it costs to query one week of data.\"</p> <p>This shows the baseline: no optimization.</p>"},{"location":"sims/bigquery-partitioning-cost/spec/#step-2-first-interaction-feel-the-cost","title":"Step 2: First Interaction - Feel the Cost","text":"<p>Prompt: \"Click 'Run Query' to see how BigQuery processes this query.\"</p> <p>Student clicks Run Query.</p> <p>Result: - Animation shows ALL data blocks highlighting yellow (full table scan) - Progress bar: \"Scanning 5,000GB... 10%... 50%... 100%\" - Results panel:   - Data scanned: 5TB   - Cost: $25.00 (at $5 per TB)   - Execution time: 45 seconds   - \u26a0\ufe0f \"Warning: This query scanned 5TB to return 7 days of data!\"</p> <p>Info panel explains: \"Without partitioning, BigQuery must scan every row to find matching dates. Even though you only want 7 days, it reads 3 years. You pay for all 5TB scanned.\"</p> <p>Prompt: \"That's $25 for one query! If your dashboard runs this every 5 minutes (288 times/day), that's $7,200/day = $216,000/month. Let's fix this with partitioning.\"</p> <p>What it teaches: Cloud data warehouse costs are based on data scanned, not returned. Poor schema design = expensive queries.</p>"},{"location":"sims/bigquery-partitioning-cost/spec/#step-3-apply-partitioning","title":"Step 3: Apply Partitioning","text":"<p>Prompt: \"Change 'Partition Strategy' to 'Daily' and run the same query again.\"</p> <p>Student selects \"Daily\" from dropdown.</p> <p>Visual change: - Table blocks rearrange into daily partitions (animation) - Color gradient appears (blue = older, red = newer) - Partition labels appear: \"2021-01-01\", \"2021-01-02\"... \"2024-01-21\" - Partition count badge: \"1,095 partitions\"</p> <p>Info panel explains: \"Daily partitioning creates one partition per day. BigQuery stores partition metadata and uses it to eliminate partitions during queries. Your WHERE clause <code>order_date &gt;= '2024-01-15'</code> allows BigQuery to skip 1,088 partitions!\"</p> <p>Prompt: \"Now click 'Run Query' again.\"</p> <p>Student clicks Run Query.</p> <p>Result: - Animation shows ONLY 7 daily partition blocks highlighting green (2024-01-15 through 2024-01-21) - Other 1,088 partitions fade to 20% opacity with \"\u2702\ufe0f Pruned\" label - Progress bar: \"Scanning 35GB... (partitions pruned!)\" - Results panel:   - Data scanned: 35GB (7 days \u00d7 5GB per day)   - Cost: $0.175   - Execution time: 3.2 seconds   - \u2705 \"Optimized: Partition pruning saved $24.82 (99.3% reduction)!\"   - Badge: \"143x cheaper than unpartitioned\"</p> <p>Cost comparison tooltip: \"$25.00 \u2192 $0.175 saved $24.82 per query. At 288 queries/day: saved $7,148/day = $214,440/month!\"</p> <p>What it teaches: Partitioning uses metadata to skip irrelevant data, dramatically reducing costs. Proper partitioning can save 99%+ on costs for filtered queries.</p>"},{"location":"sims/bigquery-partitioning-cost/spec/#step-4-exploration-partition-granularity-matters","title":"Step 4: Exploration - Partition Granularity Matters","text":"<p>Prompt: \"Try different partition strategies (Monthly, Yearly) and compare costs. Which is best?\"</p> <p>Student experiments:</p> <p>Monthly Partitioning: - Query for last 7 days (Jan 15-21) must scan entire January partition - Data scanned: 155GB (31 days) - Cost: $0.775 - Lesson: Too coarse\u2014scans 4.4x more data than necessary</p> <p>Yearly Partitioning: - Query for last 7 days must scan entire 2024 partition - Data scanned: 1.8TB (365 days so far) - Cost: $9.00 - Lesson: Way too coarse\u2014scans 51x more data than necessary</p> <p>Daily Partitioning (revisited): - Scans exactly 7 days needed - Cost: $0.175 - Lesson: Optimal granularity for this query pattern</p> <p>Key insight: Partition granularity should match query filter granularity. If queries typically filter by day/week, use daily partitioning. If queries filter by month, monthly partitioning is appropriate.</p> <p>Info panel explains: \"Daily partitioning has 365x more partitions than yearly, but each partition is 365x smaller. BigQuery partition metadata is extremely lightweight (free to store), so finer partitioning is almost always better for reducing costs.\"</p>"},{"location":"sims/bigquery-partitioning-cost/spec/#step-5-compare-different-query-patterns","title":"Step 5: Compare Different Query Patterns","text":"<p>Prompt: \"Switch query preset to 'Full Table Scan' (e.g., calculating lifetime totals). Does partitioning still help?\"</p> <p>Student selects \"Full Table Scan\" preset: <pre><code>SELECT SUM(amount) AS total_revenue\nFROM sales\n-- No WHERE clause\n</code></pre></p> <p>Student runs query with daily partitioning.</p> <p>Result: - All 1,095 partitions highlight (no pruning possible) - Data scanned: 5TB (entire table) - Cost: $25.00 - Info: \"Partitioning doesn't help queries without partition column filters\"</p> <p>Key insight: Partitioning only helps queries that filter on partition column. For full table scans, partition strategy doesn't matter. Must design partitions based on common query patterns.</p>"},{"location":"sims/bigquery-partitioning-cost/spec/#step-6-challenge","title":"Step 6: Challenge","text":"<p>Present scenario: \"You're designing a BigQuery table for IoT sensor data: - 1 billion events per day - 500GB uncompressed data per day - Table will contain 2 years of data = 365TB total - Common queries:   - Dashboard: Show last 24 hours of data (refreshes every 5 minutes)   - Weekly reports: Analyze last 7 days by sensor type   - Monthly aggregations: Calculate monthly totals per device   - Ad-hoc analyses: Engineers query arbitrary date ranges</p> <p>Your company has $5,000/month BigQuery budget. Design the optimal partitioning strategy.\"</p> <p>Interactive Challenge: Student must: 1. Choose partition strategy 2. Choose partition column (ingestion_time vs event_timestamp) 3. Decide whether to add clustering 4. Test with sample queries</p> <p>Simulator shows: - Dashboard query (last 24h) runs 8,640 times/month:   - Unpartitioned: 365TB \u00d7 8,640 = \\(15.8M/month** \ud83d\udd34   - Daily partitioned: 0.5TB \u00d7 8,640 = **\\)21,600/month \ud83d\udfe1   - Hourly partitioned: 21GB \u00d7 8,640 = $907/month \u2705</p> <p>Expected solution: - Partition: Daily or hourly on ingestion_time (ingestion_time is better than event_timestamp due to late-arriving data) - Clustering: Add clustering on sensor_id and sensor_type (helps \"by sensor type\" queries) - Result: Total cost ~$2,800/month (well under budget)</p> <p>Advanced considerations shown: - \"Hourly partitioning creates 17,520 partitions/year. BigQuery has 4,000 partition limit per table. Use partition expiration or switch to daily + clustering.\" - \"Consider BigQuery flat-rate pricing ($20,000/month) if your workload exceeds $6,000/month on-demand.\"</p> <p>Expected learning: Students understand how to calculate costs, choose partition granularity based on query patterns, and recognize partition limits and alternative pricing models.</p>"},{"location":"sims/bigquery-partitioning-cost/spec/#technical-specification","title":"Technical Specification","text":""},{"location":"sims/bigquery-partitioning-cost/spec/#technology","title":"Technology","text":"<ul> <li>Library: p5.js for table block visualization and animations</li> <li>Canvas: Responsive, min 700px width \u00d7 500px height</li> <li>Frame Rate: 60fps for smooth scanning animations</li> <li>Data: Partition metadata and cost formulas defined in JSON</li> </ul>"},{"location":"sims/bigquery-partitioning-cost/spec/#implementation-notes","title":"Implementation Notes","text":"<p>Mobile Considerations: - Screens &lt; 768px: Stack left/right panels vertically - Table blocks: Show representative sample (e.g., 100 blocks) instead of 5,000 on small screens - Touch-friendly: Large buttons, swipeable query presets - Simplified animations (instant pruning highlight instead of gradual fade)</p> <p>Accessibility: - Keyboard navigation:   - Tab through controls   - Arrow keys to adjust date range   - Enter to run query   - Escape to clear results - Screen reader announces:   - Partition strategy: \"Daily partitioning selected. 1,095 partitions created based on order_date column.\"   - Query execution: \"Query started. Scanning 35 gigabytes across 7 partitions. 1,088 partitions pruned. Cost: 17 cents. Query completed.\"   - Cost comparison: \"143 times cheaper than unpartitioned. Savings: $24.82 per query.\" - High contrast mode: Bold partition boundaries, distinct colors for scanned vs pruned - Text alternatives: Cost table showing all metrics in accessible format</p> <p>Performance: - For large tables (&gt;1TB), show representative visualization (e.g., 1 block = 10GB) - Use canvas instancing for rendering thousands of identical blocks - Debounce slider changes (500ms before recalculating) - Precompute partition metadata for instant lookups during query execution</p>"},{"location":"sims/bigquery-partitioning-cost/spec/#data-requirements","title":"Data Requirements","text":"<p>Table Metadata (JSON format):</p> <pre><code>{\n  \"table\": {\n    \"name\": \"sales\",\n    \"size_bytes\": 5497558138880,\n    \"size_human\": \"5TB\",\n    \"row_count\": 2190000000,\n    \"date_range\": {\n      \"start\": \"2021-01-01\",\n      \"end\": \"2024-01-21\"\n    },\n    \"schema\": [\n      {\"name\": \"order_id\", \"type\": \"INT64\"},\n      {\"name\": \"customer_id\", \"type\": \"INT64\"},\n      {\"name\": \"product_id\", \"type\": \"INT64\"},\n      {\"name\": \"order_date\", \"type\": \"DATE\"},\n      {\"name\": \"order_timestamp\", \"type\": \"TIMESTAMP\"},\n      {\"name\": \"amount\", \"type\": \"FLOAT64\"}\n    ]\n  },\n  \"partition_strategies\": {\n    \"none\": {\n      \"partitions\": 1,\n      \"partition_size_avg_gb\": 5000,\n      \"metadata_cost\": 0\n    },\n    \"daily\": {\n      \"partitions\": 1095,\n      \"partition_size_avg_gb\": 4.57,\n      \"metadata_cost\": 0.00\n    },\n    \"monthly\": {\n      \"partitions\": 36,\n      \"partition_size_avg_gb\": 138.89,\n      \"metadata_cost\": 0\n    },\n    \"yearly\": {\n      \"partitions\": 3,\n      \"partition_size_avg_gb\": 1666.67,\n      \"metadata_cost\": 0\n    },\n    \"hourly\": {\n      \"partitions\": 26280,\n      \"partition_size_avg_gb\": 0.19,\n      \"metadata_cost\": 0,\n      \"note\": \"Exceeds 4,000 partition limit. Requires partition expiration.\"\n    }\n  },\n  \"pricing\": {\n    \"on_demand_per_tb\": 5.00,\n    \"flat_rate_monthly\": 20000,\n    \"storage_per_gb_month\": 0.02,\n    \"free_tier_monthly_gb\": 10240\n  }\n}\n</code></pre> <p>Query Presets: <pre><code>{\n  \"query_presets\": {\n    \"last_7_days\": {\n      \"name\": \"Last 7 Days\",\n      \"sql\": \"SELECT * FROM sales WHERE order_date &gt;= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)\",\n      \"date_filter\": {\"days\": 7},\n      \"description\": \"Common dashboard query\"\n    },\n    \"last_month\": {\n      \"name\": \"Last Month\",\n      \"sql\": \"SELECT * FROM sales WHERE order_date &gt;= DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH)\",\n      \"date_filter\": {\"days\": 30},\n      \"description\": \"Monthly reporting\"\n    },\n    \"specific_date\": {\n      \"name\": \"Specific Date\",\n      \"sql\": \"SELECT * FROM sales WHERE order_date = '2024-01-15'\",\n      \"date_filter\": {\"days\": 1},\n      \"description\": \"Single day analysis\"\n    },\n    \"year_to_date\": {\n      \"name\": \"Year-to-Date\",\n      \"sql\": \"SELECT * FROM sales WHERE order_date &gt;= '2024-01-01'\",\n      \"date_filter\": {\"days\": 21},\n      \"description\": \"YTD aggregations\"\n    },\n    \"full_table\": {\n      \"name\": \"Full Table Scan\",\n      \"sql\": \"SELECT SUM(amount) FROM sales\",\n      \"date_filter\": null,\n      \"description\": \"Lifetime calculations\"\n    }\n  }\n}\n</code></pre></p> <p>Cost Calculation Logic: <pre><code>function calculateQueryCost(tableSizeGB, partitionStrategy, queryDateFilter) {\n  const onDemandRate = 5.00; // $ per TB\n\n  if (!queryDateFilter) {\n    // Full table scan\n    return {\n      dataScannedGB: tableSizeGB,\n      cost: (tableSizeGB / 1024) * onDemandRate,\n      partitionsPruned: 0\n    };\n  }\n\n  // Calculate partitions scanned based on strategy\n  const daysQueried = queryDateFilter.days;\n  let dataScannedGB;\n\n  switch(partitionStrategy) {\n    case 'daily':\n      dataScannedGB = (tableSizeGB / 1095) * daysQueried;\n      break;\n    case 'monthly':\n      const monthsQueried = Math.ceil(daysQueried / 30);\n      dataScannedGB = (tableSizeGB / 36) * monthsQueried;\n      break;\n    case 'yearly':\n      const yearsQueried = Math.ceil(daysQueried / 365);\n      dataScannedGB = (tableSizeGB / 3) * yearsQueried;\n      break;\n    default: // unpartitioned\n      dataScannedGB = tableSizeGB;\n  }\n\n  return {\n    dataScannedGB: dataScannedGB,\n    cost: (dataScannedGB / 1024) * onDemandRate,\n    partitionsPruned: calculatePartitionsPruned(partitionStrategy, daysQueried)\n  };\n}\n</code></pre></p>"},{"location":"sims/bigquery-partitioning-cost/spec/#assessment-integration","title":"Assessment Integration","text":"<p>After using this MicroSim, students should be able to answer:</p> <ol> <li>Quiz Question: \"BigQuery charges $5 per TB of data scanned. A query scans 250GB. What is the cost?\"</li> <li>A) $0.25</li> <li>B) $1.25 \u2713</li> <li>C) $5.00</li> <li>D) $25.00</li> </ol> <p>(Calculation: 250GB / 1024GB per TB \u00d7 $5 = $1.22, rounded to $1.25)</p> <ol> <li>Conceptual Question: \"Why does partitioning reduce query costs in BigQuery, but not in traditional databases like PostgreSQL?\"</li> <li> <p>Expected answer: BigQuery uses columnar storage and charges based on data scanned. Partitioning allows partition pruning\u2014skipping entire partitions based on metadata, reducing data scanned and thus cost. Traditional databases charge based on compute time or don't separate storage/compute costs, so partitioning helps performance but doesn't directly reduce costs.</p> </li> <li> <p>Design Question: \"You have a 10TB table with queries that filter by user_id (10M unique users). Would partitioning on user_id reduce costs? Why or why not?\"</p> </li> <li> <p>Expected answer: No. BigQuery only supports partitioning on DATE, TIMESTAMP, DATETIME, or INTEGER (limited range). user_id with 10M values would create too many partitions (4,000 limit). Instead, use clustering on user_id, which sorts data without partition metadata and still improves filtering performance.</p> </li> <li> <p>Scenario Question: \"Your daily dashboard query scans 2TB and costs $10. It runs every 5 minutes (288 times/day = $2,880/day). You have a $10,000/month budget. What are your options?\"</p> </li> <li>Expected answer:<ul> <li>Option 1: Optimize with partitioning/clustering to reduce data scanned (best)</li> <li>Option 2: Switch to BigQuery flat-rate pricing ($20,000/month for unlimited queries, not viable with $10K budget)</li> <li>Option 3: Cache results (BI Engine or materialized views) to avoid repeated scans</li> <li>Option 4: Reduce query frequency (dashboard refreshes every 30 minutes instead of 5)</li> <li>Option 5: Pre-aggregate data in smaller summary tables</li> </ul> </li> </ol>"},{"location":"sims/bigquery-partitioning-cost/spec/#extension-ideas-optional","title":"Extension Ideas (Optional)","text":"<ul> <li>Clustering Visualization: Show how clustering orders data within partitions, further reducing scan cost</li> <li>Partition Expiration: Demonstrate auto-deletion of old partitions to manage storage costs</li> <li>Materialized Views: Show how pre-aggregating data into smaller materialized view reduces query costs</li> <li>BI Engine Caching: Visualize how frequently-run queries get cached, eliminating scan costs for repeated queries</li> <li>Time-Unit Column Partitioning: Compare partition by DATE column vs partition by ingestion time (_PARTITIONTIME)</li> <li>Integer Range Partitioning: Show partitioning by integer columns (e.g., customer_id ranges 0-999, 1000-1999) for non-temporal data</li> <li>Partition Decorator Queries: Demonstrate querying specific partitions: <code>SELECT * FROM table$20240115</code></li> <li>Flat-Rate vs On-Demand Calculator: Let students input workload characteristics, calculate which pricing model is cheaper</li> <li>Real-World Case Studies: Show actual company examples (Spotify saved 80% costs with partitioning, etc.)</li> <li>Cross-Region Queries: Visualize data transfer costs when querying tables in different regions</li> </ul> <p>Target Learning Outcome: Students understand that BigQuery costs are determined by data scanned, not returned, and that partitioning reduces costs by enabling partition pruning. Students can choose appropriate partition granularity based on query patterns and calculate cost savings for real-world scenarios.</p>"},{"location":"sims/database-normalization/spec/","title":"MicroSim: Database Normalization Journey","text":""},{"location":"sims/database-normalization/spec/#overview","title":"Overview","text":""},{"location":"sims/database-normalization/spec/#concept-visualized","title":"Concept Visualized","text":"<ul> <li>Concept: Database normalization (1NF, 2NF, 3NF) and update/insert/delete anomalies</li> <li>Learning Goal: Students will understand why normalization matters by starting with a messy denormalized e-commerce table, observing data anomalies, and stepping through normalization forms to see problems eliminated</li> <li>Difficulty: Intermediate</li> <li>Chapter: Week 3-4 - Data Storage &amp; Modeling (Relational Databases)</li> </ul>"},{"location":"sims/database-normalization/spec/#the-aha-moment","title":"The \"Aha\" Moment","text":"<p>When the student tries to update a customer's email address in the denormalized table, they see they must update 15 different order rows. They miss one row and the system now has inconsistent data (update anomaly). Then they normalize to 3NF and try the same update\u2014this time it's a single row change, and all orders automatically reflect the new email. This demonstrates: normalization isn't just theory, it prevents real bugs.</p>"},{"location":"sims/database-normalization/spec/#interface-design","title":"Interface Design","text":""},{"location":"sims/database-normalization/spec/#layout","title":"Layout","text":"<ul> <li>Top Panel (70% height): Table visualization showing current schema state with sample data</li> <li>Right Sidebar (20% width): Controls and normalization progress tracker</li> <li>Bottom Panel (30% height): Anomaly detection panel and interactive challenges</li> </ul>"},{"location":"sims/database-normalization/spec/#controls-right-sidebar","title":"Controls (Right Sidebar)","text":"Control Type Range/Options Default Effect on Visualization Normalization Level step progress [0NF, 1NF, 2NF, 3NF] 0NF Advances through normalization forms, splitting tables Show Data Issues toggle on/off on Highlights anomalies (duplicate data, inconsistencies) Data Operation button group [\"Insert Order\", \"Update Customer\", \"Delete Product\"] - Triggers interactive scenarios demonstrating anomalies Next Step button - - Advances to next normal form with explanation Auto-Play toggle on/off off Automatically steps through normalization with 3-second pauses Highlight Keys toggle on/off on Shows primary keys (gold) and foreign keys (blue) Show Dependencies toggle on/off off Displays functional dependencies with arrows <p>Additional UI Elements: - Normalization Progress Bar: Visual showing 0NF \u2192 1NF \u2192 2NF \u2192 3NF with checkmarks for completed steps - Anomaly Counter: Badges showing count of each anomaly type (update, insert, delete) - Info Box: Explains current normal form definition and what problem it solves - Checklist: Shows normalization criteria being checked (e.g., \"\u2713 Atomic values\", \"\u2717 No partial dependencies\")</p>"},{"location":"sims/database-normalization/spec/#visualization-details-top-panel","title":"Visualization Details (Top Panel)","text":"<p>What is Displayed:</p> <p>0NF (Unnormalized) State: - Single Table: <code>ORDERS</code> with repeating groups and denormalized data - Columns: <code>order_id, order_date, customer_name, customer_email, customer_phone, products, quantities, prices, total, shipping_address</code> - Sample Row Data (visible grid): <pre><code>| order_id | order_date | customer_name | customer_email | products | quantities | prices |\n|----------|------------|---------------|----------------|----------|------------|---------|\n| 1001     | 2024-01-15 | Alice Smith   | alice@email.com | Laptop,Mouse,Keyboard | 1,2,1 | 999.99,25.00,75.00 |\n</code></pre> - Visual Indicators:   - Red wavy underlines on multi-valued cells (products, quantities, prices)   - Yellow highlight on duplicate data (customer details repeated across multiple orders)   - Icon badges: \"Update Anomaly \u00d73\", \"Insert Anomaly \u00d72\", \"Delete Anomaly \u00d71\"</p> <p>1NF (First Normal Form) State: - Table splits horizontally: Each product becomes separate row - New Schema: <code>ORDERS</code> with columns: <code>order_id, order_line_id, order_date, customer_name, customer_email, customer_phone, product_name, quantity, price, total, shipping_address</code> - Sample Data: <pre><code>| order_id | order_line_id | customer_name | customer_email | product_name | quantity | price |\n|----------|---------------|---------------|----------------|--------------|----------|-------|\n| 1001     | 1             | Alice Smith   | alice@email.com | Laptop       | 1        | 999.99 |\n| 1001     | 2             | Alice Smith   | alice@email.com | Mouse        | 2        | 25.00  |\n| 1001     | 3             | Alice Smith   | alice@email.com | Keyboard     | 1        | 75.00  |\n</code></pre> - Visual Changes:   - Red underlines removed (atomic values achieved)   - Yellow highlights remain (customer data still duplicated)   - Checkmark on \"Atomic Values\" criterion   - Anomaly counter: Update anomalies persist (still 3)</p> <p>2NF (Second Normal Form) State: - Tables split by subject: Order header separated from order lines, customer separated - New Schema:   - <code>ORDERS</code>: <code>order_id, customer_id, order_date, shipping_address, total</code>   - <code>ORDER_LINES</code>: <code>order_line_id, order_id, product_name, quantity, price</code>   - <code>CUSTOMERS</code>: <code>customer_id, customer_name, customer_email, customer_phone</code> - Visual Layout: Three tables side-by-side with arrow connections showing foreign keys - Sample Data: <pre><code>CUSTOMERS:\n| customer_id | customer_name | customer_email |\n|-------------|---------------|----------------|\n| 101         | Alice Smith   | alice@email.com |\n\nORDERS:\n| order_id | customer_id | order_date | total |\n|----------|-------------|------------|-------|\n| 1001     | 101         | 2024-01-15 | 1099.99 |\n\nORDER_LINES:\n| order_line_id | order_id | product_name | quantity | price |\n|---------------|----------|--------------|----------|-------|\n| 1             | 1001     | Laptop       | 1        | 999.99 |\n</code></pre> - Visual Changes:   - Yellow highlights removed from customer data (no more duplication)   - Arrows connecting tables: ORDERS.customer_id \u2192 CUSTOMERS.customer_id   - Checkmark on \"No Partial Dependencies\" criterion   - Anomaly counter: Update anomalies reduced to 1 (product data still duplicated)</p> <p>3NF (Third Normal Form) State: - Further decomposition: Products separated into own table - New Schema:   - <code>ORDERS</code>: <code>order_id, customer_id, order_date, shipping_address, total</code>   - <code>ORDER_LINES</code>: <code>order_line_id, order_id, product_id, quantity, price_at_purchase</code>   - <code>CUSTOMERS</code>: <code>customer_id, customer_name, customer_email, customer_phone</code>   - <code>PRODUCTS</code>: <code>product_id, product_name, category, current_price</code> - Visual Layout: Four tables with relationship lines - Sample Data: <pre><code>PRODUCTS:\n| product_id | product_name | category | current_price |\n|------------|--------------|----------|---------------|\n| 501        | Laptop       | Electronics | 999.99     |\n\nORDER_LINES:\n| order_line_id | order_id | product_id | quantity | price_at_purchase |\n|---------------|----------|------------|----------|-------------------|\n| 1             | 1001     | 501        | 1        | 999.99            |\n</code></pre> - Visual Changes:   - All yellow highlights removed (full normalization)   - Checkmark on \"No Transitive Dependencies\" criterion   - Anomaly counter: All anomalies eliminated (green checkmarks)   - Foreign key arrows: ORDER_LINES.product_id \u2192 PRODUCTS.product_id</p> <p>Color Coding: - Gold: Primary keys (key icon) - Blue: Foreign keys (arrow icon) - Red: Violations of current normal form (wavy underlines) - Yellow: Data redundancy (highlighting duplicate values) - Green: Correctly normalized data</p> <p>Animation Behaviors: - Normalization Transition: Smooth 1.5-second animation showing:   1. Highlight columns being moved (fade to yellow)   2. Table splits apart (columns slide into new table)   3. Foreign key column appears in source table (fades in)   4. Arrow animates connecting tables   5. Sample data repopulates in both tables - Anomaly Demonstration: When user triggers data operation:   - Affected rows pulse red   - Update operation: All duplicate rows highlight, then one-by-one update (showing manual effort)   - Insert anomaly: Red X appears showing operation blocked   - Delete anomaly: Row deletion causes unintended data loss (highlighted in red)</p> <p>Visual Feedback: - Hover states: Columns show functional dependency tooltip: \"customer_email \u2192 customer_id (dependent)\" - Click on cell: Shows all duplicate instances of that value across table (yellow glow) - Anomaly icon hover: Explains specific anomaly with example</p>"},{"location":"sims/database-normalization/spec/#educational-flow","title":"Educational Flow","text":""},{"location":"sims/database-normalization/spec/#step-1-default-state-0nf-unnormalized","title":"Step 1: Default State (0NF - Unnormalized)","text":"<p>Student sees messy <code>ORDERS</code> table with: - 8 rows of sample e-commerce orders - Multi-valued cells: <code>products = \"Laptop,Mouse,Keyboard\"</code> - Duplicate customer data: Alice Smith's email appears in 3 different order rows - Products listed with denormalized attributes</p> <p>Info panel explains: \"This is how many spreadsheets and early databases look. It works for small data, but causes serious problems at scale. Let's see what breaks.\"</p> <p>Anomaly indicators show: - Update Anomaly \u00d73 (customer data duplicated) - Insert Anomaly \u00d71 (can't add customer without order) - Delete Anomaly \u00d71 (deleting last order loses customer info)</p> <p>This shows the baseline: functional but problematic structure.</p>"},{"location":"sims/database-normalization/spec/#step-2-first-interaction-experience-update-anomaly","title":"Step 2: First Interaction - Experience Update Anomaly","text":"<p>Prompt: \"Alice Smith changed her email to alice.smith@newemail.com. Click 'Update Customer' to change it.\"</p> <p>Student clicks \"Update Customer\" button.</p> <p>Interactive Challenge Appears: - \"Select all rows that need to be updated\" - Student must click each row containing Alice's email - 3 rows highlight as student clicks - One row is scrolled below visible area (trap!) - Student clicks \"Apply Update\"</p> <p>Result: - 2 rows update successfully (email changes) - 1 row remains with old email (the one they missed) - Red alert appears: \"\u26a0\ufe0f UPDATE ANOMALY: Inconsistent data! Alice now has 2 different emails in the system.\" - Visual shows: Two rows with alice@email.com, one with alice.smith@newemail.com - Data corruption highlighted in red</p> <p>Info panel explains: \"This is an update anomaly. Because customer data is duplicated across multiple rows, updates are error-prone. Miss one row and you have inconsistent data\u2014different orders show different contact info for the same customer!\"</p> <p>Prompt: \"Click 'Next Step' to normalize to 1NF and see if it helps.\"</p> <p>What it teaches: Denormalization forces redundant updates that are error-prone. Students feel the pain of maintaining duplicate data.</p>"},{"location":"sims/database-normalization/spec/#step-3-first-normal-form-1nf","title":"Step 3: First Normal Form (1NF)","text":"<p>Student clicks \"Next Step\". Animation plays: - Multi-valued cells (products, quantities, prices) split into separate rows - Table goes from 8 rows to 23 rows (each product gets own row) - Atomic values achieved (checkmark)</p> <p>Info panel explains: \"First Normal Form (1NF) requires: 1. Atomic values (no lists in cells) \u2713 2. Each row must be unique \u2713</p> <p>We've eliminated multi-valued attributes. But notice customer data is still duplicated\u2014update anomalies persist.\"</p> <p>Prompt: \"Try the update challenge again in 1NF.\"</p> <p>Student clicks \"Update Customer\" in 1NF state.</p> <p>Result: Same problem\u2014now customer email appears in even MORE rows (23 instead of 8) because of the split. Anomaly is worse!</p> <p>Key insight: 1NF is necessary but not sufficient. Merely making data atomic doesn't solve redundancy.</p>"},{"location":"sims/database-normalization/spec/#step-4-second-normal-form-2nf","title":"Step 4: Second Normal Form (2NF)","text":"<p>Prompt: \"Click 'Next Step' to normalize to 2NF. We'll separate customers from orders.\"</p> <p>Student advances to 2NF. Animation shows: - Customer columns (name, email, phone) slide out into new CUSTOMERS table - customer_id column appears in ORDERS table - Foreign key arrow connects them - Customer data now appears once per customer</p> <p>Info panel explains: \"Second Normal Form (2NF) requires: 1. Must be in 1NF \u2713 2. No partial dependencies (non-key attributes fully depend on entire primary key) \u2713</p> <p>We've separated customers from orders. Each customer's data exists in exactly one place.\"</p> <p>Prompt: \"Now try updating Alice's email again.\"</p> <p>Student clicks \"Update Customer\" in 2NF state.</p> <p>Interactive Challenge: - \"Update Alice's email in the CUSTOMERS table\" - Student clicks single row in CUSTOMERS table - Clicks \"Apply Update\"</p> <p>Result: - Email updates in 1 row in CUSTOMERS table - Visual shows: All 3 related ORDERS automatically reflect new email (animated foreign key link glows) - Green checkmark: \"\u2713 UPDATE SUCCESS: Changed 1 row, all orders updated automatically via relationship\"</p> <p>Info panel explains: \"By normalizing to 2NF, customer data exists in one place. Update once, and all references are automatically correct. No more update anomalies for customer data!\"</p> <p>Anomaly counter updates: Update Anomaly \u00d71 (reduced from 3), Insert Anomaly \u00d70, Delete Anomaly \u00d70</p> <p>What it teaches: Proper normalization eliminates update anomalies through single source of truth. Foreign keys maintain relationships without duplication.</p>"},{"location":"sims/database-normalization/spec/#step-5-third-normal-form-3nf","title":"Step 5: Third Normal Form (3NF)","text":"<p>Info panel: \"One problem remains: Product information (name, category, price) is duplicated in ORDER_LINES. If we change a product's category, we'd have the same update anomaly.\"</p> <p>Prompt: \"Click 'Next Step' for full normalization to 3NF.\"</p> <p>Student advances to 3NF. Animation shows: - Product details slide out into PRODUCTS table - product_id replaces product_name in ORDER_LINES - Note: price_at_purchase stays in ORDER_LINES (explained as historical data)</p> <p>Info panel explains: \"Third Normal Form (3NF) requires: 1. Must be in 2NF \u2713 2. No transitive dependencies (non-key attributes depend only on primary key, not on other non-key attributes) \u2713</p> <p>Products are now separate. Historical price (price_at_purchase) stays in ORDER_LINES because it represents the price at that specific time, not current price.\"</p> <p>Prompt: \"Try all three data operations now that we're in 3NF.\"</p> <p>Student tests: 1. Update Customer Email: Single row change \u2713 2. Insert New Customer: Can add customer without requiring an order \u2713 3. Delete Product: Can delete product without losing order history (product_id preserved in ORDER_LINES) \u2713</p> <p>All anomalies eliminated!</p>"},{"location":"sims/database-normalization/spec/#step-6-challenge","title":"Step 6: Challenge","text":"<p>Present scenario: \"Your startup's MVP uses a single denormalized user_activities table with 50,000 rows: - <code>user_id, user_name, user_email, user_subscription_tier, activity_type, activity_timestamp, device_name, device_os, device_browser</code></p> <p>You've noticed: - When users change email, customer support must update 100+ rows per user - 20 users have inconsistent emails in the system from incomplete updates - You can't add new users until they perform an activity - Your database has grown to 2GB despite only 5,000 users</p> <p>Normalize this to 3NF. How many tables would you create? What are they?\"</p> <p>Student interaction: Blank workspace appears with denormalized table. Student must: 1. Identify entities: Users, Activities, Devices 2. Drag columns into appropriate tables 3. Define primary keys 4. Connect tables with foreign keys</p> <p>Expected solution: <pre><code>USERS (user_id PK, user_name, user_email, subscription_tier)\nDEVICES (device_id PK, device_name, device_os, device_browser)\nACTIVITIES (activity_id PK, user_id FK, device_id FK, activity_type, activity_timestamp)\n</code></pre></p> <p>Validation: - Check that each table has no redundant data - Check that all functional dependencies are proper - Show metrics: \"Original: 1 table, 50K rows, 2GB. Normalized: 3 tables, 55K rows total, 1.2GB (40% savings)\"</p> <p>Expected learning: Students can identify entities, eliminate redundancy, and apply normalization principles to real-world scenarios.</p>"},{"location":"sims/database-normalization/spec/#technical-specification","title":"Technical Specification","text":""},{"location":"sims/database-normalization/spec/#technology","title":"Technology","text":"<ul> <li>Library: p5.js for table rendering and animations</li> <li>Canvas: Responsive, min 800px width \u00d7 600px height</li> <li>Frame Rate: 60fps for smooth table transformations</li> <li>Data: Sample database records in JSON, normalization rules defined declaratively</li> </ul>"},{"location":"sims/database-normalization/spec/#implementation-notes","title":"Implementation Notes","text":"<p>Mobile Considerations: - Screens &lt; 768px: Tables stack vertically, foreign key arrows become horizontal - Touch gestures: Swipe between normalization levels - Simplified animations on mobile (instant table splits instead of slide animations) - Accordion view for multiple tables (expand/collapse)</p> <p>Accessibility: - Keyboard navigation:   - Tab through cells in table   - Arrow keys to navigate within table   - Spacebar to trigger data operations   - N key to advance to Next normalization level - Screen reader announces:   - Table structure: \"Orders table with 8 rows, 12 columns. Contains duplicate customer data in 3 rows.\"   - Normalization step: \"Advancing to Second Normal Form. Separating customers into new table. Customer data duplicated 0 times.\"   - Anomaly detection: \"Update anomaly detected: Customer email exists in 3 rows. Must update all to maintain consistency.\" - High contrast mode: Bold borders, remove background colors - Text alternatives for all visual indicators (anomaly badges described in text)</p> <p>Performance: - Render only visible rows (virtual scrolling for tables &gt; 50 rows) - Use CSS Grid for table layout (more performant than canvas for large tables) - Debounce interactive operations (prevent double-clicks) - Preload animation states for smooth transitions</p>"},{"location":"sims/database-normalization/spec/#data-requirements","title":"Data Requirements","text":"<p>Sample Dataset (JSON format):</p> <pre><code>{\n  \"unnormalized_data\": [\n    {\n      \"order_id\": 1001,\n      \"order_date\": \"2024-01-15\",\n      \"customer_name\": \"Alice Smith\",\n      \"customer_email\": \"alice@email.com\",\n      \"customer_phone\": \"555-0101\",\n      \"products\": \"Laptop,Mouse,Keyboard\",\n      \"quantities\": \"1,2,1\",\n      \"prices\": \"999.99,25.00,75.00\",\n      \"total\": 1099.99,\n      \"shipping_address\": \"123 Main St, Boston, MA\"\n    },\n    {\n      \"order_id\": 1002,\n      \"order_date\": \"2024-01-16\",\n      \"customer_name\": \"Alice Smith\",\n      \"customer_email\": \"alice@email.com\",\n      \"customer_phone\": \"555-0101\",\n      \"products\": \"Monitor\",\n      \"quantities\": \"1\",\n      \"prices\": \"299.99\",\n      \"total\": 299.99,\n      \"shipping_address\": \"123 Main St, Boston, MA\"\n    },\n    {\n      \"order_id\": 1003,\n      \"order_date\": \"2024-01-17\",\n      \"customer_name\": \"Bob Johnson\",\n      \"customer_email\": \"bob@email.com\",\n      \"customer_phone\": \"555-0102\",\n      \"products\": \"Keyboard,Mouse\",\n      \"quantities\": \"1,1\",\n      \"prices\": \"75.00,25.00\",\n      \"total\": 100.00,\n      \"shipping_address\": \"456 Oak Ave, Seattle, WA\"\n    }\n  ],\n  \"anomalies\": [\n    {\n      \"type\": \"update\",\n      \"description\": \"Customer email duplicated across multiple orders\",\n      \"affected_rows\": [1001, 1002],\n      \"entity\": \"customer\",\n      \"solution_normal_form\": \"2NF\"\n    },\n    {\n      \"type\": \"insert\",\n      \"description\": \"Cannot add customer without placing an order\",\n      \"entity\": \"customer\",\n      \"solution_normal_form\": \"2NF\"\n    },\n    {\n      \"type\": \"delete\",\n      \"description\": \"Deleting last order removes customer information\",\n      \"affected_entities\": [\"customer\", \"order\"],\n      \"solution_normal_form\": \"2NF\"\n    },\n    {\n      \"type\": \"update\",\n      \"description\": \"Product details duplicated across order lines\",\n      \"affected_rows\": \"multiple\",\n      \"entity\": \"product\",\n      \"solution_normal_form\": \"3NF\"\n    }\n  ],\n  \"normalization_rules\": {\n    \"1NF\": {\n      \"definition\": \"Each cell contains atomic (indivisible) values, and each row is unique\",\n      \"transformations\": [\n        \"Split multi-valued attributes into separate rows\",\n        \"Create composite primary key (order_id, order_line_id)\"\n      ]\n    },\n    \"2NF\": {\n      \"definition\": \"In 1NF and all non-key attributes fully depend on the entire primary key\",\n      \"transformations\": [\n        \"Extract customers into CUSTOMERS table\",\n        \"Add customer_id foreign key to ORDERS table\",\n        \"Remove partial dependencies\"\n      ]\n    },\n    \"3NF\": {\n      \"definition\": \"In 2NF and no transitive dependencies (non-key attributes depend only on primary key)\",\n      \"transformations\": [\n        \"Extract products into PRODUCTS table\",\n        \"Add product_id foreign key to ORDER_LINES table\",\n        \"Keep price_at_purchase in ORDER_LINES (represents historical fact, not product attribute)\"\n      ]\n    }\n  }\n}\n</code></pre> <p>Functional Dependencies: <pre><code>0NF state:\n  order_id \u2192 order_date, customer_name, customer_email, total, shipping_address, products\n  customer_email \u2192 customer_name, customer_phone (transitive)\n  products \u2192 prices (multi-valued dependency)\n\n2NF state:\n  order_id \u2192 customer_id, order_date, total, shipping_address\n  customer_id \u2192 customer_name, customer_email, customer_phone\n  (order_id, order_line_id) \u2192 product_name, quantity, price\n\n3NF state:\n  order_id \u2192 customer_id, order_date, total, shipping_address\n  customer_id \u2192 customer_name, customer_email, customer_phone\n  (order_id, order_line_id) \u2192 product_id, quantity, price_at_purchase\n  product_id \u2192 product_name, category, current_price\n</code></pre></p>"},{"location":"sims/database-normalization/spec/#assessment-integration","title":"Assessment Integration","text":"<p>After using this MicroSim, students should be able to answer:</p> <ol> <li>Quiz Question: \"A table is in 2NF if:\"</li> <li>A) It has atomic values</li> <li>B) It is in 1NF and has no partial dependencies \u2713</li> <li>C) It has no transitive dependencies</li> <li> <p>D) It has a primary key</p> </li> <li> <p>Conceptual Question: \"Explain why denormalized databases suffer from update anomalies. Give a concrete example.\"</p> </li> <li> <p>Expected answer: Update anomalies occur when the same fact is stored in multiple places. If customer email is duplicated across 10 order rows, updating it requires changing 10 rows. Missing one row creates inconsistent data. Normalization eliminates this by storing each fact once.</p> </li> <li> <p>Design Question: \"You have a table: <code>EMPLOYEES (emp_id, emp_name, dept_id, dept_name, dept_location)</code>. What normal form is this? What's the problem? How would you normalize it?\"</p> </li> <li> <p>Expected answer:</p> <ul> <li>Violates 3NF (transitive dependency: emp_id \u2192 dept_id \u2192 dept_name, dept_location)</li> <li>Problem: Department info duplicated for every employee. Changing dept_name requires updating many employee rows.</li> <li>Solution: Split into EMPLOYEES (emp_id, emp_name, dept_id) and DEPARTMENTS (dept_id, dept_name, dept_location)</li> </ul> </li> <li> <p>Trade-off Question: \"When might you intentionally denormalize a properly normalized database?\"</p> </li> <li>Expected answer:<ul> <li>Data warehousing (star schema for query performance)</li> <li>Read-heavy applications where query speed matters more than update efficiency</li> <li>Caching frequently joined data</li> <li>But: Must accept update anomaly risks and implement careful update procedures</li> </ul> </li> </ol>"},{"location":"sims/database-normalization/spec/#extension-ideas-optional","title":"Extension Ideas (Optional)","text":"<ul> <li>Boyce-Codd Normal Form (BCNF): Show edge cases where 3NF isn't sufficient (overlapping candidate keys)</li> <li>Fourth Normal Form (4NF): Demonstrate multi-valued dependencies</li> <li>Denormalization Calculator: Show read vs write operation costs, let students decide when denormalization is justified</li> <li>Real Database Connection: Load actual database schema, detect normalization violations</li> <li>Performance Comparison: Show query execution times for normalized vs denormalized schemas under different workloads</li> <li>Migration Tool: Generate ALTER TABLE scripts to normalize existing database</li> <li>Anomaly Sandbox: Free-form mode where students create any schema and system detects all possible anomalies</li> <li>Historical Perspective: Show how old databases evolved (hierarchical \u2192 network \u2192 relational) and why normalization matters</li> <li>NoSQL Connection: Explain why document databases (MongoDB) intentionally denormalize, and when that's appropriate</li> </ul> <p>Target Learning Outcome: Students understand that normalization eliminates data anomalies by removing redundancy, can identify which normal form a table satisfies, can normalize tables to 3NF, and recognize when denormalization is a justified trade-off.</p>"},{"location":"sims/git-merge-rebase/spec/","title":"MicroSim: Git Merge vs Rebase Interactive","text":""},{"location":"sims/git-merge-rebase/spec/#overview","title":"Overview","text":""},{"location":"sims/git-merge-rebase/spec/#concept-visualized","title":"Concept Visualized","text":"<ul> <li>Concept: Merge vs Rebase (merge-vs-rebase), Git Merge (git-merge), Git Rebase (git-rebase)</li> <li>Learning Goal: Students understand the fundamental difference between merge and rebase by manipulating branch histories and observing how commit graphs diverge, helping them choose the right integration strategy</li> <li>Difficulty: Intermediate</li> <li>Chapter: Week 1-2 (Foundations &amp; Environment Setup)</li> </ul>"},{"location":"sims/git-merge-rebase/spec/#the-aha-moment","title":"The \"Aha\" Moment","text":"<p>When students click \"Merge\" vs \"Rebase\" on the same starting scenario, they see two completely different commit graph outcomes side-by-side: merge preserves history with a merge commit, while rebase creates a linear history by replaying commits, demonstrating why rebase makes cleaner history but merge is safer for shared branches.</p>"},{"location":"sims/git-merge-rebase/spec/#interface-design","title":"Interface Design","text":""},{"location":"sims/git-merge-rebase/spec/#layout","title":"Layout","text":"<ul> <li>Left Panel (60%): Split into two side-by-side commit graphs (Merge result | Rebase result)</li> <li>Right Panel (40%): Scenario controls and strategy comparison</li> </ul>"},{"location":"sims/git-merge-rebase/spec/#controls-right-panel","title":"Controls (Right Panel)","text":"Control Type Range/Options Default Effect Scenario dropdown \"Simple Feature\", \"Multiple Commits\", \"Conflicting Changes\" \"Simple Feature\" Loads different branch divergence scenarios Show Merge Result button N/A N/A Animates merge operation on left graph Show Rebase Result button N/A N/A Animates rebase operation on right graph Show Both button N/A N/A Animates both operations simultaneously Reset button N/A N/A Clears to initial state Speed slider 0.5x - 2x 1x Controls animation playback speed <p>Strategy Comparison Panel: - Displays side-by-side comparison table after operation:   - Commit count   - History linearity (Linear/Non-linear)   - Preserves original timestamps (Yes/No)   - Creates merge commit (Yes/No)   - Safe for shared branches (Yes/No)   - When to use (description)</p>"},{"location":"sims/git-merge-rebase/spec/#visualization-left-panel","title":"Visualization (Left Panel)","text":"<p>What is Displayed: - Two identical starting commit graphs showing:   - main branch (blue circles) with 3 commits   - feature branch (green circles) diverged after commit 2, with 2 commits - Commit nodes: circles with SHA (e.g., \"a1b2c3\"), timestamp, and commit message - Branch labels: colored tags showing \"main\" and \"feature\" - Arrows showing parent relationships</p> <p>How it Updates: - Merge animation (left side):   1. Feature branch moves toward main (1s)   2. New merge commit node appears combining both branches (0.5s)   3. Graph shows diamond pattern with merge commit having two parents   4. Branch pointers update</p> <ul> <li>Rebase animation (right side):</li> <li>Feature commits lift off the branch (0.5s)</li> <li>Commits fade to transparent showing they're being \"replayed\" (0.5s)</li> <li>New commits with different SHAs appear on top of main (1s, sequential)</li> <li>Original feature commits disappear</li> <li> <p>Linear history is formed</p> </li> <li> <p>Color coding: Blue (main), Green (feature), Purple (merge commit), Yellow (rebased commits with new SHA)</p> </li> <li>Transitions: Smooth bezier curve movements, fade in/out for commits</li> </ul>"},{"location":"sims/git-merge-rebase/spec/#educational-flow","title":"Educational Flow","text":""},{"location":"sims/git-merge-rebase/spec/#step-1-default-state","title":"Step 1: Default State","text":"<p>Student sees initial scenario: \"Simple Feature\" - main branch: commits A \u2192 B \u2192 C - feature branch: diverged at B, has commits D \u2192 E - Both graphs show identical starting state - No operations performed yet</p>"},{"location":"sims/git-merge-rebase/spec/#step-2-first-interaction","title":"Step 2: First Interaction","text":"<p>Prompt: \"Click 'Show Both' to see how merge and rebase handle this scenario differently\"</p> <p>Result: - Left graph: Shows merge creating commit F with two parents (C and E), diamond pattern - Right graph: Shows commits D' and E' replayed on top of C, linear history - Comparison table populates:   - Merge: 6 commits (A,B,C,D,E,F), Non-linear, Yes preserves timestamps, Creates merge commit   - Rebase: 5 commits (A,B,C,D',E'), Linear, No (new timestamps), No merge commit</p> <p>Students observe: Same end result, different history structure</p>"},{"location":"sims/git-merge-rebase/spec/#step-3-exploration","title":"Step 3: Exploration","text":"<p>Prompt: \"Try the 'Multiple Commits' scenario - feature branch has 5 commits instead of 2\"</p> <p>Pattern they should discover: - Merge: Always creates just one merge commit regardless of feature branch size - Rebase: Replays each commit individually, creating N new commits - Merge history shows \"what actually happened\" (when branches diverged) - Rebase history shows \"what could have happened\" (if feature was built on latest main) - SHA hashes change during rebase, showing commits are rewritten</p>"},{"location":"sims/git-merge-rebase/spec/#step-4-challenge","title":"Step 4: Challenge","text":"<p>Scenario: \"Conflicting Changes - both main and feature modified the same file\" Question: \"Your feature branch has already been pushed to GitHub and your teammate pulled it. Should you use merge or rebase to integrate with main?\"</p> <p>Expected learning: - Students should choose MERGE - Understand that rebase rewrites history (changes SHAs) - Rewriting pushed commits causes problems for collaborators - Merge is safe for shared branches, rebase for local cleanup - \"Golden rule\": Never rebase commits that have been pushed to shared branches</p>"},{"location":"sims/git-merge-rebase/spec/#technical-specification","title":"Technical Specification","text":"<ul> <li>Library: p5.js</li> <li>Canvas: Responsive, min 700px width \u00d7 400px height (350px per graph)</li> <li>Frame Rate: 30fps</li> <li>Data: Static JSON scenarios with commit graph structures   <pre><code>{\n  \"scenarios\": [\n    {\n      \"name\": \"Simple Feature\",\n      \"main\": [\"A\", \"B\", \"C\"],\n      \"feature\": {\"base\": \"B\", \"commits\": [\"D\", \"E\"]}\n    },\n    // ... more scenarios\n  ]\n}\n</code></pre></li> </ul>"},{"location":"sims/git-merge-rebase/spec/#assessment-integration","title":"Assessment Integration","text":"<p>After using this MicroSim, students should answer:</p> <ol> <li>Knowledge Check: What is the main difference between git merge and git rebase?</li> <li>a) Merge is faster than rebase</li> <li>b) Merge preserves commit history, rebase rewrites it \u2713</li> <li>c) Rebase creates a merge commit, merge doesn't</li> <li> <p>d) There is no difference, they're aliases</p> </li> <li> <p>Application: You have a local feature branch with 3 commits that you haven't pushed yet. Main branch has advanced. What should you do?</p> </li> <li>a) Always use merge to be safe</li> <li>b) Use rebase to keep history clean \u2713</li> <li>c) Delete and recreate the branch</li> <li> <p>d) Never integrate local branches</p> </li> <li> <p>Scenario-Based: Your teammate says \"I can't pull the feature branch anymore, getting conflicts on commits that were already there.\" What likely happened?</p> </li> <li>a) Someone merged main into feature</li> <li>b) Someone rebased a shared branch \u2713</li> <li>c) GitHub is experiencing issues</li> <li>d) The branch was deleted</li> </ol>"},{"location":"sims/git-merge-rebase/spec/#extension-ideas","title":"Extension Ideas","text":"<ul> <li>Add interactive conflict resolution step showing how conflicts differ between merge/rebase</li> <li>Include git log output panel showing actual Git command results</li> <li>Add \"Interactive Rebase\" mode showing commit squashing and reordering</li> <li>Show remote tracking branches and how push --force-with-lease works</li> <li>Include team collaboration scenario with multiple developers</li> <li>Add \"Undo\" operations showing git reset vs git revert</li> <li>Visualize reflog showing how to recover from rebase mistakes</li> </ul>"},{"location":"sims/git-merge-vs-rebase/spec/","title":"MicroSim: Git Merge vs Rebase Interactive","text":""},{"location":"sims/git-merge-vs-rebase/spec/#overview","title":"Overview","text":""},{"location":"sims/git-merge-vs-rebase/spec/#concept-visualized","title":"Concept Visualized","text":"<ul> <li>Concept: Git branching strategies, merge vs rebase workflows, commit history management</li> <li>Learning Goal: Students will understand the fundamental difference between merge and rebase by manipulating a visual commit graph and observing how each operation creates different history structures</li> <li>Difficulty: Intermediate</li> <li>Chapter: Week 1-2 - Foundations (Git/GitHub workflows)</li> </ul>"},{"location":"sims/git-merge-vs-rebase/spec/#the-aha-moment","title":"The \"Aha\" Moment","text":"<p>When the student clicks \"Rebase\" after seeing the branched history, they watch commits literally lift off the old base and replay one-by-one onto the new base, creating a linear history. Then they try \"Merge\" and see it preserves the branch structure with a merge commit joining the two histories. This demonstrates: rebase rewrites history linearly, merge preserves history truthfully.</p>"},{"location":"sims/git-merge-vs-rebase/spec/#interface-design","title":"Interface Design","text":""},{"location":"sims/git-merge-vs-rebase/spec/#layout","title":"Layout","text":"<ul> <li>Left Panel (60% width): Commit graph visualization showing branches, commits, and their relationships</li> <li>Right Panel (40% width): Interactive controls, scenario selector, and educational explanations</li> </ul>"},{"location":"sims/git-merge-vs-rebase/spec/#controls-right-panel","title":"Controls (Right Panel)","text":"Control Type Range/Options Default Effect on Visualization Scenario dropdown [\"Simple Feature Branch\", \"Long-Running Feature\", \"Merge Conflict Preview\", \"Multiple Contributors\"] \"Simple Feature Branch\" Loads different starting commit graphs Action button group [\"Merge\", \"Rebase\", \"Reset\"] - Triggers animated transformation of commit graph Show Command toggle on/off on Displays equivalent git commands below graph Speed slider 0.5x-2x 1x Controls animation playback speed Step Through button - - Pauses animation and allows frame-by-frame stepping History View tabs [\"Graph View\", \"Log View\", \"Timeline\"] \"Graph View\" Changes visualization style <p>Additional UI Elements: - Command Display: Shows git commands like <code>git merge feature-branch</code> or <code>git rebase main</code> - Info Panel: Explains what's happening at each animation step - Pros/Cons Table: Updates to show trade-offs of current action</p>"},{"location":"sims/git-merge-vs-rebase/spec/#visualization-details-left-panel","title":"Visualization Details (Left Panel)","text":"<p>What is Displayed:</p> <p>Graph View (Default): - Commits: Circles containing commit hash (first 7 chars: <code>a3f2c8b</code>) - Commit Metadata: Author icon, timestamp, message snippet on hover - Branches: Colored lines connecting commits   - <code>main</code> branch: Blue line   - <code>feature</code> branch: Green line   - Other branches: Purple/orange for multi-contributor scenarios - Branch Labels: Rounded rectangles with branch names at HEAD position - Commit Parents: Arrows showing parent relationships (usually pointing left/down) - Merge Commits: Diamond shape (two parents) vs regular circle (one parent) - Current HEAD: Bold outline with \"HEAD\" label</p> <p>Color Coding: - Blue: main/master branch commits - Green: feature branch commits - Yellow: Commits being moved during rebase animation - Red: Conflict indicators - Gray: Commits that will be \"abandoned\" during rebase (old versions)</p> <p>Log View: - Text-based commit log similar to <code>git log --oneline --graph</code> - ASCII art showing branch structure - Updates in real-time as operations execute</p> <p>Timeline View: - Horizontal timeline with commits as events - Shows chronological order (merge preserves, rebase changes)</p> <p>How it Updates:</p> <p>When \"Merge\" button clicked: 1. Pause (500ms) with highlight on both branch tips 2. Arrow animates from feature branch HEAD to main branch HEAD (500ms) 3. New merge commit (diamond shape) appears at convergence point (300ms) 4. Merge commit labeled with message: \"Merge branch 'feature-branch' into main\" 5. Both branches now point to merge commit 6. Fade previous branch pointers (300ms) 7. Final state: Y-shaped history preserved, feature commits retain original hashes</p> <p>When \"Rebase\" button clicked: 1. Highlight feature branch commits that will be moved (yellow glow, 500ms) 2. Show \"ghost\" versions of commits staying in place (fade to 30% opacity) 3. Animated \"lifting\" of commits off old base (800ms):    - Commits float upward with slight rotation    - Break parent connection arrows 4. Reposition commits horizontally to align with main branch tip (600ms) 5. Replay commits one-by-one onto new base (400ms each):    - Each commit \"lands\" with small bounce animation    - Hash changes (e.g., <code>a3f2c8b</code> \u2192 <code>f7e9d1a</code>) to show they're new commits    - Parent arrow connects to previous commit 6. Ghost commits fade away completely (300ms) 7. Final state: Linear history, feature branch points to tip, no merge commit</p> <p>When \"Reset\" button clicked: - Smooth reverse animation back to initial state (1 second)</p> <p>Visual Feedback: - Hover states: Commits enlarge slightly and show detailed tooltip (message, author, date, full hash) - Active states: Currently animating commits have pulsing yellow outline - Conflict indicators: If \"Merge Conflict Preview\" scenario, red exclamation mark appears on conflicting commits</p>"},{"location":"sims/git-merge-vs-rebase/spec/#educational-flow","title":"Educational Flow","text":""},{"location":"sims/git-merge-vs-rebase/spec/#step-1-default-state","title":"Step 1: Default State","text":"<p>Student sees Simple Feature Branch scenario: - <code>main</code> branch: 4 commits (A \u2192 B \u2192 C \u2192 D) - <code>feature</code> branch: Split from C, has 2 commits (C \u2192 E \u2192 F) - Current state: Two diverged branches</p> <p>Info panel shows: \"You've been working on a feature while main branch has advanced. Your branch is 2 commits ahead, 1 commit behind main. How should you integrate your changes?\"</p> <p>This shows the classic integration problem every developer faces.</p>"},{"location":"sims/git-merge-vs-rebase/spec/#step-2-first-interaction","title":"Step 2: First Interaction","text":"<p>Prompt: \"Try clicking 'Merge' to see what happens. Watch how the history changes.\"</p> <p>Student clicks Merge button.</p> <p>Result: - Animation shows merge commit (M) connecting F and D - Final graph: A \u2192 B \u2192 C \u2192 D \u2192 M                    \u2514\u2192 E \u2192 F \u2197 - History preserves that feature development happened in parallel - Command display shows: <code>git merge feature-branch</code></p> <p>Info panel explains: \"Merge creates a new commit (M) with two parents, preserving the true history. Anyone can see that feature work happened separately. This is the 'safe' option\u2014it never changes existing commits.\"</p> <p>Prompt: \"Click 'Reset' and then try 'Rebase' instead.\"</p> <p>Student resets, then clicks Rebase.</p> <p>Result: - Animation shows commits E and F lifting off, moving right, landing after D - Final graph: A \u2192 B \u2192 C \u2192 D \u2192 E' \u2192 F' (linear) - Commits E' and F' have new hashes (rewritten) - Command display shows: <code>git rebase main</code></p> <p>Info panel explains: \"Rebase replays your commits on top of main's latest state. The history is linear\u2014it looks like you started your feature work after commit D. Your original commits E and F are replaced with new versions E' and F'. Cleaner history, but history is rewritten.\"</p> <p>What it teaches: - Merge preserves true history, rebase creates clean linear history - Rebase changes commit hashes (creates new commits) - Both achieve integration, but create different history structures</p>"},{"location":"sims/git-merge-vs-rebase/spec/#step-3-exploration","title":"Step 3: Exploration","text":"<p>Prompt: \"Switch scenarios to 'Long-Running Feature' and try both approaches. Notice the difference in how messy the merge looks vs how clean the rebase looks.\"</p> <p>Student selects \"Long-Running Feature\" scenario: - <code>main</code> branch: 10 commits - <code>feature</code> branch: Split early, has 8 commits, with main advancing 6 commits since split - Many interleaved development</p> <p>Student tries merge: Sees complex graph with many crossing lines and merge commit Student tries rebase: Sees clean linear progression</p> <p>Key insight: Rebase is especially valuable for long-running branches\u2014it makes history readable. But rebase rewrites more commits (all 8 feature commits get new hashes), which is risky if others are working on the same branch.</p> <p>Prompt: \"Try 'Multiple Contributors' scenario. What happens if two people are working on the same feature branch?\"</p> <p>Student selects scenario showing shared feature branch.</p> <p>Info panel warns: \"Rebase is dangerous on shared branches! If you rebase and force-push, anyone else who has the old commits will have a 'diverged branch' problem. Only rebase local branches or after coordinating with team.\"</p> <p>Key insight: Rebase is safe for personal feature branches, dangerous for shared branches. This is the golden rule of rebasing.</p>"},{"location":"sims/git-merge-vs-rebase/spec/#step-4-challenge","title":"Step 4: Challenge","text":"<p>Present scenario: \"You're on a team following trunk-based development. The team policy is: main must have linear history (no merge commits). You've finished your feature branch with 5 commits. But main has advanced 3 commits since you branched. How do you integrate?\"</p> <p>Student must: 1. Select appropriate scenario (or use custom state) 2. Choose Rebase (only option that creates linear history) 3. Observe that rebase makes it look like feature work happened after main's advances</p> <p>Follow-up question shown: \"What command would you run before creating a pull request to make sure your branch is up-to-date?\"</p> <p>Student should answer: <code>git rebase main</code> (or click Rebase button)</p> <p>Advanced challenge: \"After rebasing, you try to push but get 'rejected' error. Why? What flag do you need?\"</p> <p>Expected learning: - After rebase, must force-push because history is rewritten: <code>git push --force-with-lease</code> - <code>--force-with-lease</code> is safer than <code>--force</code> (checks that remote hasn't changed) - This is why rebasing shared branches is dangerous\u2014force-pushing affects others</p>"},{"location":"sims/git-merge-vs-rebase/spec/#technical-specification","title":"Technical Specification","text":""},{"location":"sims/git-merge-vs-rebase/spec/#technology","title":"Technology","text":"<ul> <li>Library: p5.js for graph rendering and animations</li> <li>Canvas: Responsive, min 600px width \u00d7 400px height, scales to viewport</li> <li>Frame Rate: 60fps for smooth commit movement animations</li> <li>Data: Commit graph stored as directed acyclic graph (DAG) in JSON format</li> </ul>"},{"location":"sims/git-merge-vs-rebase/spec/#implementation-notes","title":"Implementation Notes","text":"<p>Mobile Considerations: - Touch-friendly buttons (min 44px \u00d7 44px) - On screens &lt; 768px: Stack layout (graph above controls) - Pinch-to-zoom on graph for detailed inspection - Simplified animations on mobile (reduce particle effects, simpler transitions)</p> <p>Accessibility: - Keyboard navigation:   - Tab through buttons and controls   - Arrow keys to navigate commits in graph   - Enter/Space to activate buttons   - Escape to pause/reset animation - ARIA live regions announce animation progress: \"Replaying commit E onto main...\" - Screen reader describes graph structure: \"Main branch has 4 commits. Feature branch diverged at commit C with 2 additional commits.\" - High contrast mode option (removes subtle gradients, increases stroke width) - Text alternatives for all visual elements</p> <p>Performance: - For graphs with &gt;20 commits, use virtualization (render only visible portion + buffer) - Use requestAnimationFrame for smooth animations - Separate animation loop from interaction handlers - Memoize graph layout calculations (only recalculate on structural changes)</p>"},{"location":"sims/git-merge-vs-rebase/spec/#data-requirements","title":"Data Requirements","text":"<p>Commit Graph Format (JSON):</p> <pre><code>{\n  \"scenarios\": {\n    \"simple_feature\": {\n      \"name\": \"Simple Feature Branch\",\n      \"description\": \"Basic case: feature branch and main have diverged\",\n      \"commits\": [\n        {\"id\": \"a3f2c8b\", \"message\": \"Initial commit\", \"author\": \"Alice\", \"timestamp\": \"2024-01-15T10:00:00Z\", \"parents\": []},\n        {\"id\": \"b7e9d3a\", \"message\": \"Add database schema\", \"author\": \"Bob\", \"timestamp\": \"2024-01-16T11:30:00Z\", \"parents\": [\"a3f2c8b\"]},\n        {\"id\": \"c4f8e2d\", \"message\": \"Add user model\", \"author\": \"Alice\", \"timestamp\": \"2024-01-17T09:15:00Z\", \"parents\": [\"b7e9d3a\"]},\n        {\"id\": \"d2a7f9c\", \"message\": \"Update README\", \"author\": \"Charlie\", \"timestamp\": \"2024-01-18T14:20:00Z\", \"parents\": [\"c4f8e2d\"]},\n        {\"id\": \"e9b3d1f\", \"message\": \"Start auth feature\", \"author\": \"Alice\", \"timestamp\": \"2024-01-17T15:00:00Z\", \"parents\": [\"c4f8e2d\"]},\n        {\"id\": \"f1c5e7b\", \"message\": \"Complete auth feature\", \"author\": \"Alice\", \"timestamp\": \"2024-01-18T16:45:00Z\", \"parents\": [\"e9b3d1f\"]}\n      ],\n      \"branches\": {\n        \"main\": {\"current_commit\": \"d2a7f9c\", \"color\": \"#4A90E2\"},\n        \"feature-auth\": {\"current_commit\": \"f1c5e7b\", \"color\": \"#7ED321\"}\n      },\n      \"head\": \"feature-auth\"\n    }\n  },\n  \"merge_algorithm\": {\n    \"create_merge_commit\": true,\n    \"merge_commit_message\": \"Merge branch '{branch}' into {target}\",\n    \"merge_commit_parents\": [\"target_head\", \"branch_head\"]\n  },\n  \"rebase_algorithm\": {\n    \"identify_commits_to_replay\": \"commits reachable from branch HEAD but not from target\",\n    \"replay_order\": \"chronological by original commit timestamp\",\n    \"generate_new_hash\": \"simulate with hash(parent_hash + commit_message + timestamp_new)\"\n  }\n}\n</code></pre> <p>Animation Timeline Definition: <pre><code>{\n  \"merge_animation\": [\n    {\"step\": 1, \"duration\": 500, \"action\": \"highlight_both_heads\"},\n    {\"step\": 2, \"duration\": 500, \"action\": \"animate_arrow\", \"from\": \"branch_head\", \"to\": \"target_head\"},\n    {\"step\": 3, \"duration\": 300, \"action\": \"create_merge_commit\", \"shape\": \"diamond\"},\n    {\"step\": 4, \"duration\": 300, \"action\": \"update_branch_pointer\"}\n  ],\n  \"rebase_animation\": [\n    {\"step\": 1, \"duration\": 500, \"action\": \"highlight_commits_to_move\", \"color\": \"yellow\"},\n    {\"step\": 2, \"duration\": 300, \"action\": \"create_ghost_commits\", \"opacity\": 0.3},\n    {\"step\": 3, \"duration\": 800, \"action\": \"lift_commits\", \"direction\": \"up\", \"distance\": 50},\n    {\"step\": 4, \"duration\": 600, \"action\": \"reposition_commits\", \"align_with\": \"target_head\"},\n    {\"step\": 5, \"duration\": 400, \"action\": \"replay_commit\", \"effect\": \"bounce\", \"repeat_for_each\": true},\n    {\"step\": 6, \"duration\": 300, \"action\": \"fade_ghost_commits\"}\n  ]\n}\n</code></pre></p>"},{"location":"sims/git-merge-vs-rebase/spec/#assessment-integration","title":"Assessment Integration","text":"<p>After using this MicroSim, students should be able to answer:</p> <ol> <li>Quiz Question: \"Your team uses pull requests and shared feature branches. You want to clean up your branch before merging to main. Should you use merge or rebase?\"</li> <li>A) Merge\u2014always safe \u2713</li> <li>B) Rebase\u2014cleaner history</li> <li>C) Either one works equally well</li> <li>D) Force push to main directly</li> </ol> <p>Answer: A (Rebase on shared branches requires force-push, affecting team members)</p> <ol> <li>Conceptual Question: \"After rebasing your feature branch onto main, you try <code>git push</code> but get rejected. Why? What command should you use?\"</li> <li> <p>Expected answer: Rebase rewrites history (changes commit hashes), so remote and local have diverged. Must use <code>git push --force-with-lease</code> to update remote with rewritten history.</p> </li> <li> <p>Trade-off Question: \"What is the main advantage of merge over rebase? What is the main advantage of rebase over merge?\"</p> </li> <li> <p>Expected answer:</p> <ul> <li>Merge advantage: Preserves true history, safe for shared branches, never changes existing commits</li> <li>Rebase advantage: Creates clean linear history, easier to understand project evolution, avoids \"merge commit soup\"</li> </ul> </li> <li> <p>Scenario Question: \"You just rebased your branch and realized you made a mistake. Can you undo a rebase? How?\"</p> </li> <li>Expected answer: Yes, use <code>git reflog</code> to find commit before rebase, then <code>git reset --hard &lt;commit&gt;</code>. Reflog tracks HEAD movements even after rebase.</li> </ol>"},{"location":"sims/git-merge-vs-rebase/spec/#extension-ideas-optional","title":"Extension Ideas (Optional)","text":"<ul> <li>Interactive Rebase Simulation: Show <code>git rebase -i</code> functionality where students can reorder commits, squash commits together, or edit commit messages during rebase</li> <li>Conflict Resolution Preview: Animate what happens when rebase encounters conflicts (pauses at conflicting commit, shows resolution options)</li> <li>Cherry-Pick Visualization: Show how <code>git cherry-pick</code> works (similar to rebase but for individual commits)</li> <li>Fork Point Detection: Highlight the \"merge base\" (last common ancestor) between branches\u2014the point where rebase starts replaying</li> <li>Team Workflow Comparison: Show side-by-side comparison of different team strategies:</li> <li>\"Merge everything\" workflow (preserves all history)</li> <li>\"Rebase before PR\" workflow (clean feature branches, merge to main)</li> <li>\"Rebase always\" workflow (fully linear history)</li> <li>Force-Push Disaster Scenario: Demonstrate what happens when someone rebases a shared branch without coordination (show teammate's broken state)</li> <li>Reflog Explorer: Visualization of Git reflog showing how to recover from mistakes</li> <li>Bisect Integration: Show how linear history (from rebase) makes <code>git bisect</code> more effective for finding bugs</li> </ul> <p>Target Learning Outcome: Students understand that merge and rebase are different tools for integration with different trade-offs, and can choose the appropriate strategy based on branch sharing status and team workflow preferences. Students recognize that rebase rewrites history and requires force-pushing, making it unsuitable for shared branches.</p>"},{"location":"sims/normalization-journey/spec/","title":"MicroSim: Database Normalization Journey","text":""},{"location":"sims/normalization-journey/spec/#overview","title":"Overview","text":""},{"location":"sims/normalization-journey/spec/#concept-visualized","title":"Concept Visualized","text":"<ul> <li>Concept: First Normal Form (first-normal-form), Second Normal Form (second-normal-form), Third Normal Form (third-normal-form), Normalization (normalization)</li> <li>Learning Goal: Students understand database normalization by transforming a denormalized table step-by-step through 1NF, 2NF, and 3NF, observing how data anomalies are eliminated at each stage</li> <li>Difficulty: Intermediate</li> <li>Chapter: Week 3-4 (Data Storage &amp; Modeling)</li> </ul>"},{"location":"sims/normalization-journey/spec/#the-aha-moment","title":"The \"Aha\" Moment","text":"<p>When students progress from 2NF to 3NF, they see a seemingly normalized Customer table split further because ZIP code determines city/state (transitive dependency), demonstrating that normalization isn't just about removing duplication\u2014it's about removing dependencies that cause update anomalies.</p>"},{"location":"sims/normalization-journey/spec/#interface-design","title":"Interface Design","text":""},{"location":"sims/normalization-journey/spec/#layout","title":"Layout","text":"<ul> <li>Left Panel (60%): Visual table representation showing current normalization form</li> <li>Right Panel (40%): Step controls, anomaly detector, and educational explanations</li> </ul>"},{"location":"sims/normalization-journey/spec/#controls-right-panel","title":"Controls (Right Panel)","text":"Control Type Range/Options Default Effect Normalization Stage stepper \"Unnormalized\" \u2192 \"1NF\" \u2192 \"2NF\" \u2192 \"3NF\" Unnormalized Advances through normalization stages Show Next Step button N/A N/A Advances to next normal form with animation Reset Journey button N/A N/A Returns to unnormalized starting state Highlight Anomalies toggle On/Off On Pulses cells with update/insert/delete anomalies Show Dependencies toggle On/Off Off Draws arrows showing functional dependencies Speed slider 0.5x - 2x 1x Controls animation speed <p>Anomaly Detector Panel: - Shows active anomalies in current form:   - Insert Anomaly: Can't add X without Y   - Update Anomaly: Changing X requires updating N rows   - Delete Anomaly: Deleting X loses information about Y - Count decreases as normalization progresses</p> <p>Educational Explanation Panel: - Displays current rule being applied:   - 1NF: \"Eliminate repeating groups, ensure atomic values\"   - 2NF: \"Remove partial dependencies (non-key attributes depending on part of composite key)\"   - 3NF: \"Remove transitive dependencies (non-key attributes depending on other non-key attributes)\"</p>"},{"location":"sims/normalization-journey/spec/#visualization-left-panel","title":"Visualization (Left Panel)","text":"<p>What is Displayed: - Tables rendered as spreadsheet-style grids with column headers and sample rows - Primary keys highlighted in yellow - Foreign keys highlighted in blue - Cells with anomalies pulse with red border when \"Highlight Anomalies\" is on - Dependency arrows (when enabled) show functional dependencies: ColumnA \u2192 ColumnB</p> <p>Initial Unnormalized State (Example: Order Management): <pre><code>Orders Table (5 rows shown):\n| OrderID | OrderDate | CustomerName | CustomerAddress | Products | Quantities | Prices |\n|---------|-----------|--------------|-----------------|----------|------------|--------|\n| 101     | 2024-01-15| John Smith   | 123 Main, CA    | Mouse,Keyboard | 2,1    | 25,80  |\n| 102     | 2024-01-16| Jane Doe     | 456 Oak, NY     | Monitor  | 1          | 300    |\n| 103     | 2024-01-16| John Smith   | 123 Main, CA    | Mouse    | 5          | 25     |\n</code></pre></p> <p>Problems visible: - Multi-valued attributes: Products, Quantities, Prices (comma-separated) - Redundant data: CustomerAddress repeated for John Smith - Update anomaly: If John Smith moves, must update multiple rows</p>"},{"location":"sims/normalization-journey/spec/#educational-flow","title":"Educational Flow","text":""},{"location":"sims/normalization-journey/spec/#step-1-default-state-unnormalized","title":"Step 1: Default State (Unnormalized)","text":"<p>Student sees the messy unnormalized Orders table with visible problems: - Anomaly Detector shows: 3 insert anomalies, 5 update anomalies, 2 delete anomalies - Multi-valued fields highlighted: \"Products\" column contains \"Mouse,Keyboard\" - Redundant data pulsing: John Smith's address appears twice - Instructions: \"Click 'Show Next Step' to achieve First Normal Form\"</p>"},{"location":"sims/normalization-journey/spec/#step-2-first-interaction-transition-to-1nf","title":"Step 2: First Interaction - Transition to 1NF","text":"<p>Prompt: \"Click 'Show Next Step' to eliminate repeating groups and multi-valued attributes\"</p> <p>Result - Animated transformation: 1. Multi-valued Product cells \"explode\" into separate rows (1s animation) 2. Table expands from 3 rows to 5 rows: <pre><code>Orders Table (1NF):\n| OrderID | OrderDate | CustomerName | CustomerAddress | Product  | Quantity | Price |\n|---------|-----------|--------------|-----------------|----------|----------|-------|\n| 101     | 2024-01-15| John Smith   | 123 Main, CA    | Mouse    | 2        | 25    |\n| 101     | 2024-01-15| John Smith   | 123 Main, CA    | Keyboard | 1        | 80    |\n| 102     | 2024-01-16| Jane Doe     | 456 Oak, NY     | Monitor  | 1        | 300   |\n| 103     | 2024-01-16| John Smith   | 123 Main, CA    | Mouse    | 5        | 25    |\n</code></pre> 3. Composite primary key appears: (OrderID, Product) highlighted in yellow 4. Anomaly count updates: Insert anomalies eliminated, but update/delete anomalies persist 5. Explanation updates: \"\u2713 Atomic values achieved. But notice: customer info still repeats...\"</p> <p>Students observe: Table is cleaner but still has redundancy</p>"},{"location":"sims/normalization-journey/spec/#step-3-transition-to-2nf","title":"Step 3: Transition to 2NF","text":"<p>Prompt: \"Click 'Show Next Step' to remove partial dependencies\"</p> <p>Result - Animated transformation: 1. Table \"splits\" vertically into two tables (1.5s animation) 2. Customer columns slide out forming new table 3. Two tables emerge:</p> <pre><code>Orders Table (2NF):\n| OrderID | OrderDate | CustomerID |\n|---------|-----------|------------|\n| 101     | 2024-01-15| 1001       |\n| 102     | 2024-01-16| 1002       |\n| 103     | 2024-01-16| 1001       |\n\nOrder_Items Table (2NF):\n| OrderID | Product  | Quantity | Price |\n|---------|----------|----------|-------|\n| 101     | Mouse    | 2        | 25    |\n| 101     | Keyboard | 1        | 80    |\n| 102     | Monitor  | 1        | 300   |\n| 103     | Mouse    | 5        | 25    |\n\nCustomers Table (2NF):\n| CustomerID | CustomerName | CustomerAddress | ZIP   | City | State |\n|------------|--------------|-----------------|-------|------|-------|\n| 1001       | John Smith   | 123 Main St     | 90210 | LA   | CA    |\n| 1002       | Jane Doe     | 456 Oak Ave     | 10001 | NYC  | NY    |\n</code></pre> <ol> <li>Foreign key CustomerID highlighted in blue with arrow to Customers table</li> <li>Anomaly count: Update anomalies reduced (customer info no longer duplicated)</li> <li>Explanation: \"\u2713 Non-key attributes now depend on entire key. But look at Customers table...\"</li> </ol> <p>Students observe: Better, but Customers table still has redundancy (ZIP \u2192 City, State)</p>"},{"location":"sims/normalization-journey/spec/#step-4-transition-to-3nf","title":"Step 4: Transition to 3NF","text":"<p>Prompt: \"Enable 'Show Dependencies' and click 'Show Next Step' to remove transitive dependencies\"</p> <p>Result - Animated transformation: 1. Dependency arrows appear in Customers table: ZIP \u2192 City, ZIP \u2192 State (transitive) 2. Customers table splits (1.5s animation):</p> <pre><code>Customers Table (3NF):\n| CustomerID | CustomerName | CustomerAddress | ZIP   |\n|------------|--------------|-----------------|-------|\n| 1001       | John Smith   | 123 Main St     | 90210 |\n| 1002       | Jane Doe     | 456 Oak Ave     | 10001 |\n\nZIP_Codes Table (3NF):\n| ZIP   | City | State |\n|-------|------|-------|\n| 90210 | LA   | CA    |\n| 10001 | NYC  | NY    |\n</code></pre> <ol> <li>Foreign key relationship drawn: Customers.ZIP \u2192 ZIP_Codes.ZIP</li> <li>Anomaly count: 0 update anomalies, 0 insert anomalies, 0 delete anomalies</li> <li>Explanation: \"\u2713 Third Normal Form achieved! Each non-key attribute depends only on the primary key.\"</li> </ol> <p>Students observe: Completely normalized, no redundancy, no anomalies</p>"},{"location":"sims/normalization-journey/spec/#step-5-challenge","title":"Step 5: Challenge","text":"<p>Scenario presented: \"A developer suggests denormalizing back to 2NF, keeping City and State in Customers table because 'it makes queries simpler.' What are the consequences?\"</p> <p>Expected learning: - Students should identify:   - Update anomaly returns: If NYC changes to \"New York City\", must update thousands of rows   - Data integrity risk: Inconsistent city names (NYC vs New York vs Manhattan)   - Storage waste: Repeated city/state strings - Understand trade-off: Query simplicity vs data integrity - Recognize when denormalization is acceptable (read-heavy OLAP) vs problematic (OLTP)</p>"},{"location":"sims/normalization-journey/spec/#technical-specification","title":"Technical Specification","text":"<ul> <li>Library: p5.js</li> <li>Canvas: Responsive, min 700px width \u00d7 600px height</li> <li>Frame Rate: 30fps</li> <li>Data: Static sample dataset with predefined transformations   <pre><code>const normalization_stages = {\n  unnormalized: { tables: [...], anomalies: [...] },\n  first_nf: { tables: [...], anomalies: [...], transformation: {...} },\n  second_nf: { tables: [...], anomalies: [...], transformation: {...} },\n  third_nf: { tables: [...], anomalies: [...], transformation: {...} }\n}\n</code></pre></li> </ul>"},{"location":"sims/normalization-journey/spec/#assessment-integration","title":"Assessment Integration","text":"<p>After using this MicroSim, students should answer:</p> <ol> <li>Knowledge Check: What does First Normal Form (1NF) require?</li> <li>a) All non-key attributes depend on the primary key</li> <li>b) Each cell contains atomic (indivisible) values \u2713</li> <li>c) No transitive dependencies exist</li> <li> <p>d) Foreign keys are properly defined</p> </li> <li> <p>Application: You have a table where ZIP code determines City and State. The primary key is CustomerID. Which normal form is violated?</p> </li> <li>a) 1NF</li> <li>b) 2NF</li> <li>c) 3NF \u2713 (transitive dependency: CustomerID \u2192 ZIP \u2192 City/State)</li> <li> <p>d) The table is fully normalized</p> </li> <li> <p>Scenario-Based: A Student table has columns: StudentID (PK), Name, CourseID, CourseName, InstructorName. StudentID + CourseID form a composite key for enrollment. What's the problem?</p> </li> <li>a) Violates 1NF (multi-valued attributes)</li> <li>b) Violates 2NF (CourseName and InstructorName depend only on CourseID, partial dependency) \u2713</li> <li>c) Violates 3NF (transitive dependency)</li> <li>d) No problem, it's normalized</li> </ol>"},{"location":"sims/normalization-journey/spec/#extension-ideas","title":"Extension Ideas","text":"<ul> <li>Add \"Update Anomaly Simulator\" showing what happens when you update denormalized data</li> <li>Include BCNF (Boyce-Codd Normal Form) as optional advanced step</li> <li>Add \"Denormalization Decision\" mode showing when to intentionally denormalize (data warehouses)</li> <li>Show storage calculation: bytes saved through normalization</li> <li>Include real-world examples from e-commerce, healthcare, finance</li> <li>Add interactive \"Find the Dependency\" game where students identify functional dependencies</li> <li>Show SQL DDL code generation for each normal form</li> <li>Include performance testing showing query differences between normalized/denormalized</li> </ul>"},{"location":"sims/scd-timeline/spec/","title":"MicroSim: Slowly Changing Dimension Timeline","text":""},{"location":"sims/scd-timeline/spec/#overview","title":"Overview","text":""},{"location":"sims/scd-timeline/spec/#concept-visualized","title":"Concept Visualized","text":"<ul> <li>Concept: Slowly Changing Dimensions (slowly-changing-dims), SCD Type 1 (scd-type-1), SCD Type 2 (scd-type-2), SCD Type 3 (scd-type-3)</li> <li>Learning Goal: Students understand the differences between SCD Type 1, Type 2, and Type 3 by observing how each strategy handles dimensional attribute changes over time, demonstrating trade-offs between history preservation, storage, and query complexity</li> <li>Difficulty: Intermediate</li> <li>Chapter: Week 3-4 (Data Storage &amp; Modeling)</li> </ul>"},{"location":"sims/scd-timeline/spec/#the-aha-moment","title":"The \"Aha\" Moment","text":"<p>When students trigger a customer address change (John moves from California to Texas), they see three parallel timelines diverge: Type 1 overwrites the old value (losing history), Type 2 creates a new row with version dates (preserving full history), and Type 3 adds a \"previous_address\" column (limited history), demonstrating that SCD type choice determines what historical questions can be answered.</p>"},{"location":"sims/scd-timeline/spec/#interface-design","title":"Interface Design","text":""},{"location":"sims/scd-timeline/spec/#layout","title":"Layout","text":"<ul> <li>Left Panel (60%): Three horizontal timeline panels showing SCD Type 1, 2, and 3 side-by-side</li> <li>Right Panel (40%): Dimension change simulator and historical query tester</li> </ul>"},{"location":"sims/scd-timeline/spec/#controls-right-panel","title":"Controls (Right Panel)","text":"Control Type Range/Options Default Effect Select Customer dropdown \"John Smith\", \"Jane Doe\", \"Bob Wilson\" \"John Smith\" Chooses which customer to track Change Type dropdown \"Address Change\", \"Job Title Change\", \"Phone Number Change\" \"Address Change\" Selects attribute to modify Apply Change button N/A N/A Triggers dimension change event Playback Speed slider 0.5x - 3x 1x Controls animation speed Reset Timeline button N/A N/A Returns to initial state Ask Historical Query dropdown Various questions None Tests what each SCD type can answer <p>Historical Query Examples: - \"Where did John live on January 15, 2024?\" - \"How many customers were in California in Q1 2024?\" - \"What was John's previous address before current one?\" - \"Show all addresses John has ever had\"</p> <p>Query Result Panel: Shows whether each SCD type can answer the selected question: <pre><code>Query: \"Where did John live on January 15, 2024?\"\n\u251c\u2500 Type 1: \u2717 Cannot answer (history overwritten)\n\u251c\u2500 Type 2: \u2713 Texas (has valid_from/valid_to dates)\n\u2514\u2500 Type 3: \u2717 Only knows current &amp; previous (not specific dates)\n\nQuery: \"What was John's address before current?\"\n\u251c\u2500 Type 1: \u2717 Cannot answer\n\u251c\u2500 Type 2: \u2713 California (can traverse version history)\n\u2514\u2500 Type 3: \u2713 California (stored in previous_address column)\n</code></pre></p>"},{"location":"sims/scd-timeline/spec/#visualization-left-panel","title":"Visualization (Left Panel)","text":"<p>What is Displayed:</p> <p>Three horizontal timeline panels stacked vertically:</p> <p>Panel 1: SCD Type 1 (Overwrite) - Single row representing current dimension record - Timeline shows point-in-time snapshots as discrete boxes - When change occurs: Old value dissolves, new value fades in - Table structure:   <pre><code>| customer_id | name       | address      | phone      | updated_at |\n|-------------|------------|--------------|------------|------------|\n| 1001        | John Smith | California   | 555-0123   | 2024-01-10 |\n</code></pre></p> <p>Panel 2: SCD Type 2 (Add Row) - Multiple rows stacked vertically showing version history - Timeline shows valid date ranges as colored bars - When change occurs: New row slides in below, old row's valid_to date fills in - Table structure:   <pre><code>| surrogate_key | customer_id | name       | address    | valid_from | valid_to   | is_current |\n|---------------|-------------|------------|------------|------------|------------|------------|\n| 1             | 1001        | John Smith | California | 2023-01-01 | 2024-02-15 | No         |\n| 2             | 1001        | John Smith | Texas      | 2024-02-15 | 9999-12-31 | Yes        |\n</code></pre></p> <p>Panel 3: SCD Type 3 (Add Column) - Single row with additional \"previous\" columns - Timeline shows current and previous values as connected boxes - When change occurs: Current value shifts to \"previous\" column, new value enters \"current\" - Table structure:   <pre><code>| customer_id | name       | current_address | previous_address | address_changed_date |\n|-------------|------------|-----------------|------------------|----------------------|\n| 1001        | John Smith | Texas           | California       | 2024-02-15           |\n</code></pre></p> <p>Visual Elements: - Timeline ruler at bottom showing dates (Jan 2023 \u2192 Dec 2024) - Color coding:   - Green: Current/active records   - Blue: Historical records (Type 2)   - Gray: Overwritten data (shown briefly in Type 1 before deletion) - Animated transitions showing data movement</p> <p>How it Updates: - When \"Apply Change\" clicked:   1. Timeline playhead animates to change date (1s)   2. Three panels update simultaneously with different strategies:      - Type 1: Old cell fades out (red flash), new value fades in (0.8s)      - Type 2: New row slides in from bottom, old row's end date fills in, is_current flag changes (1.2s)      - Type 3: Current value slides left to \"previous\" column, new value slides in from right (1s)   3. Updated_at / changed_date timestamps update   4. Query result panel updates showing impact on historical queries</p>"},{"location":"sims/scd-timeline/spec/#educational-flow","title":"Educational Flow","text":""},{"location":"sims/scd-timeline/spec/#step-1-default-state","title":"Step 1: Default State","text":"<p>Student sees initial state (January 1, 2024): - Customer: John Smith - Address: California - All three SCD types show identical data - Timeline playhead at January 1, 2024 - No changes applied yet - Educational note: \"All SCD types start the same. The difference is how they handle changes.\"</p>"},{"location":"sims/scd-timeline/spec/#step-2-first-interaction-address-change","title":"Step 2: First Interaction - Address Change","text":"<p>Prompt: \"John Smith moves from California to Texas on February 15, 2024. Click 'Apply Change' to see how each SCD type handles this.\"</p> <p>Result - Synchronized animations across three panels:</p> <p>Type 1 Panel: - \"California\" cell flashes red and dissolves - \"Texas\" fades in - updated_at changes to \"2024-02-15\" - No trace of California remains - Icon appears: \u26a0 \"History Lost\"</p> <p>Type 2 Panel: - Original row's valid_to changes from \"9999-12-31\" to \"2024-02-15\" - is_current flag changes to \"No\" - New row slides in below:   - surrogate_key: 2 (new)   - customer_id: 1001 (same business key)   - address: Texas   - valid_from: 2024-02-15   - valid_to: 9999-12-31   - is_current: Yes - Timeline bars extend showing continuous coverage - Icon appears: \u2713 \"Full History Preserved\"</p> <p>Type 3 Panel: - \"California\" slides from \"current_address\" to \"previous_address\" - \"Texas\" slides into \"current_address\" - address_changed_date: \"2024-02-15\" - Icon appears: \u24d8 \"Limited History (1 Previous Value)\"</p> <p>Query Panel Updates: Query automatically runs: \"Where did John live in January 2024?\" - Type 1: \u2717 \"Cannot answer (only knows Texas)\" - Type 2: \u2713 \"California (valid_from/valid_to covers Jan 2024)\" - Type 3: \u2717 \"Cannot answer (no date ranges, only current + previous)\"</p> <p>Students observe: Each strategy handles changes completely differently</p>"},{"location":"sims/scd-timeline/spec/#step-3-second-change-phone-number-change","title":"Step 3: Second Change - Phone Number Change","text":"<p>Prompt: \"Apply another change: John changes phone number on June 1, 2024. Observe how Type 3 handles multiple changes.\"</p> <p>Result:</p> <p>Type 1: - Phone number updates (overwrites) - Previous phone lost</p> <p>Type 2: - Yet another new row created - Now has 3 versions: Original, After address change, After phone change - Full audit trail preserved</p> <p>Type 3: - Previous phone number overwrites previous address in \"previous\" column - California address is now completely lost! - Demonstrates: Type 3 only keeps ONE previous value</p> <p>Educational callout appears: \"Type 3 can only track one level of history. After the second change, California is lost!\"</p> <p>Students discover: Type 3's severe limitation</p>"},{"location":"sims/scd-timeline/spec/#step-4-historical-query-testing","title":"Step 4: Historical Query Testing","text":"<p>Prompt: \"Select different historical queries from the dropdown to see what each SCD type can answer\"</p> <p>Students try various queries:</p> <p>Query: \"How many customers lived in California in Q1 2024?\" - Type 1: \u2717 Cannot aggregate historical state - Type 2: \u2713 Can filter by valid_from/valid_to date ranges - Type 3: \u2717 No date ranges for historical aggregation</p> <p>Query: \"Show John's complete address history\" - Type 1: \u2717 Only current address - Type 2: \u2713 Full version history (California \u2192 Texas) - Type 3: Partial (Current: Texas, Previous: California, older lost)</p> <p>Query: \"What was John's address immediately before current?\" - Type 1: \u2717 Cannot answer - Type 2: \u2713 Query second-to-last record - Type 3: \u2713 Read previous_address column (simple!)</p> <p>Pattern discovered: - Type 1: Simplest schema, no history, cannot answer time-based questions - Type 2: Most powerful, full history, complex queries with date joins - Type 3: Middle ground, simple queries for immediate previous value only</p>"},{"location":"sims/scd-timeline/spec/#step-5-challenge-trade-off-decision","title":"Step 5: Challenge - Trade-off Decision","text":"<p>Scenario presented: \"You're designing a Customer dimension for an e-commerce data warehouse. Requirements: - 50 million customers - Customer addresses change ~2% per month (1M updates/month) - Business needs: 'What was revenue by customer state last quarter?' - Typical query: JOIN fact_sales with dim_customer on current records</p> <p>Which SCD type should you use?\"</p> <p>Expected learning: Should choose Type 2 because: - Business needs historical state analysis (\"by state last quarter\") - Type 1 cannot answer historical questions - Type 3 doesn't support date-based queries - Type 2 storage impact: ~1M new rows/month is acceptable for 50M base - Query pattern: Most joins use is_current = TRUE (fast), historical analysis uses date ranges</p> <p>Trade-offs to consider: - Type 2 storage: +24M rows/year (manageable) - Type 2 query complexity: Need to join on date ranges for historical analysis - Alternative for current-only queries: Create view filtering is_current = TRUE</p> <p>Advanced consideration: \"For attributes that change frequently but history isn't needed (e.g., last_login_date), use Type 1 for those columns even in Type 2 dimension\"</p>"},{"location":"sims/scd-timeline/spec/#technical-specification","title":"Technical Specification","text":"<ul> <li>Library: p5.js</li> <li>Canvas: Responsive, min 800px width \u00d7 600px height (200px per SCD panel)</li> <li>Frame Rate: 30fps</li> <li>Data: Static timeline with scripted changes   <pre><code>const timeline_events = [\n  { date: \"2024-02-15\", customer: \"John Smith\", change: \"address\", from: \"California\", to: \"Texas\" },\n  { date: \"2024-06-01\", customer: \"John Smith\", change: \"phone\", from: \"555-0123\", to: \"555-9999\" },\n  { date: \"2024-09-10\", customer: \"John Smith\", change: \"address\", from: \"Texas\", to: \"Florida\" }\n];\n\nconst historical_queries = [\n  { query: \"Where did John live on 2024-01-15?\", type1: false, type2: \"California\", type3: false },\n  { query: \"Show all addresses\", type1: \"Current only\", type2: \"Full history\", type3: \"Current + 1 previous\" }\n];\n</code></pre></li> </ul>"},{"location":"sims/scd-timeline/spec/#assessment-integration","title":"Assessment Integration","text":"<p>After using this MicroSim, students should answer:</p> <ol> <li>Knowledge Check: What is the main disadvantage of SCD Type 1?</li> <li>a) It's the most complex to implement</li> <li>b) It requires the most storage space</li> <li>c) It cannot answer historical questions \u2713</li> <li> <p>d) It's slower for current-state queries</p> </li> <li> <p>Application: A dimension has 1 million rows and 5% change monthly. After 12 months with SCD Type 2, approximately how many total rows exist?</p> </li> <li>a) 1 million (same as start)</li> <li>b) 1.6 million (1M + 12 months \u00d7 5% \u00d7 1M = 1.6M) \u2713</li> <li>c) 12 million (one copy per month)</li> <li> <p>d) 5 million</p> </li> <li> <p>Trade-off Analysis: When is SCD Type 3 the best choice?</p> </li> <li>a) When you need complete audit history</li> <li>b) When you need to track one previous value and changes are infrequent \u2713</li> <li>c) When storage is unlimited</li> <li> <p>d) SCD Type 3 is never the best choice</p> </li> <li> <p>Query Understanding: With SCD Type 2, how do you query for current records only?</p> </li> <li>a) Filter WHERE is_current = TRUE \u2713</li> <li>b) Filter WHERE valid_to = '9999-12-31' (also correct, alternative approach)</li> <li>c) Use MAX(valid_from)</li> <li>d) Current records cannot be distinguished</li> </ol>"},{"location":"sims/scd-timeline/spec/#extension-ideas","title":"Extension Ideas","text":"<ul> <li>Add fact table join visualization showing how historical analysis works</li> <li>Include storage calculator showing disk space impact over time</li> <li>Show SQL code snippets for inserting/updating each SCD type</li> <li>Add \"Hybrid SCD\" example (Type 1 for some attributes, Type 2 for others)</li> <li>Visualize SCD Type 4 (mini-dimension) and Type 6 (hybrid 1+2+3)</li> <li>Include performance comparison: query speed for current vs historical queries</li> <li>Add bi-temporal tracking (valid time vs transaction time)</li> <li>Show data warehouse ETL process applying SCD logic</li> <li>Include real-world examples: customer demographics, product prices, employee roles</li> </ul>"},{"location":"sims/sql-execution-plan/spec/","title":"MicroSim: SQL Query Execution Plan Visualizer","text":""},{"location":"sims/sql-execution-plan/spec/#overview","title":"Overview","text":""},{"location":"sims/sql-execution-plan/spec/#concept-visualized","title":"Concept Visualized","text":"<ul> <li>Concept: Execution Plans (execution-plans), Database Indexes (database-indexes), Query Optimization (query-optimization)</li> <li>Learning Goal: Students understand how database indexes dramatically reduce the number of rows scanned during query execution by manipulating index configurations and observing execution plan changes</li> <li>Difficulty: Intermediate</li> <li>Chapter: Week 3-4 (Data Storage &amp; Modeling)</li> </ul>"},{"location":"sims/sql-execution-plan/spec/#the-aha-moment","title":"The \"Aha\" Moment","text":"<p>When students add a B-tree index on a WHERE clause column, they see the execution plan switch from a full table scan (examining 1,000,000 rows) to an index seek (examining only 150 rows), demonstrating how indexes transform query performance from minutes to milliseconds.</p>"},{"location":"sims/sql-execution-plan/spec/#interface-design","title":"Interface Design","text":""},{"location":"sims/sql-execution-plan/spec/#layout","title":"Layout","text":"<ul> <li>Left Panel (60%): Visual execution plan tree with animated row flow</li> <li>Right Panel (40%): Index configuration controls and performance metrics</li> </ul>"},{"location":"sims/sql-execution-plan/spec/#controls-right-panel","title":"Controls (Right Panel)","text":"Control Type Range/Options Default Effect Query Type dropdown \"SELECT WHERE\", \"JOIN\", \"GROUP BY\" \"SELECT WHERE\" Changes the SQL query being executed Indexed Columns multi-checkbox \"customer_id\", \"order_date\", \"status\", \"amount\" None checked Adds/removes indexes on selected columns Index Type dropdown \"None\", \"B-Tree\", \"Hash\" \"B-Tree\" Changes index algorithm Table Size slider 1K - 10M rows 1M rows Scales the dataset size Run Query button N/A N/A Animates query execution with current settings"},{"location":"sims/sql-execution-plan/spec/#visualization-left-panel","title":"Visualization (Left Panel)","text":"<p>What is Displayed: - Execution plan tree (top-down flowchart: Query \u2192 Index Seek/Table Scan \u2192 Filter \u2192 Result) - Each node shows: operation name, rows processed, estimated cost - Animated data flow: colored particles flowing through nodes - Color coding: Green (index used, fast), Red (table scan, slow), Yellow (intermediate operations) - Bottom metrics bar: Total rows scanned, execution time estimate, cost score</p> <p>How it Updates: - When indexes are toggled, execution plan tree restructures in real-time (smooth 0.5s transition) - \"Table Scan\" node transforms to \"Index Seek\" when appropriate index is added - Row counts animate counting up/down to new values (1s duration) - Particle flow speed increases dramatically when index is used - Transitions: Smooth morph animations between plan structures</p>"},{"location":"sims/sql-execution-plan/spec/#educational-flow","title":"Educational Flow","text":""},{"location":"sims/sql-execution-plan/spec/#step-1-default-state","title":"Step 1: Default State","text":"<p>Student sees a simple query: <code>SELECT * FROM orders WHERE customer_id = 42;</code> - Execution plan shows: Table Scan \u2192 Filter \u2192 Result - Metrics: 1,000,000 rows scanned, 2.3s estimated time - Slow red particles trickling through the plan</p>"},{"location":"sims/sql-execution-plan/spec/#step-2-first-interaction","title":"Step 2: First Interaction","text":"<p>Prompt: \"Check the checkbox to add an index on customer_id\" Result: - Execution plan restructures to: Index Seek (customer_id) \u2192 Result - Metrics update: 150 rows scanned, 0.003s estimated time - Fast green particles rushing through the plan - Students observe 766x improvement and understand index purpose</p>"},{"location":"sims/sql-execution-plan/spec/#step-3-exploration","title":"Step 3: Exploration","text":"<p>Prompt: \"Try different queries and see which columns benefit from indexing\" Pattern they should discover: - Indexes help WHERE clause columns - Indexes help JOIN columns - Indexes on GROUP BY columns provide moderate benefit - Hash indexes faster for equality checks, B-Tree for range queries - Multiple indexes can compound benefits but have diminishing returns</p>"},{"location":"sims/sql-execution-plan/spec/#step-4-challenge","title":"Step 4: Challenge","text":"<p>Scenario: \"Your analytics query runs too slow: <code>SELECT SUM(amount) FROM orders WHERE order_date BETWEEN '2024-01-01' AND '2024-12-31' GROUP BY status;</code> Which index(es) should you add to optimize this query?\"</p> <p>Expected learning: - Students should index order_date (WHERE clause filter is primary bottleneck) - Optionally index status (GROUP BY benefit) - Understand that order_date needs B-Tree (range query), not Hash (equality only) - Recognize trade-off: indexes speed reads but slow writes</p>"},{"location":"sims/sql-execution-plan/spec/#technical-specification","title":"Technical Specification","text":"<ul> <li>Library: p5.js</li> <li>Canvas: Responsive, min 600px width \u00d7 500px height</li> <li>Frame Rate: 30fps</li> <li>Data: Static configuration with formula-based performance calculations</li> <li><code>rows_scanned = has_index ? table_size * 0.00015 : table_size</code></li> <li><code>execution_time_ms = rows_scanned * 0.0023</code></li> <li>Cost score based on rows scanned + sort operations</li> </ul>"},{"location":"sims/sql-execution-plan/spec/#assessment-integration","title":"Assessment Integration","text":"<p>After using this MicroSim, students should answer:</p> <ol> <li>Knowledge Check: You have a query <code>SELECT * FROM users WHERE email = 'user@example.com'</code>. The users table has 5 million rows. Without an index, approximately how many rows would the database scan?</li> <li>a) 1 row</li> <li>b) 5,000 rows</li> <li>c) 5,000,000 rows \u2713</li> <li> <p>d) It depends on the query optimizer</p> </li> <li> <p>Application: A query uses <code>WHERE created_at &gt; '2024-01-01'</code>. Should you use a B-Tree index or Hash index on created_at, and why?</p> </li> <li>a) Hash index, because it's faster</li> <li>b) B-Tree index, because it supports range queries \u2713</li> <li>c) No index needed for date columns</li> <li> <p>d) Either works equally well</p> </li> <li> <p>Trade-off Analysis: Your table receives 10,000 INSERT operations per second but only 10 SELECT queries per hour. Should you add multiple indexes?</p> </li> <li>a) Yes, indexes always improve performance</li> <li>b) No, indexes would slow down the frequent INSERT operations \u2713</li> <li>c) Only add Hash indexes</li> <li>d) Indexes don't affect INSERT performance</li> </ol>"},{"location":"sims/sql-execution-plan/spec/#extension-ideas","title":"Extension Ideas","text":"<ul> <li>Add composite index support showing index column order importance</li> <li>Include \"EXPLAIN ANALYZE\" output panel showing real PostgreSQL explain text</li> <li>Add cost-based optimizer simulation showing why optimizer chooses certain plans</li> <li>Include index fragmentation visualization over time with many writes</li> <li>Show covering index concept where query is satisfied entirely from index</li> <li>Add negative example: index on low-cardinality column (e.g., status with only 3 values)</li> </ul>"},{"location":"sims/sql-query-execution-plan/spec/","title":"MicroSim: SQL Query Execution Plan Visualizer","text":""},{"location":"sims/sql-query-execution-plan/spec/#overview","title":"Overview","text":""},{"location":"sims/sql-query-execution-plan/spec/#concept-visualized","title":"Concept Visualized","text":"<ul> <li>Concept: Query execution plans and database indexes (B-tree, hash, composite)</li> <li>Learning Goal: Students will understand how indexes dramatically affect query performance by adding/removing indexes and observing execution plan changes and timing differences</li> <li>Difficulty: Intermediate</li> <li>Chapter: Week 2 - SQL Mastery &amp; Week 3 - Relational Databases</li> </ul>"},{"location":"sims/sql-query-execution-plan/spec/#the-aha-moment","title":"The \"Aha\" Moment","text":"<p>When the student adds an index to a WHERE clause column, they see the execution plan switch from Sequential Scan (scanning all 100,000 rows) to Index Scan (scanning only matching rows), with query time dropping from 850ms to 12ms. This viscerally demonstrates why indexes matter in production databases.</p>"},{"location":"sims/sql-query-execution-plan/spec/#interface-design","title":"Interface Design","text":""},{"location":"sims/sql-query-execution-plan/spec/#layout","title":"Layout","text":"<ul> <li>Left Panel (60% width): Visualization canvas showing execution plan tree and data scan animation</li> <li>Right Panel (40% width): Interactive controls, query editor, and performance metrics</li> </ul>"},{"location":"sims/sql-query-execution-plan/spec/#controls-right-panel","title":"Controls (Right Panel)","text":"Control Type Range/Options Default Effect on Visualization Table Size slider 10K-1M rows 100K Affects scan time proportionally; visual shows more table blocks Query Type dropdown [\"Simple WHERE\", \"JOIN\", \"Aggregate with GROUP BY\", \"Complex Multi-Join\"] \"Simple WHERE\" Changes execution plan structure and query text Indexes checkbox group [\"idx_customer_id\", \"idx_order_date\", \"idx_product_category\", \"idx_composite_date_customer\"] None selected Adds/removes index; changes scan type in execution plan Show Timing toggle on/off on Displays/hides millisecond timing for each node Animate Scan button - - Triggers animation showing sequential vs index scan Run Query button - - Executes query and displays results with timing <p>Additional UI Elements: - Query Editor: Read-only SQL text that updates based on Query Type selection - Metrics Panel: Shows total rows scanned, execution time, cost estimate - Legend: Color codes for scan types (Sequential=red, Index=green, Bitmap=yellow)</p>"},{"location":"sims/sql-query-execution-plan/spec/#visualization-details-left-panel","title":"Visualization Details (Left Panel)","text":"<p>What is Displayed: - Execution Plan Tree: Hierarchical node structure showing operators (Scan, Join, Aggregate, Sort)   - Nodes represented as rounded rectangles with operator name and row estimates   - Color-coded by operation type:     - Red: Sequential Scan (expensive)     - Green: Index Scan (efficient)     - Yellow: Bitmap Index Scan (medium efficiency)     - Blue: Hash Join / Nested Loop     - Purple: Aggregate / Sort   - Arrow thickness represents data flow volume   - Node size proportional to estimated cost</p> <ul> <li>Table Visualization (below tree):</li> <li>Visual representation of table as grid of blocks</li> <li>Animated scan shows row-by-row evaluation for Sequential Scan</li> <li>Animated jump directly to matching rows for Index Scan</li> <li> <p>Progress bar showing percentage of table scanned</p> </li> <li> <p>B-Tree Index Structure (appears when index selected):</p> </li> <li>Simplified B-tree with root, internal nodes, leaf nodes</li> <li>Highlights path from root to target data during Index Scan animation</li> <li>Shows key comparisons at each level</li> </ul> <p>How it Updates: - When indexes checkbox changes:   - Execution plan tree regenerates with different scan node types   - Cost estimates recalculate (shown in nodes)   - Timing updates (Sequential Scan: 850ms \u2192 Index Scan: 12ms for 100K rows)   - Smooth 500ms transition animation</p> <ul> <li>When table size slider changes:</li> <li>All timing metrics scale proportionally</li> <li>Visual table grid expands/contracts</li> <li> <p>Cost numbers update in execution plan nodes</p> </li> <li> <p>When query type dropdown changes:</p> </li> <li>Entire execution plan tree rebuilds with different structure</li> <li>Query text updates in editor</li> <li> <p>Appropriate indexes for that query become relevant</p> </li> <li> <p>When Animate Scan button clicked:</p> </li> <li>3-second animation plays showing data access pattern</li> <li>Sequential Scan: red highlight moves row-by-row through entire table</li> <li>Index Scan: green highlight jumps through B-tree levels, then directly to matching rows</li> </ul> <p>Visual Feedback: - Hover states: Nodes show detailed cost breakdown tooltip (startup cost, total cost, rows) - Active states: Currently executing node pulses with blue glow - Performance indicator: Red/yellow/green badge on query timing (&gt;500ms=red, 50-500ms=yellow, &lt;50ms=green)</p>"},{"location":"sims/sql-query-execution-plan/spec/#educational-flow","title":"Educational Flow","text":""},{"location":"sims/sql-query-execution-plan/spec/#step-1-default-state","title":"Step 1: Default State","text":"<p>Student sees Simple WHERE query (<code>SELECT * FROM orders WHERE customer_id = 12345</code>) with execution plan showing Sequential Scan scanning all 100,000 rows. Timing shows 850ms. Table visualization shows red scanning animation going through every row.</p> <p>This shows the baseline: without an index, database must check every single row.</p>"},{"location":"sims/sql-query-execution-plan/spec/#step-2-first-interaction","title":"Step 2: First Interaction","text":"<p>Prompt: \"Click 'Animate Scan' to see how the database finds your data. Notice it checks every single row\u2014all 100,000 of them.\"</p> <p>Student watches sequential scan animation (3 seconds of red highlighting moving through entire table).</p> <p>Prompt: \"Now check the box for idx_customer_id and click 'Run Query' again.\"</p> <p>Student enables the index and clicks Run Query.</p> <p>Result: Execution plan immediately changes to Index Scan with 12ms timing (70x faster!). Animation shows green path jumping directly through B-tree levels to the matching rows. Only 15 rows highlighted in table (the matches).</p> <p>What it teaches: Indexes let the database jump directly to relevant data instead of checking everything. The B-tree visualization shows how this works\u2014like using a book index instead of reading every page.</p>"},{"location":"sims/sql-query-execution-plan/spec/#step-3-exploration","title":"Step 3: Exploration","text":"<p>Prompt: \"Try different query types from the dropdown. Which queries benefit most from indexes?\"</p> <p>Student experiments: - Simple WHERE: Sees index makes huge difference - JOIN: Sees two index scans (one per table) instead of sequential scans + nested loop - Aggregate with GROUP BY: Sees index scan + sort operation - Complex Multi-Join: Sees execution plan with 4 tables, some using indexes, some not</p> <p>Key insight: Indexes help filtering (WHERE) and joining, but don't eliminate all work\u2014sorting and aggregation still happen. Student discovers that composite indexes help when query filters on multiple columns (idx_composite_date_customer helps <code>WHERE order_date = X AND customer_id = Y</code>).</p>"},{"location":"sims/sql-query-execution-plan/spec/#step-4-challenge","title":"Step 4: Challenge","text":"<p>Present scenario: \"Your production database has 5 million orders. A critical dashboard query takes 45 seconds and is timing out. The query filters by order_date and product_category. Which index(es) would you add?\"</p> <p>Student uses controls to: 1. Set table size to 1M rows (simulating large table) 2. Select \"Complex Multi-Join\" query type 3. Try different index combinations:    - idx_order_date only: Improves but still 8 seconds    - idx_product_category only: Improves but still 9 seconds    - Both single indexes: Better, down to 3 seconds    - idx_composite_date_category: Best! Down to 450ms</p> <p>Expected learning: - Composite indexes are more efficient for queries filtering on multiple columns - Index order matters in composite indexes - Even with indexes, large result sets take time to return - Tradeoffs: Every index slows down INSERT/UPDATE operations (shown in warning message when too many indexes added)</p>"},{"location":"sims/sql-query-execution-plan/spec/#technical-specification","title":"Technical Specification","text":""},{"location":"sims/sql-query-execution-plan/spec/#technology","title":"Technology","text":"<ul> <li>Library: p5.js for canvas rendering and animations</li> <li>Canvas: Responsive, min 600px width \u00d7 500px height</li> <li>Frame Rate: 60fps for smooth animations</li> <li>Data: Static execution plan structures defined in JSON, timing calculations based on formulas</li> </ul>"},{"location":"sims/sql-query-execution-plan/spec/#implementation-notes","title":"Implementation Notes","text":"<ul> <li>Mobile: Touch-friendly controls; stacked layout (visualization above controls) on screens &lt; 768px</li> <li>Accessibility:</li> <li>Keyboard navigation: Tab through controls, Space/Enter to activate</li> <li>ARIA labels on all interactive elements</li> <li>Text alternatives for execution plan tree (screen reader announces node traversal)</li> <li>High contrast mode option for color-blind users</li> <li>Performance:</li> <li>Limit animation complexity for tables &gt; 500K rows (show representative subset)</li> <li>Use canvas layering: static execution plan on bottom layer, animations on top layer</li> <li>Debounce slider changes (300ms delay before recalculating)</li> </ul>"},{"location":"sims/sql-query-execution-plan/spec/#data-requirements","title":"Data Requirements","text":"<p>Execution Plan Templates (JSON format):</p> <pre><code>{\n  \"query_types\": {\n    \"simple_where\": {\n      \"sql\": \"SELECT * FROM orders WHERE customer_id = 12345\",\n      \"without_index\": {\n        \"node_type\": \"Sequential Scan\",\n        \"table\": \"orders\",\n        \"cost\": \"0.00..2512.00\",\n        \"rows\": 100000,\n        \"time_ms\": 850\n      },\n      \"with_index\": {\n        \"node_type\": \"Index Scan\",\n        \"index_name\": \"idx_customer_id\",\n        \"table\": \"orders\",\n        \"cost\": \"0.42..8.44\",\n        \"rows\": 15,\n        \"time_ms\": 12\n      }\n    },\n    \"join\": {\n      \"sql\": \"SELECT * FROM orders o JOIN customers c ON o.customer_id = c.id WHERE c.country = 'USA'\",\n      \"execution_plan\": {\n        \"node_type\": \"Hash Join\",\n        \"cost\": \"5234.23..8945.67\",\n        \"children\": [\n          {\"node_type\": \"Sequential Scan\", \"table\": \"customers\", \"filter\": \"country = 'USA'\"},\n          {\"node_type\": \"Sequential Scan\", \"table\": \"orders\"}\n        ]\n      }\n    }\n  },\n  \"timing_formulas\": {\n    \"sequential_scan_per_row\": 0.0085,\n    \"index_scan_overhead\": 8,\n    \"index_scan_per_match\": 0.3\n  }\n}\n</code></pre> <p>Sample Table Schema: <pre><code>orders (100K rows):\n  - order_id (PK)\n  - customer_id (FK)\n  - order_date\n  - product_category\n  - amount\n\nPossible indexes:\n  - idx_customer_id (B-tree on customer_id)\n  - idx_order_date (B-tree on order_date)\n  - idx_product_category (Hash on product_category)\n  - idx_composite_date_customer (B-tree on order_date, customer_id)\n</code></pre></p>"},{"location":"sims/sql-query-execution-plan/spec/#assessment-integration","title":"Assessment Integration","text":"<p>After using this MicroSim, students should be able to answer:</p> <ol> <li>Quiz Question: \"You have a query with WHERE customer_id = 100 AND order_date &gt; '2024-01-01'. Which index would be most efficient?\"</li> <li>A) Single index on customer_id</li> <li>B) Single index on order_date</li> <li>C) Composite index on (customer_id, order_date) \u2713</li> <li> <p>D) No index needed</p> </li> <li> <p>Conceptual Question: \"Why does adding an index speed up SELECT queries but slow down INSERT operations?\"</p> </li> <li> <p>Expected answer: Index must be updated on every insert, requiring additional writes and B-tree rebalancing</p> </li> <li> <p>Trade-off Question: \"Your table has 10 million rows but only 5 distinct values for the 'status' column. Would an index on 'status' be effective? Why or why not?\"</p> </li> <li>Expected answer: No\u2014low cardinality means index scan returns huge result set, making sequential scan potentially faster due to better I/O patterns</li> </ol>"},{"location":"sims/sql-query-execution-plan/spec/#extension-ideas-optional","title":"Extension Ideas (Optional)","text":"<ul> <li>Index Type Comparison: Add visualization comparing B-tree vs Hash index behavior (hash for equality only, no range scans)</li> <li>Covering Index Demo: Show how including non-key columns in index eliminates need to access table (Index-Only Scan)</li> <li>Join Strategy Comparison: Visualize Hash Join vs Nested Loop Join vs Merge Join with different data sizes</li> <li>Query Planner Simulation: Let students see how database estimates row counts and chooses between multiple possible plans</li> <li>Write Operation Impact: Add \"Run 1000 INSERTs\" button that shows how execution time increases with each index added</li> <li>Bitmap Index Scan: Show intermediate strategy combining bitmap of matching rows from multiple indexes before table access</li> <li>EXPLAIN ANALYZE Integration: Advanced mode that shows actual vs estimated rows, highlighting where planner guessed wrong</li> </ul> <p>Target Learning Outcome: Students understand that indexes are data structures (B-trees) that allow logarithmic-time lookups instead of linear scans, and can identify when indexes will/won't help query performance.</p>"},{"location":"sims/star-vs-snowflake/spec/","title":"MicroSim: Star Schema vs Snowflake Schema Comparison","text":""},{"location":"sims/star-vs-snowflake/spec/#overview","title":"Overview","text":""},{"location":"sims/star-vs-snowflake/spec/#concept-visualized","title":"Concept Visualized","text":"<ul> <li>Concept: Star Schema (star-schema), Snowflake Schema (snowflake-schema), Schema Design Tradeoffs (schema-comparison)</li> <li>Learning Goal: Students understand the structural and performance tradeoffs between star and snowflake schemas by toggling normalization levels and observing changes in join complexity, storage, and query patterns</li> <li>Difficulty: Intermediate</li> <li>Chapter: Week 3-4 (Data Storage &amp; Modeling)</li> </ul>"},{"location":"sims/star-vs-snowflake/spec/#the-aha-moment","title":"The \"Aha\" Moment","text":"<p>When students click \"Normalize Customer Dimension,\" they see a single Customer dimension table split into Customer \u2192 City \u2192 State \u2192 Country, transforming the simple 2-table star schema join into a complex 5-table snowflake join, demonstrating the tradeoff between storage savings (reduced redundancy) and query complexity (more joins).</p>"},{"location":"sims/star-vs-snowflake/spec/#interface-design","title":"Interface Design","text":""},{"location":"sims/star-vs-snowflake/spec/#layout","title":"Layout","text":"<ul> <li>Left Panel (60%): Side-by-side schema diagrams (Star Schema | Snowflake Schema)</li> <li>Right Panel (40%): Dimension normalization controls and metrics comparison</li> </ul>"},{"location":"sims/star-vs-snowflake/spec/#controls-right-panel","title":"Controls (Right Panel)","text":"Control Type Range/Options Default Effect Normalize Product toggle On/Off Off Splits Product into Product \u2192 Category \u2192 Department Normalize Customer toggle On/Off Off Splits Customer into Customer \u2192 City \u2192 State \u2192 Country Normalize Date toggle On/Off Off Splits Date into Date \u2192 Month \u2192 Quarter \u2192 Year Show Sample Query dropdown \"Monthly Sales by State\", \"Product Category Revenue\", \"Customer Lifetime Value\" None Displays SQL for selected analytical query Highlight Join Path button N/A N/A Animates join path through schema for active query Storage Calculator info panel N/A N/A Shows estimated storage with/without normalization <p>Metrics Comparison Panel: | Metric | Star Schema | Snowflake Schema | |--------|-------------|------------------| | Tables Count | 5 | Dynamic (5-11) | | Avg Joins per Query | 2-3 | 4-7 | | Storage (GB) | Higher | Lower | | Query Complexity | Simple | Complex | | Query Performance | Faster | Slower | | Redundant Data | Yes | Minimal |</p>"},{"location":"sims/star-vs-snowflake/spec/#visualization-left-panel","title":"Visualization (Left Panel)","text":"<p>What is Displayed: - Center: Fact table (Sales_Fact) shown as yellow rectangle with columns:   - sale_id, date_key, product_key, customer_key, store_key, quantity, amount - Surrounding: Dimension tables shown as blue rectangles radiating from fact table:   - Dim_Date, Dim_Product, Dim_Customer, Dim_Store - Relationships: Lines connecting fact table foreign keys to dimension primary keys - Table details: Each table shows key columns, row count estimate, storage size</p> <p>How it Updates: - When normalization toggle is activated:   1. Dimension table \"splits\" with animation (0.8s)   2. Sub-tables slide out from parent (1s, cascade effect)   3. New relationship lines draw from parent to child tables (0.5s)   4. Star schema side stays static, snowflake side updates   5. Metrics panel updates with new counts</p> <ul> <li>Example - Normalize Customer:</li> <li>Original: Dim_Customer [customer_key, name, address, city, state, country, phone]</li> <li> <p>Becomes:</p> <ul> <li>Dim_Customer [customer_key, name, city_key, phone]</li> <li>Dim_City [city_key, city_name, state_key]</li> <li>Dim_State [state_key, state_name, country_key]</li> <li>Dim_Country [country_key, country_name]</li> </ul> </li> <li> <p>Color coding: Yellow (Fact), Blue (Dimensions), Purple (Normalized Sub-dimensions)</p> </li> <li>Visual feedback: Redundant data cells pulse red in star schema, green checkmarks in snowflake for eliminated redundancy</li> </ul>"},{"location":"sims/star-vs-snowflake/spec/#educational-flow","title":"Educational Flow","text":""},{"location":"sims/star-vs-snowflake/spec/#step-1-default-state","title":"Step 1: Default State","text":"<p>Student sees simple e-commerce star schema: - 1 Fact table (Sales_Fact) in center: 10M rows, 850 MB - 4 Dimension tables around it: Date, Product, Customer, Store - Each dimension shows sample data with visible redundancy (e.g., \"California\" repeated 50K times in Customer table) - Metrics show: 5 tables, 2-3 joins average, 1.2 GB total storage</p>"},{"location":"sims/star-vs-snowflake/spec/#step-2-first-interaction","title":"Step 2: First Interaction","text":"<p>Prompt: \"Enable 'Normalize Customer' to see how snowflake schema reduces redundancy\"</p> <p>Result: - Star schema side: Unchanged - Snowflake schema side: Customer dimension splits into 4 tables - Storage metrics update:   - Star: Dim_Customer = 200 MB (redundant city/state/country)   - Snowflake: Combined = 145 MB (55 MB saved, 27.5% reduction) - Metrics panel shows joins increased from 2 to 4 for customer-related queries</p> <p>Students observe: Storage savings come at cost of query complexity</p>"},{"location":"sims/star-vs-snowflake/spec/#step-3-exploration","title":"Step 3: Exploration","text":"<p>Prompt: \"Try different queries using 'Show Sample Query' dropdown and observe join differences\"</p> <p>Pattern they should discover: - Star Schema advantages:   - Simple queries: <code>SELECT SUM(amount) FROM sales_fact JOIN dim_customer ON customer_key WHERE state = 'CA'</code>   - Only 2 tables involved   - Faster execution (fewer joins)   - Easier for business users to understand</p> <ul> <li>Snowflake Schema advantages:</li> <li>Storage efficiency: Eliminates redundancy (state name stored once, not 50K times)</li> <li>Data integrity: State name changes update in one place</li> <li> <p>Better for slowly changing dimensions (covered in SCD MicroSim)</p> </li> <li> <p>Trade-off discovery:</p> </li> <li>Snowflake saves storage but adds join overhead</li> <li>Star is denormalized for query performance</li> <li>Choice depends on: query patterns, storage costs, update frequency</li> </ul>"},{"location":"sims/star-vs-snowflake/spec/#step-4-challenge","title":"Step 4: Challenge","text":"<p>Scenario: \"Your data warehouse has 500M fact records and dimensions that rarely change. Queries are run thousands of times per day by business analysts using BI tools. Storage costs $0.02/GB/month. Which schema should you choose?\"</p> <p>Expected learning: - Should choose STAR SCHEMA - Reasoning:   - Query performance is critical (thousands of queries/day)   - Dimensions rarely change (denormalization doesn't cause update anomalies)   - Storage cost is minimal compared to query compute costs   - Business users need simple queries - Extra credit: Recognize dimensional models are typically denormalized by design for analytics</p>"},{"location":"sims/star-vs-snowflake/spec/#technical-specification","title":"Technical Specification","text":"<ul> <li>Library: p5.js</li> <li>Canvas: Responsive, min 800px width \u00d7 600px height</li> <li>Frame Rate: 30fps</li> <li>Data: Static schema definitions with normalization rules   <pre><code>const schemas = {\n  star: {\n    fact: { name: \"Sales_Fact\", rows: 10000000, columns: [...] },\n    dimensions: [\n      { name: \"Dim_Customer\", rows: 50000, columns: [...] }\n    ]\n  },\n  normalization_rules: {\n    customer: {\n      splits_into: [\"Dim_Customer\", \"Dim_City\", \"Dim_State\", \"Dim_Country\"],\n      storage_reduction: 0.275\n    }\n  }\n}\n</code></pre></li> </ul>"},{"location":"sims/star-vs-snowflake/spec/#assessment-integration","title":"Assessment Integration","text":"<p>After using this MicroSim, students should answer:</p> <ol> <li>Knowledge Check: What is the primary difference between star and snowflake schemas?</li> <li>a) Star schema has more tables</li> <li>b) Snowflake schema normalizes dimension tables \u2713</li> <li>c) Star schema cannot handle large datasets</li> <li> <p>d) Snowflake schema is always faster</p> </li> <li> <p>Application: A dimension table has 1M rows with city/state/country columns where \"California, USA\" appears 200K times. Normalizing would split this into 3 tables: Cities (5000 rows), States (50 rows), Countries (3 rows). Approximately how much storage is saved?</p> </li> <li>a) No savings, more tables means more storage</li> <li>b) Minimal savings (&lt; 5%)</li> <li>c) Significant savings (&gt; 20%) \u2713</li> <li> <p>d) It depends on the fact table size</p> </li> <li> <p>Trade-off Analysis: When is snowflake schema preferred over star schema?</p> </li> <li>a) When query performance is the top priority</li> <li>b) When storage costs are high and dimensions change frequently \u2713</li> <li>c) When business users write their own SQL queries</li> <li>d) Snowflake is never preferred, star is always better</li> </ol>"},{"location":"sims/star-vs-snowflake/spec/#extension-ideas","title":"Extension Ideas","text":"<ul> <li>Add query execution time simulator showing performance impact of join count</li> <li>Include actual SQL query builder that generates JOIN statements for both schemas</li> <li>Show index impact: star needs fewer indexes, snowflake benefits from more indexes</li> <li>Add \"Hybrid Schema\" option showing selective normalization</li> <li>Include realistic data samples showing redundancy visually (scrollable data preview)</li> <li>Add BI tool perspective showing how tools auto-generate queries differently</li> <li>Visualize incremental dimension updates showing update complexity differences</li> <li>Include cost calculator: storage cost vs compute cost for queries</li> </ul>"},{"location":"sims/star-vs-snowflake-schema/spec/","title":"MicroSim: Star Schema vs Snowflake Schema Comparison","text":""},{"location":"sims/star-vs-snowflake-schema/spec/#overview","title":"Overview","text":""},{"location":"sims/star-vs-snowflake-schema/spec/#concept-visualized","title":"Concept Visualized","text":"<ul> <li>Concept: Dimensional modeling\u2014star schema vs snowflake schema design patterns</li> <li>Learning Goal: Students will understand the structural and performance trade-offs between star and snowflake schemas by interacting with side-by-side models and observing query path differences</li> <li>Difficulty: Intermediate</li> <li>Chapter: Week 3-4 - Data Storage &amp; Modeling (Data Warehousing)</li> </ul>"},{"location":"sims/star-vs-snowflake-schema/spec/#the-aha-moment","title":"The \"Aha\" Moment","text":"<p>When the student traces a query through both schemas, they see the star schema requires 2 joins (fact \u2192 dimension) while the snowflake schema requires 4 joins (fact \u2192 dimension \u2192 subdimension \u2192 subdimension). The star schema query completes in 85ms, the snowflake in 210ms. But then they notice the snowflake schema saves 35% storage space due to normalization. This viscerally demonstrates the classic denormalization trade-off: query speed vs storage efficiency.</p>"},{"location":"sims/star-vs-snowflake-schema/spec/#interface-design","title":"Interface Design","text":""},{"location":"sims/star-vs-snowflake-schema/spec/#layout","title":"Layout","text":"<ul> <li>Left Panel (50% width): Star schema visualization with fact table and dimension tables</li> <li>Right Panel (50% width): Snowflake schema visualization with normalized dimension tables</li> <li>Bottom Panel (full width, 20% height): Interactive controls, query builder, and metrics comparison</li> </ul>"},{"location":"sims/star-vs-snowflake-schema/spec/#controls-bottom-panel","title":"Controls (Bottom Panel)","text":"Control Type Range/Options Default Effect on Visualization Query Scenario dropdown [\"Product Sales by Category\", \"Customer Demographics\", \"Time Period Analysis\", \"Multi-Dimension Drill-Down\"] \"Product Sales by Category\" Changes highlighted query path and SQL Run Query button - - Animates query execution through both schemas simultaneously Show SQL toggle on/off on Displays SQL queries under each schema Highlight Joins toggle on/off on Colors join paths as query animates Data Volume slider 1K-10M rows 100K Updates metrics (storage, query time) Show Storage toggle on/off off Displays table size metrics on each table Toggle Normalization button - - Animates snowflake dimension collapsing/expanding <p>Additional UI Elements: - Metrics Panel: Side-by-side comparison table showing:   - Total Joins Required   - Query Execution Time   - Storage Size   - Maintenance Complexity   - Update Anomaly Risk - SQL Display: Shows queries for both schemas with JOIN clauses highlighted - Legend: Explains table types (fact=orange, dimension=blue, subdimension=light blue)</p>"},{"location":"sims/star-vs-snowflake-schema/spec/#visualization-details-left-right-panels","title":"Visualization Details (Left &amp; Right Panels)","text":"<p>What is Displayed:</p> <p>Star Schema (Left Panel): - Fact Table (center): Large orange rectangle labeled \"FACT_SALES\"   - Columns shown: <code>sale_id, product_id, customer_id, date_id, store_id, quantity, revenue</code>   - Primary key marked with key icon   - Foreign keys marked with arrow icons   - Row count badge: \"2.5M rows\"</p> <ul> <li>Dimension Tables (surrounding fact table): Blue rectangles connected to fact table with lines</li> <li><code>DIM_PRODUCT</code>: <code>product_id, product_name, category, subcategory, brand, price</code></li> <li><code>DIM_CUSTOMER</code>: <code>customer_id, name, age, gender, city, state, country, segment</code></li> <li><code>DIM_DATE</code>: <code>date_id, date, day_of_week, month, quarter, year, is_holiday</code></li> <li> <p><code>DIM_STORE</code>: <code>store_id, store_name, city, state, region, square_footage</code></p> </li> <li> <p>Relationship Lines: Thick gray lines connecting fact table foreign keys to dimension table primary keys</p> </li> <li>Visual Layout: Star pattern with fact in center, dimensions radiating outward</li> </ul> <p>Snowflake Schema (Right Panel): - Fact Table (center): Identical to star schema\u2014<code>FACT_SALES</code> with same columns - Dimension Tables: Same as star but with fewer columns (normalized)   - <code>DIM_PRODUCT</code>: <code>product_id, product_name, subcategory_id, brand_id, price</code>   - <code>DIM_CUSTOMER</code>: <code>customer_id, name, age, gender, city_id, segment_id</code>   - <code>DIM_DATE</code>: <code>date_id, date, day_of_week, month_id, is_holiday</code>   - <code>DIM_STORE</code>: <code>store_id, store_name, city_id, square_footage</code></p> <ul> <li>Subdimension Tables: Light blue rectangles, smaller, connected to dimension tables</li> <li><code>DIM_PRODUCT_CATEGORY</code>: <code>subcategory_id, subcategory_name, category_id</code></li> <li><code>DIM_CATEGORY</code>: <code>category_id, category_name</code></li> <li><code>DIM_BRAND</code>: <code>brand_id, brand_name</code></li> <li><code>DIM_CUSTOMER_SEGMENT</code>: <code>segment_id, segment_name, segment_description</code></li> <li><code>DIM_GEO_CITY</code>: <code>city_id, city_name, state_id</code></li> <li><code>DIM_GEO_STATE</code>: <code>state_id, state_name, country_id</code></li> <li><code>DIM_GEO_COUNTRY</code>: <code>country_id, country_name, region</code></li> <li><code>DIM_MONTH</code>: <code>month_id, month_name, quarter_id</code></li> <li> <p><code>DIM_QUARTER</code>: <code>quarter_id, quarter_name, year</code></p> </li> <li> <p>Relationship Lines: Connected as tree structure (fact \u2192 dimension \u2192 subdimension \u2192 subdimension)</p> </li> <li>Visual Layout: Snowflake pattern with branches extending from dimensions</li> </ul> <p>Color Coding: - Orange: Fact tables - Dark Blue: First-level dimension tables - Light Blue: Normalized subdimension tables - Green: Active query path during animation - Yellow: Tables currently being read during query animation</p> <p>How it Updates:</p> <p>When \"Run Query\" button clicked: 1. Initial State (500ms): Both fact tables pulse yellow (starting point) 2. Star Schema Animation (left):    - Arrow animates from fact table to DIM_PRODUCT (300ms)    - DIM_PRODUCT highlights yellow, join condition appears: \"ON f.product_id = p.product_id\"    - Arrow animates from DIM_PRODUCT to DIM_CUSTOMER (300ms)    - DIM_CUSTOMER highlights yellow, join condition appears    - Green checkmark appears on star schema (query complete)    - Timer shows: 85ms</p> <ol> <li>Snowflake Schema Animation (right, simultaneous):</li> <li>Arrow animates from fact table to DIM_PRODUCT (300ms)</li> <li>DIM_PRODUCT highlights yellow</li> <li>Arrow animates from DIM_PRODUCT to DIM_PRODUCT_CATEGORY (300ms)</li> <li>DIM_PRODUCT_CATEGORY highlights yellow</li> <li>Arrow animates from DIM_PRODUCT_CATEGORY to DIM_CATEGORY (300ms)</li> <li>DIM_CATEGORY highlights yellow</li> <li>Arrow animates to DIM_CUSTOMER, then DIM_GEO_CITY, etc. (continues through normalized hierarchy)</li> <li>Green checkmark appears on snowflake schema (query complete)</li> <li> <p>Timer shows: 210ms</p> </li> <li> <p>Metrics Panel Updates:</p> </li> <li>\"Total Joins\" counter increments during animation</li> <li>\"Query Time\" counts up in milliseconds</li> <li>\"Storage Size\" displays final numbers (Star: 3.2 GB, Snowflake: 2.1 GB)</li> <li>Highlight differences in green (snowflake storage advantage) and red (snowflake query time disadvantage)</li> </ol> <p>When \"Toggle Normalization\" button clicked: - Snowflake schema animates: subdimension tables slide into parent dimensions (800ms) - Columns merge and duplicate data appears (showing denormalization) - Schema morphs into star schema structure - Reverse animation shows normalization: duplicate data highlighted, then split into subdimensions</p> <p>When \"Data Volume\" slider changes: - Storage size metrics update proportionally - Query time updates based on join cost formulas - Row count badges on tables update</p> <p>Visual Feedback: - Hover states: Tables show detailed column list with sample values - Join line hover: Shows join cardinality (1:many) and join condition - Active query path: Animated data flow particles moving along join lines - Performance indicator: Red/yellow/green badges on query times</p>"},{"location":"sims/star-vs-snowflake-schema/spec/#educational-flow","title":"Educational Flow","text":""},{"location":"sims/star-vs-snowflake-schema/spec/#step-1-default-state","title":"Step 1: Default State","text":"<p>Student sees both schemas side-by-side with \"Product Sales by Category\" query scenario loaded.</p> <p>SQL displayed under star schema: <pre><code>SELECT c.category_name, SUM(f.revenue)\nFROM FACT_SALES f\nJOIN DIM_PRODUCT p ON f.product_id = p.product_id\nGROUP BY c.category_name;\n</code></pre></p> <p>SQL displayed under snowflake schema: <pre><code>SELECT cat.category_name, SUM(f.revenue)\nFROM FACT_SALES f\nJOIN DIM_PRODUCT p ON f.product_id = p.product_id\nJOIN DIM_PRODUCT_CATEGORY sc ON p.subcategory_id = sc.subcategory_id\nJOIN DIM_CATEGORY cat ON sc.category_id = cat.category_id\nGROUP BY cat.category_name;\n</code></pre></p> <p>Info panel explains: \"Both schemas answer the same business question: 'What is revenue by product category?' But notice the star schema has category already denormalized in DIM_PRODUCT, while snowflake normalizes it into separate tables.\"</p> <p>This shows the fundamental structural difference before any performance discussion.</p>"},{"location":"sims/star-vs-snowflake-schema/spec/#step-2-first-interaction","title":"Step 2: First Interaction","text":"<p>Prompt: \"Click 'Run Query' and watch how the query flows through each schema. Count the joins.\"</p> <p>Student clicks Run Query and watches simultaneous animation.</p> <p>Result: - Star schema: 1 join (fact \u2192 product dimension), completes in 85ms - Snowflake schema: 3 joins (fact \u2192 product \u2192 subcategory \u2192 category), completes in 210ms - Metrics panel highlights: \"Star schema: 2.5x faster for this query\"</p> <p>Info panel explains: \"Star schema is faster because it denormalizes dimensions\u2014category is already in the product dimension. Snowflake schema normalizes dimensions to eliminate redundancy, but requires more joins.\"</p> <p>Prompt: \"Now toggle 'Show Storage' to see the space trade-off.\"</p> <p>Student enables storage display.</p> <p>Result: - Star schema: DIM_PRODUCT shows 1.2 GB (450K rows \u00d7 8 columns with repeated category/subcategory values) - Snowflake schema: DIM_PRODUCT 850 MB + DIM_PRODUCT_CATEGORY 5 MB + DIM_CATEGORY 1 MB = 856 MB total - Savings: 35% less storage with snowflake</p> <p>What it teaches: - Star schema trades storage for query speed (denormalization) - Snowflake schema trades query speed for storage efficiency (normalization) - The choice depends on priorities: query performance vs storage costs</p>"},{"location":"sims/star-vs-snowflake-schema/spec/#step-3-exploration","title":"Step 3: Exploration","text":"<p>Prompt: \"Try different query scenarios. Which queries benefit more from star schema? Do any queries perform similarly in both?\"</p> <p>Student selects \"Customer Demographics\" scenario: - Query needs customer name, city, state, country - Star schema: 1 join (fact \u2192 customer), 95ms - Snowflake schema: 4 joins (fact \u2192 customer \u2192 city \u2192 state \u2192 country), 285ms - Star schema advantage is even larger for deeply nested hierarchies</p> <p>Student selects \"Time Period Analysis\": - Query needs just date and month - Star schema: 1 join, 80ms - Snowflake schema: 2 joins (fact \u2192 date \u2192 month), 120ms - Smaller difference for shallow hierarchies</p> <p>Key insight: The more levels of hierarchy (geography: city \u2192 state \u2192 country \u2192 region), the more snowflake schema suffers in query performance. Star schema flattens all hierarchy levels into one table.</p> <p>Prompt: \"What happens when you need to UPDATE a product's category in each schema?\"</p> <p>Student clicks on DIM_PRODUCT category field in star schema.</p> <p>Info panel shows: \"Star schema update: Must update category in 15,000 rows (every product in 'Electronics' category). Risk: Update anomaly if you miss some rows.\"</p> <p>Student clicks on DIM_CATEGORY in snowflake schema.</p> <p>Info panel shows: \"Snowflake schema update: Update 1 row in DIM_CATEGORY. All products automatically reflect change through foreign key relationship.\"</p> <p>Key insight: Snowflake schema is easier to maintain\u2014updates happen in one place. Star schema has data redundancy, making updates error-prone.</p>"},{"location":"sims/star-vs-snowflake-schema/spec/#step-4-challenge","title":"Step 4: Challenge","text":"<p>Present scenario: \"You're designing a data warehouse for an e-commerce company. They have: - 10 million orders (fact table) - 500,000 products with 3-level category hierarchy (Electronics \u2192 Laptops \u2192 Gaming Laptops) - 2 million customers across 50,000 cities in 200 countries - Dashboard queries run every 5 minutes analyzing sales by category, geography, time - Storage costs $0.02/GB/month, and you have budget constraints - Marketing team occasionally updates product categories (10-20 times/month)</p> <p>Should you use star schema or snowflake schema? Why?\"</p> <p>Student must: 1. Use Data Volume slider to set 10M rows 2. Run different query scenarios to measure performance 3. Check storage metrics 4. Consider maintenance complexity</p> <p>Expected reasoning: - Query frequency: Dashboard runs every 5 minutes \u2192 query performance is critical \u2192 favors star schema - Storage costs: Budget constraints and large dimension tables \u2192 favors snowflake schema - Maintenance: Frequent category updates \u2192 easier in snowflake \u2192 favors snowflake schema - Data volume: 10M facts with 500K products \u2192 denormalizing products adds significant storage \u2192 favors snowflake schema</p> <p>Best answer: Hybrid approach or snowflake schema with strategic denormalization: - Use snowflake schema for geographic hierarchy (saves most space) - Consider denormalizing product category into DIM_PRODUCT (most frequently queried) - Or use materialized views to pre-join snowflake schema for dashboards - Or use columnar storage (like BigQuery) where snowflake's extra joins matter less</p> <p>Advanced insight shown: \"In modern cloud data warehouses with columnar storage and massively parallel processing, snowflake schema join penalties are minimized. But for traditional row-based databases, star schema often wins.\"</p>"},{"location":"sims/star-vs-snowflake-schema/spec/#technical-specification","title":"Technical Specification","text":""},{"location":"sims/star-vs-snowflake-schema/spec/#technology","title":"Technology","text":"<ul> <li>Library: p5.js for schema rendering and animations</li> <li>Canvas: Responsive, min 1000px width \u00d7 600px height (side-by-side), stacks on mobile</li> <li>Frame Rate: 60fps for smooth query path animations</li> <li>Data: Schema metadata and query paths defined in JSON</li> </ul>"},{"location":"sims/star-vs-snowflake-schema/spec/#implementation-notes","title":"Implementation Notes","text":"<p>Mobile Considerations: - Screens &lt; 1024px: Stack schemas vertically instead of side-by-side - Touch-friendly: Tap tables to expand column details - Simplified animation on mobile (remove particle effects, instant transitions for slower devices) - Pinch-to-zoom enabled for detailed table inspection</p> <p>Accessibility: - Keyboard navigation:   - Tab through tables in reading order (fact \u2192 dimensions \u2192 subdimensions)   - Arrow keys to navigate relationships   - Enter to trigger query animation   - Space to pause/resume animation - Screen reader announces:   - Table structure: \"Fact table FACT_SALES with 7 columns, connected to 4 dimension tables\"   - Query progress: \"Joining fact table to product dimension... now joining product to category...\"   - Metrics comparison: \"Star schema completed in 85 milliseconds with 2 joins. Snowflake schema completed in 210 milliseconds with 5 joins.\" - High contrast mode: Increase line thickness, remove subtle gradients - Text alternatives for all animations (text-based query execution log)</p> <p>Performance: - Use CSS transforms for table positioning (hardware-accelerated) - Canvas layering: Static schema on bottom layer, animations on top layer - Debounce slider changes (400ms before recalculating metrics) - Limit simultaneous animations to 2 paths (star + snowflake)</p>"},{"location":"sims/star-vs-snowflake-schema/spec/#data-requirements","title":"Data Requirements","text":"<p>Schema Definition (JSON format):</p> <pre><code>{\n  \"star_schema\": {\n    \"tables\": {\n      \"FACT_SALES\": {\n        \"type\": \"fact\",\n        \"columns\": [\"sale_id\", \"product_id\", \"customer_id\", \"date_id\", \"store_id\", \"quantity\", \"revenue\"],\n        \"primary_key\": \"sale_id\",\n        \"foreign_keys\": [\"product_id\", \"customer_id\", \"date_id\", \"store_id\"],\n        \"row_count\": 2500000,\n        \"size_mb\": 850\n      },\n      \"DIM_PRODUCT\": {\n        \"type\": \"dimension\",\n        \"columns\": [\"product_id\", \"product_name\", \"category\", \"subcategory\", \"brand\", \"price\"],\n        \"primary_key\": \"product_id\",\n        \"row_count\": 450000,\n        \"size_mb\": 1200,\n        \"sample_data\": {\n          \"product_id\": 101,\n          \"product_name\": \"Gaming Laptop X1\",\n          \"category\": \"Electronics\",\n          \"subcategory\": \"Laptops\",\n          \"brand\": \"TechCorp\",\n          \"price\": 1299.99\n        }\n      }\n    },\n    \"relationships\": [\n      {\"from\": \"FACT_SALES.product_id\", \"to\": \"DIM_PRODUCT.product_id\", \"cardinality\": \"many-to-one\"}\n    ]\n  },\n  \"snowflake_schema\": {\n    \"tables\": {\n      \"FACT_SALES\": { \"same\": \"as star schema\" },\n      \"DIM_PRODUCT\": {\n        \"type\": \"dimension\",\n        \"columns\": [\"product_id\", \"product_name\", \"subcategory_id\", \"brand_id\", \"price\"],\n        \"primary_key\": \"product_id\",\n        \"foreign_keys\": [\"subcategory_id\", \"brand_id\"],\n        \"row_count\": 450000,\n        \"size_mb\": 850\n      },\n      \"DIM_PRODUCT_CATEGORY\": {\n        \"type\": \"subdimension\",\n        \"columns\": [\"subcategory_id\", \"subcategory_name\", \"category_id\"],\n        \"primary_key\": \"subcategory_id\",\n        \"foreign_keys\": [\"category_id\"],\n        \"row_count\": 500,\n        \"size_mb\": 5\n      },\n      \"DIM_CATEGORY\": {\n        \"type\": \"subdimension\",\n        \"columns\": [\"category_id\", \"category_name\"],\n        \"primary_key\": \"category_id\",\n        \"row_count\": 50,\n        \"size_mb\": 1\n      }\n    }\n  },\n  \"query_scenarios\": {\n    \"product_sales_by_category\": {\n      \"description\": \"Analyze revenue by product category\",\n      \"star_query_path\": [\"FACT_SALES\", \"DIM_PRODUCT\"],\n      \"star_joins\": 1,\n      \"star_time_ms\": 85,\n      \"snowflake_query_path\": [\"FACT_SALES\", \"DIM_PRODUCT\", \"DIM_PRODUCT_CATEGORY\", \"DIM_CATEGORY\"],\n      \"snowflake_joins\": 3,\n      \"snowflake_time_ms\": 210\n    }\n  }\n}\n</code></pre> <p>Performance Calculation Formulas: <pre><code>// Query time = base_join_cost * num_joins * (row_count_factor)\nstar_time = 50 + (num_joins * 15) * (row_count / 100000)\nsnowflake_time = 50 + (num_joins * 25) * (row_count / 100000) // Higher cost per join due to normalization\n\n// Storage calculation\nstar_storage = fact_table_size + sum(denormalized_dimension_sizes)\nsnowflake_storage = fact_table_size + sum(normalized_dimension_sizes) + sum(subdimension_sizes)\n// Snowflake typically 20-40% smaller due to elimination of redundancy\n</code></pre></p>"},{"location":"sims/star-vs-snowflake-schema/spec/#assessment-integration","title":"Assessment Integration","text":"<p>After using this MicroSim, students should be able to answer:</p> <ol> <li>Quiz Question: \"A star schema typically has _____ query performance but _____ storage requirements compared to snowflake schema.\"</li> <li>A) Better, higher \u2713</li> <li>B) Worse, lower</li> <li>C) Better, lower</li> <li> <p>D) Worse, higher</p> </li> <li> <p>Conceptual Question: \"Why does snowflake schema require more joins than star schema for the same analytical query?\"</p> </li> <li> <p>Expected answer: Snowflake schema normalizes dimensions into multiple related tables (e.g., splitting geography into city \u2192 state \u2192 country tables), so queries must join through hierarchy levels. Star schema denormalizes dimensions into flat tables, requiring only one join from fact to dimension.</p> </li> <li> <p>Trade-off Question: \"Your data warehouse has a product dimension with 1 million products across a 4-level category hierarchy. Dashboard queries frequently filter by top-level category. Should you use star or snowflake schema? What's a hybrid approach?\"</p> </li> <li> <p>Expected answer:</p> <ul> <li>Pure star: Fast queries (1 join) but massive storage waste (category repeated 1M times)</li> <li>Pure snowflake: Saves storage but requires 4 joins</li> <li>Hybrid (best): Snowflake structure but denormalize top-level category into product dimension (most queried field), keep other hierarchy levels normalized</li> <li>Or use materialized aggregate tables pre-joined at category level</li> </ul> </li> <li> <p>Scenario Question: \"A marketing team needs to rename a product category from 'Electronics' to 'Consumer Electronics'. How does the update differ between star and snowflake schema? Which is safer?\"</p> </li> <li>Expected answer:<ul> <li>Star schema: Must UPDATE millions of rows in product dimension (WHERE category = 'Electronics'). Risk of inconsistency if update fails partway.</li> <li>Snowflake schema: UPDATE 1 row in category table. All products automatically reflect change via foreign key.</li> <li>Snowflake is safer for maintenance\u2014single source of truth.</li> </ul> </li> </ol>"},{"location":"sims/star-vs-snowflake-schema/spec/#extension-ideas-optional","title":"Extension Ideas (Optional)","text":"<ul> <li>Hybrid Schema Builder: Let students create custom hybrid approach, denormalizing selected columns while keeping others normalized</li> <li>Query Optimizer Simulation: Show how modern query optimizers handle snowflake joins (join elimination, predicate pushdown)</li> <li>Real Database Comparison: Load actual data into star/snowflake schemas in BigQuery, run real queries, compare actual execution times</li> <li>Columnar Storage Impact: Toggle between row-based and columnar storage, showing how columnar reduces snowflake join penalty</li> <li>Aggregate Table Strategy: Show how pre-aggregated fact tables at different grain levels can make snowflake queries fast</li> <li>Slowly Changing Dimension Integration: Demonstrate how SCD Type 2 complicates snowflake schema (more tables to historize)</li> <li>Dimension Role-Playing: Show how one dimension (like date) can be used multiple times in fact table (order_date, ship_date, delivery_date), and how this affects schema design</li> <li>Fact Table Types: Compare transaction fact tables vs snapshot fact tables vs accumulating snapshot fact tables in both schemas</li> <li>Indexing Strategy: Show how different index strategies (bitmap indexes on dimensions, partitioning on fact table) affect query performance in each schema</li> </ul> <p>Target Learning Outcome: Students understand that star and snowflake schemas represent a trade-off between query performance and storage/maintenance efficiency, can identify when each is appropriate, and recognize that hybrid approaches often provide the best balance for real-world requirements.</p>"}]}